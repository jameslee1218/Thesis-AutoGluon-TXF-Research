{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Y4IBghCHgk"
      },
      "source": [
        "# AutoGluon ÊªæÂãïË®ìÁ∑¥Ôºà‰∏âÂπ¥Ë®ìÁ∑¥„ÄÅÈ†êÊ∏¨Á¨¨ÂõõÂπ¥Ôºâ‚Äî ‰æùÂ∫èÁâà\n",
        "\n",
        "- **Ëº∏ÂÖ•**Ôºö`data/autogluon_ready/{0900,0915,0930}/merged_for_autogluon_*.csv`ÔºàÁî± `merge_output2_for_autogluon.py` Áî¢Âá∫Ôºâ„ÄÇ\n",
        "- **ÊµÅÁ®ã**Ôºö‰æùÂ∫èË∑ë 0900 ‚Üí 0915 ‚Üí 0930 ‰∏âÁµÑÔºõÊØèÁµÑÁî®**Ââç‰∏âÂπ¥**Ë≥áÊñôË®ìÁ∑¥„ÄÅÈ†êÊ∏¨**Á¨¨ÂõõÂπ¥**ÔºõËº∏Âá∫Ëá≥ `data/models/{cutoff}/`„ÄÇ\n",
        "- **Êñ∑Á∑öÁ∫åË∑ë**ÔºöËã•Ëº∏Âá∫Ë≥áÊñôÂ§æÂ∑≤Êúâ `predictions.csv` ÂâáË∑≥ÈÅéË©≤Ê¨°Ë®ìÁ∑¥ÔºåÁõ¥Êé•ËºâÂÖ•Êó¢ÊúâÁµêÊûú„ÄÇ\n",
        "- **‰∏¶Ë°åÁâà**ÔºöËã•Ë¶ÅÂêåÊôÇË∑ë‰∏âÂÄãÊà™Èªû‰∫í‰∏çÂπ≤ÊìæÔºåË´ã‰ΩøÁî® `train_autogluon_colab_0900.ipynb`„ÄÅ`train_autogluon_colab_0915.ipynb`„ÄÅ`train_autogluon_colab_0930.ipynb`ÔºàÂêÑÈñã‰∏ÄÂÄã ColabÔºâ„ÄÇ\n",
        "- **Ë®ìÁ∑¥Âπ¥ÊØîËºÉ**ÔºöËã•Ë¶ÅËø¥Âúà train_years=2,3,4,5 ÊØîËºÉÂì™ÂÄãËºÉ‰Ω≥ÔºåË´ã‰ΩøÁî®‰∏äËø∞‰∏âÂÄãÂñÆ‰∏ÄÊà™Èªû notebookÔºàÂ∑≤ÂÖßÂª∫ TRAIN_YEARS_LISTÔºâ„ÄÇ\n",
        "- **Colab**ÔºöÂ∞á merge ÂæåÁöÑË≥áÊñô‰∏äÂÇ≥Ëá≥ Drive Â∞àÊ°à `data/autogluon_ready/`Ôºå‰∏ãÊñπË®≠ÂÆö `DRIVE_PROJECT_ROOT`„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3I6AI3pCHgm"
      },
      "source": [
        "## 1. ÊéõËºâ Google DriveÔºàColab ÂøÖË∑ëÔºõÊú¨Ê©üÂèØÁï•Ôºâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsnOGcy5CHgn",
        "outputId": "0a693cf1-67bf-4705-a37e-6af9b4a16184"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "print(\"Colab:\", IN_COLAB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Colab: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjGjh9cVCHgo"
      },
      "source": [
        "## 2. Ë∑ØÂæëËàáÂèÉÊï∏"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yizqNL0cscZO",
        "outputId": "cdd066fa-b1b2-476b-8dbc-cc2fea90029a"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# ========== Ë∑ØÂæëË®≠ÂÆöÔºàColab Ë´ãÂÖàË∑ë‰∏äÊñπ„ÄåÊéõËºâ Drive„ÄçÔºâ ==========\n",
        "DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research\"\n",
        "LOCAL_PROJECT_ROOT = \"/Volumes/Transcend/thesis/github_clone/Thesis-AutoGluon-TXF-Research\"\n",
        "\n",
        "PROJECT_ROOT = Path(DRIVE_PROJECT_ROOT) if IN_COLAB else Path(LOCAL_PROJECT_ROOT)\n",
        "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
        "\n",
        "# ‰æùÂ∫èË∑ë 0900 ‚Üí 0915 ‚Üí 0930 ‰∏âÁµÑ\n",
        "CUTOFFS = (\"0900\", \"0915\", \"0930\")\n",
        "TRAIN_YEARS = 3  # ‰∏âÂπ¥Ë®ìÁ∑¥„ÄÅÈ†êÊ∏¨Á¨¨ÂõõÂπ¥\n",
        "LABEL = \"target_return\"\n",
        "TIME_LIMIT = 30  # ÁßíÔºõÊØèÊÆµË®ìÁ∑¥ÊôÇÈñì\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"CUTOFFS:\", CUTOFFS, \"| TRAIN_YEARS:\", TRAIN_YEARS)\n",
        "for c in CUTOFFS:\n",
        "    p = DATA_ROOT / \"autogluon_ready\" / c / f\"merged_for_autogluon_{c}.csv\"\n",
        "    print(f\"  {c}: {'‚úÖ' if p.exists() else '‚ùå'} {p}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MERGED_CSV_PATH: /content/drive/MyDrive/2026/Ë´ñÊñá/Thesis-AutoGluon-TXF-Research/data/merged_for_autogluon_0900/merged_for_autogluon_0900.csv\n",
            "ROLL_OUTPUT: /Volumes/Transcend/thesis/github_clone/Thesis-AutoGluon-TXF-Research/data/models\n",
            "TRAIN_YEARS: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNemA6jsCHgo",
        "outputId": "14601ee7-c0e6-4a94-901a-3d173f612034"
      },
      "source": [
        "# Ë∑ØÂæëËàáÂèÉÊï∏Â∑≤Âú®‰∏äÊñπ cell Ë®≠ÂÆöÔºõÊ≠§ cell ÂèØÁï•ÈÅéÊàñÂà™Èô§\n",
        "# Ëã•ÈúÄÊîπÊà™ÈªûÔºåË´ã‰øÆÊîπ‰∏äÊñπ CUTOFF = \"0900\" | \"0915\" | \"0930\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'IN_COLAB' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ColabÔºöË®≠ÁÇ∫ Drive ‰∏äÂ∞àÊ°à data ÁõÆÈåÑÔºõÊú¨Ê©üÔºöÂ∞àÊ°à data/\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m DATA_ROOT \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mIN_COLAB\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m MERGED_CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/Transcend/thesis/github_clone/Thesis-AutoGluon-TXF-Research/data/legacy/merged_for_autogluon/merged_for_autogluon0900.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ÊªæÂãïË®≠ÂÆöÔºöÂâç TRAIN_YEARS Âπ¥Ë®ìÁ∑¥ÔºåÈ†êÊ∏¨‰∏ã‰∏ÄÂπ¥\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'IN_COLAB' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwJIqJf4CHgp"
      },
      "source": [
        "## 3. ÂÆâË£ù AutoGluonÔºàColab ÈÄöÂ∏∏ÈúÄÂü∑Ë°å‰∏ÄÊ¨°Ôºâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd4N28sOCHgp",
        "outputId": "23bff688-f4c1-4aff-9c4d-d43eed4a3f4e"
      },
      "source": [
        "!pip3 install autogluon.tabular"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autogluon.tabular\n",
            "  Downloading autogluon_tabular-1.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2.4.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (2.0.2)\n",
            "Requirement already satisfied: scipy<1.17,>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (1.16.3)\n",
            "Requirement already satisfied: pandas<2.4.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (1.6.1)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular) (3.6.1)\n",
            "Collecting autogluon.core==1.5.0 (from autogluon.tabular)\n",
            "  Downloading autogluon_core-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting autogluon.features==1.5.0 (from autogluon.tabular)\n",
            "  Downloading autogluon_features-1.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.5.0->autogluon.tabular) (4.67.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.5.0->autogluon.tabular) (2.32.4)\n",
            "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.5.0->autogluon.tabular) (3.10.0)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.5.0->autogluon.tabular)\n",
            "  Downloading boto3-1.42.44-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting autogluon.common==1.5.0 (from autogluon.core==1.5.0->autogluon.tabular)\n",
            "  Downloading autogluon_common-1.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyarrow<21.0.0,>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.tabular) (18.1.0)\n",
            "Requirement already satisfied: psutil<7.2.0,>=5.7.3 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.tabular) (5.9.5)\n",
            "Requirement already satisfied: joblib<1.7,>=1.2 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.tabular) (1.5.3)\n",
            "Requirement already satisfied: pyyaml>=5.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.tabular) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.tabular) (3.6.0)\n",
            "Collecting botocore<1.43.0,>=1.42.44 (from boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.tabular)\n",
            "  Downloading botocore-1.42.44-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.tabular)\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.tabular)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.tabular) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.4.0,>=2.0.0->autogluon.tabular) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.5.0->autogluon.tabular) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.5.0->autogluon.tabular) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.5.0->autogluon.tabular) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.5.0->autogluon.tabular) (2026.1.4)\n",
            "Downloading autogluon_tabular-1.5.0-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon_core-1.5.0-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon_features-1.5.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon_common-1.5.0-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.42.44-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.44-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\n",
            "Successfully installed autogluon.common-1.5.0 autogluon.core-1.5.0 autogluon.features-1.5.0 autogluon.tabular-1.5.0 boto3-1.42.44 botocore-1.42.44 jmespath-1.1.0 s3transfer-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8rB07Y5CHgp"
      },
      "source": [
        "## 4. ËºâÂÖ•Ë≥áÊñôÔºàÂ∑≤ÁßªËá≥‰∏ãÊñπË®ìÁ∑¥ cellÔºå‰æù cutoff ÂàÜÂà•ËºâÂÖ•Ôºâ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCuP06qXCHgq",
        "outputId": "c26a801d-5e73-4f2c-de7a-38e51f6268ae"
      },
      "source": [
        "# Ë≥áÊñôËºâÂÖ•Â∑≤ÁßªËá≥‰∏ãÊñπ„ÄåÊªæÂãïË®ìÁ∑¥„ÄçcellÔºå‰æùÊØèÂÄã cutoff ÂàÜÂà•ËºâÂÖ•\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (2271, 52)\n",
            "Years in data: [np.int32(2013), np.int32(2014), np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023)]\n",
            "Predict years (train 2yr ‚Üí predict 1yr): [np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffkPpIQscZP"
      },
      "source": [
        "# Sharpe ÂõûÊ∏¨ÔºàËàá working/1129 intraday_macro_features_test sharpe.ipynb Áõ∏ÂêåÈÇèËºØÔºâ\n",
        "COST_PER_TRADE = 0.0005\n",
        "TRADE_THRESHOLD = 0.0001\n",
        "\n",
        "def compute_sharpe_backtest(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    positions = np.zeros_like(y_pred, dtype=float)\n",
        "    positions[y_pred > TRADE_THRESHOLD] = 1.0\n",
        "    positions[y_pred < -TRADE_THRESHOLD] = -1.0\n",
        "    strategy_returns = positions * y_true\n",
        "    trades = np.sum(np.abs(np.diff(np.insert(positions, 0, 0))) > 0)\n",
        "    total_cost = trades * COST_PER_TRADE\n",
        "    mean_return_with_cost = (np.sum(strategy_returns) - total_cost) / max(len(strategy_returns), 1)\n",
        "    std_return = np.std(strategy_returns)\n",
        "    if std_return == 0:\n",
        "        return 0.0\n",
        "    return float(mean_return_with_cost / (std_return + 1e-9) * np.sqrt(252))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkB-0M57scZP"
      },
      "source": [
        "## 5. ÊªæÂãïË®ìÁ∑¥Ôºö‰æùÂ∫è 0900‚Üí0915‚Üí0930Ôºå‰∏âÂπ¥Ë®ìÁ∑¥È†êÊ∏¨Á¨¨ÂõõÂπ¥ÔºåÂ∑≤Â≠òÂú®ÂâáË∑≥ÈÅé\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gkCcaWNXCHgq",
        "outputId": "325762f3-3e2c-4140-bac8-f08aadec7f19"
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "import shutil\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(DRIVE_PROJECT_ROOT) / \"data\" if IN_COLAB else Path(LOCAL_PROJECT_ROOT) / \"data\"\n",
        "\n",
        "HAS_GPU = torch.cuda.is_available()\n",
        "print(f\"Á≥ªÁµ±Ê™¢Êü•: GPU {'ÂèØÁî® ‚úÖ' if HAS_GPU else 'Êú™ÂÅµÊ∏¨Âà∞ ‚ö†Ô∏è (Â∞á‰ΩøÁî® CPU)'}\")\n",
        "print(f\"Ë®ìÁ∑¥Ë®≠ÂÆö: ‰æùÂ∫è {CUTOFFS}, ÊªæÂãïË¶ñÁ™ó={TRAIN_YEARS}Âπ¥‚ÜíÈ†êÊ∏¨Á¨¨ÂõõÂπ¥, ÈôêÊôÇ={TIME_LIMIT}Áßí\")\n",
        "print(f\"Êñ∑Á∑öÁ∫åË∑ë: Ëã• roll_YYYY ÂÖßÂ∑≤Êúâ predictions.csv ÂâáË∑≥ÈÅé\\n\")\n",
        "\n",
        "all_summary = []\n",
        "\n",
        "for cutoff in CUTOFFS:\n",
        "    merged_path = DATA_ROOT / \"autogluon_ready\" / cutoff / f\"merged_for_autogluon_{cutoff}.csv\"\n",
        "    if not merged_path.exists():\n",
        "        print(f\"‚è≠Ô∏è Ë∑≥ÈÅé {cutoff}: Ê™îÊ°à‰∏çÂ≠òÂú® {merged_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'#'*50}\")\n",
        "    print(f\"# Êà™Èªû {cutoff}\")\n",
        "    print(f\"{'#'*50}\")\n",
        "\n",
        "    df = pd.read_csv(merged_path)\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"]).drop(columns=[\"datetime\"], errors=\"ignore\")\n",
        "    df[\"year\"] = df[\"date\"].dt.year\n",
        "    df = df.dropna()\n",
        "    if LABEL not in df.columns:\n",
        "        print(f\"‚ùå {cutoff}: ÁÑ° {LABEL} Ê¨Ñ‰Ωç\")\n",
        "        continue\n",
        "\n",
        "    years = sorted(df[\"year\"].unique())\n",
        "    predict_years = [y for y in years if all((y - i) in years for i in range(1, TRAIN_YEARS + 1))]\n",
        "    predict_years = sorted(set(predict_years))\n",
        "    print(f\"  Ë≥áÊñô: {len(df)} Âàó, È†êÊ∏¨Âπ¥: {predict_years}\")\n",
        "\n",
        "    ROLL_OUTPUT = DATA_ROOT / \"models\" / cutoff\n",
        "    ROLL_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "    summary_list = []\n",
        "    per_year_reports = {}\n",
        "\n",
        "    for predict_year in predict_years:\n",
        "        required_train_years = range(predict_year - TRAIN_YEARS, predict_year)\n",
        "        path_roll = ROLL_OUTPUT / f\"roll_{predict_year}\"\n",
        "\n",
        "        if (path_roll / \"predictions.csv\").exists():\n",
        "            print(f\"  ‚è≠Ô∏è {predict_year}: Â∑≤Â≠òÂú®ÔºåË∑≥ÈÅéË®ìÁ∑¥\")\n",
        "            try:\n",
        "                with open(path_roll / \"metrics.json\") as f:\n",
        "                    m = json.load(f)\n",
        "                summary_list.append({\n",
        "                    \"cutoff\": cutoff, \"predict_year\": int(predict_year),\n",
        "                    \"train_period\": m.get(\"train_period\", \"\"),\n",
        "                    \"rmse\": m.get(\"rmse\"), \"sharpe\": m.get(\"sharpe\"),\n",
        "                    \"best_model\": m.get(\"best_model\", \"\"), \"model_path\": f\"roll_{predict_year}\", \"skipped\": True\n",
        "                })\n",
        "                lb = pd.read_csv(path_roll / \"leaderboard_with_metrics.csv\") if (path_roll / \"leaderboard_with_metrics.csv\").exists() else pd.DataFrame()\n",
        "                perf = pd.read_csv(path_roll / \"models_performance.csv\") if (path_roll / \"models_performance.csv\").exists() else pd.DataFrame()\n",
        "                fi = pd.read_csv(path_roll / \"feature_importance_all_models.csv\") if (path_roll / \"feature_importance_all_models.csv\").exists() else pd.DataFrame()\n",
        "                per_year_reports[int(predict_year)] = {\"leaderboard\": lb, \"model_performance\": perf, \"feature_importance\": fi}\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö†Ô∏è ËºâÂÖ•Êó¢ÊúâÁµêÊûúÂ§±Êïó: {e}\")\n",
        "            continue\n",
        "\n",
        "        missing_years = [y for y in required_train_years if y not in df[\"year\"].unique()]\n",
        "        if missing_years:\n",
        "            print(f\"  ‚ùå Ë∑≥ÈÅé {predict_year}: Áº∫Â∞ë {missing_years}\")\n",
        "            continue\n",
        "\n",
        "        train_df = df[df[\"year\"].isin(required_train_years)].copy()\n",
        "        test_df = df[df[\"year\"] == predict_year].copy()\n",
        "        train_ag = train_df.drop(columns=[\"date\", \"datetime\", \"year\"], errors=\"ignore\").dropna()\n",
        "        test_ag = test_df.drop(columns=[\"date\", \"datetime\", \"year\"], errors=\"ignore\").dropna()\n",
        "\n",
        "        if len(train_ag) < 50 or len(test_ag) < 10:\n",
        "            print(f\"  ‚ö†Ô∏è Ë∑≥ÈÅé {predict_year}: Ë≥áÊñôÈÅéÂ∞ë\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  üöÄ {predict_year}: Ë®ìÁ∑¥ {min(required_train_years)}-{max(required_train_years)} ‚Üí È†êÊ∏¨ {predict_year} ({len(train_ag)}/{len(test_ag)} Á≠Ü)\")\n",
        "\n",
        "        if 'predictor' in locals():\n",
        "            del predictor\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache() if HAS_GPU else None\n",
        "\n",
        "        path_roll.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            predictor = TabularPredictor(\n",
        "                label=LABEL, problem_type=\"regression\", eval_metric=\"rmse\", path=str(path_roll),\n",
        "            ).fit(\n",
        "                train_data=train_ag, time_limit=TIME_LIMIT, presets=\"best_quality\",\n",
        "                dynamic_stacking=True, ag_args_fit={'num_gpus': 1} if HAS_GPU else {'num_gpus': 0}\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Ë®ìÁ∑¥ÈåØË™§ {predict_year}: {e}\")\n",
        "            continue\n",
        "\n",
        "        preds = predictor.predict(test_ag)\n",
        "        y_true = test_ag[LABEL].values\n",
        "        y_pred = np.asarray(preds)\n",
        "        rmse = float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "        sharpe = compute_sharpe_backtest(y_true, y_pred)\n",
        "\n",
        "        lb = predictor.leaderboard(test_ag, silent=True)\n",
        "        model_names = lb[\"model\"].tolist()\n",
        "        model_perf_rows = []\n",
        "        fi_rows = []\n",
        "        for model_name in model_names:\n",
        "            try:\n",
        "                preds_m = np.asarray(predictor.predict(test_ag, model=model_name))\n",
        "                model_perf_rows.append({\n",
        "                    \"predict_year\": int(predict_year), \"train_period\": f\"{min(required_train_years)}-{max(required_train_years)}\",\n",
        "                    \"model\": model_name, \"rmse\": float(np.sqrt(np.mean((preds_m - y_true) ** 2))),\n",
        "                    \"sharpe\": compute_sharpe_backtest(y_true, preds_m),\n",
        "                })\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                fi_m = predictor.feature_importance(data=test_ag, model=model_name).reset_index().rename(columns={\"index\": \"feature\"})\n",
        "                fi_m[\"model\"] = model_name\n",
        "                fi_m[\"predict_year\"] = int(predict_year)\n",
        "                fi_rows.append(fi_m)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        df_model_perf = pd.DataFrame(model_perf_rows)\n",
        "        lb_with_metrics = lb.merge(df_model_perf, on=\"model\", how=\"left\")\n",
        "        df_fi_all = pd.concat(fi_rows, ignore_index=True) if fi_rows else pd.DataFrame()\n",
        "\n",
        "        lb.to_csv(path_roll / \"leaderboard.csv\", index=False)\n",
        "        lb_with_metrics.to_csv(path_roll / \"leaderboard_with_metrics.csv\", index=False)\n",
        "        df_model_perf.to_csv(path_roll / \"models_performance.csv\", index=False)\n",
        "        df_fi_all.to_csv(path_roll / \"feature_importance_all_models.csv\", index=False)\n",
        "\n",
        "        out_pred = test_df.loc[test_ag.index].copy()\n",
        "        out_pred[\"pred\"] = y_pred\n",
        "        out_pred[[\"date\", LABEL, \"pred\"]].to_csv(path_roll / \"predictions.csv\", index=False)\n",
        "\n",
        "        metrics = {\n",
        "            \"cutoff\": cutoff, \"predict_year\": int(predict_year),\n",
        "            \"train_period\": f\"{min(required_train_years)}-{max(required_train_years)}\",\n",
        "            \"rmse\": rmse, \"sharpe\": sharpe, \"best_model\": predictor.model_best,\n",
        "            \"num_models\": len(model_names), \"model_path\": f\"roll_{predict_year}\",\n",
        "        }\n",
        "        with open(path_roll / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        summary_list.append({\n",
        "            \"cutoff\": cutoff, \"predict_year\": int(predict_year),\n",
        "            \"train_period\": metrics[\"train_period\"], \"rmse\": rmse, \"sharpe\": sharpe,\n",
        "            \"best_model\": predictor.model_best, \"num_models\": len(model_names),\n",
        "            \"model_path\": f\"roll_{predict_year}\", \"skipped\": False\n",
        "        })\n",
        "        per_year_reports[int(predict_year)] = {\"leaderboard\": lb_with_metrics, \"model_performance\": df_model_perf, \"feature_importance\": df_fi_all}\n",
        "        print(f\"  üéâ ÂÆåÊàê {predict_year}: Sharpe={sharpe:.4f}, RMSE={rmse:.5f}, Best={predictor.model_best}\")\n",
        "\n",
        "    if summary_list:\n",
        "        df_summary = pd.DataFrame(summary_list)\n",
        "        summary_path = ROLL_OUTPUT / \"rolling_summary_final.csv\"\n",
        "        df_summary.to_csv(summary_path, index=False)\n",
        "        print(f\"\\n‚úÖ {cutoff} ÂÆåÊàêÔºåÁ∏ΩË°®: {summary_path}\")\n",
        "        all_summary.append(df_summary)\n",
        "\n",
        "        excel_path = ROLL_OUTPUT / \"rolling_models_by_year.xlsx\"\n",
        "        try:\n",
        "            with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
        "                for year in sorted(per_year_reports.keys()):\n",
        "                    report = per_year_reports[year]\n",
        "                    sheet_name = str(year)[:31]\n",
        "                    startrow = 0\n",
        "                    if not report[\"leaderboard\"].empty:\n",
        "                        pd.DataFrame({\"section\": [\"leaderboard_with_metrics\"]}).to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)\n",
        "                        startrow += 1\n",
        "                        report[\"leaderboard\"].to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)\n",
        "                        startrow += len(report[\"leaderboard\"]) + 2\n",
        "                    if not report[\"model_performance\"].empty:\n",
        "                        pd.DataFrame({\"section\": [\"model_performance\"]}).to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)\n",
        "                        startrow += 1\n",
        "                        report[\"model_performance\"].to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)\n",
        "                        startrow += len(report[\"model_performance\"]) + 2\n",
        "                    if not report[\"feature_importance\"].empty:\n",
        "                        pd.DataFrame({\"section\": [\"feature_importance_all_models\"]}).to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)\n",
        "                        startrow += 1\n",
        "                        report[\"feature_importance\"].to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)\n",
        "            print(f\"  Excel: {excel_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Excel Ëº∏Âá∫Â§±Êïó: {e}\")\n",
        "\n",
        "if all_summary:\n",
        "    combined = pd.concat(all_summary, ignore_index=True)\n",
        "    combined.to_csv(DATA_ROOT / \"models\" / \"rolling_summary_all_cutoffs.csv\", index=False)\n",
        "    print(f\"\\n‚úÖ ÂÖ®ÈÉ®ÂÆåÊàêÔºÅÂΩôÁ∏Ω: data/models/rolling_summary_all_cutoffs.csv\")\n",
        "    display(combined)\n",
        "else:\n",
        "    print(\"\\n‚ùå Ê≤íÊúâÁî¢Áîü‰ªª‰ΩïÁµêÊûúÔºåË´ãÊ™¢Êü•Ë≥áÊñôÊàñË∑ØÂæë„ÄÇ\")\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Á≥ªÁµ±Ê™¢Êü•: GPU Êú™ÂÅµÊ∏¨Âà∞ ‚ö†Ô∏è (Â∞á‰ΩøÁî® CPU)\n",
            "Ë®ìÁ∑¥Ë®≠ÂÆö: ÊªæÂãïË¶ñÁ™ó=2Âπ¥, È†êÊ∏¨=[np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023)], ÈôêÊôÇ=30Áßí\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2015\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 435 Á≠Ü (2013-2014), Ê∏¨Ë©¶ÈõÜ 202 Á≠Ü (2015)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2015_20260208_161232\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.42 GB / 12.67 GB (82.3%)\n",
            "Disk Space Avail:   81.73 GB / 107.72 GB (75.9%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2015_20260208_161232/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2015_20260208_161232/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    386\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10673.25 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.58s of the 6.86s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 296. Best iteration is:\n",
            "\t[26]\tvalid_set's rmse: 0.00555634\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.92s of the 3.20s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00567701\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.23s of the 2.51s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0053\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.57s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.88s of the -3.71s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.875, 'RandomForestMSE_BAG_L1': 0.125}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.88s of the -5.30s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.875, 'RandomForestMSE_BAG_L1': 0.125}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1151.0 rows/s (49 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2015_20260208_161232/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  RandomForestMSE_BAG_L1      -0.005575  -0.005324  root_mean_squared_error        0.180598       0.274351  4.573736                 0.180598                0.274351           4.573736            1       True          2\n",
            "1     WeightedEnsemble_L3      -0.005584  -0.005183  root_mean_squared_error        0.241188       0.282461  8.002612                 0.007549                0.000450           0.012971            3       True          4\n",
            "2     WeightedEnsemble_L2      -0.005584  -0.005183  root_mean_squared_error        0.244034       0.282663  8.008793                 0.010395                0.000652           0.019152            2       True          3\n",
            "3       LightGBMXT_BAG_L1      -0.005594  -0.005184  root_mean_squared_error        0.053042       0.007660  3.415906                 0.053042                0.007660           3.415906            1       True          1\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t14s\t = DyStack   runtime |\t16s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 16s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2015_20260208_161232\"\n",
            "Train Data Rows:    435\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10632.46 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 10.27s of the 15.39s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.68s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.36s of the 11.48s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 47. Best iteration is:\n",
            "\t[31]\tvalid_set's rmse: 0.00466094\n",
            "\tRan out of time, early stopping on iteration 187. Best iteration is:\n",
            "\t[23]\tvalid_set's rmse: 0.00510163\n",
            "\tRan out of time, early stopping on iteration 265. Best iteration is:\n",
            "\t[40]\tvalid_set's rmse: 0.00543898\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.07s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.06s of the 5.18s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.87s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 15.41s of the -0.35s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 15.41s of the -1.95s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.69s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6340.4 rows/s (55 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2015_20260208_161232\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 202 rows with 5 shuffle sets...\n",
            "\t14.3s\t= Expected runtime (2.86s per shuffle set)\n",
            "\t2.05s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2015: Sharpe=2.1658, RMSE=0.00795, Best=WeightedEnsemble_L2\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2016\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 421 Á≠Ü (2014-2015), Ê∏¨Ë©¶ÈõÜ 194 Á≠Ü (2016)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2016_20260208_161308\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.38 GB / 12.67 GB (81.9%)\n",
            "Disk Space Avail:   81.72 GB / 107.72 GB (75.9%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2016_20260208_161308/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2016_20260208_161308/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    374\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10642.13 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.56s of the 6.82s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00892581\n",
            "\tRan out of time, early stopping on iteration 55. Best iteration is:\n",
            "\t[52]\tvalid_set's rmse: 0.00735564\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00585278\n",
            "\tRan out of time, early stopping on iteration 134. Best iteration is:\n",
            "\t[9]\tvalid_set's rmse: 0.00559499\n",
            "\t-0.0067\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.72s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.58s of the 2.83s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.0089025\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.84s of the 0.79s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0067\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.71s of the 0.69s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00888565\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.85s of the -1.40s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0067\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 8.58s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3929.7 rows/s (47 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2016_20260208_161308/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0    LightGBMXT_BAG_L1      -0.004293  -0.006657  root_mean_squared_error        0.054549       0.011890  3.716933                 0.054549                0.011890           3.716933            1       True          1\n",
            "1  WeightedEnsemble_L3      -0.004293  -0.006657  root_mean_squared_error        0.060479       0.012336  3.722644                 0.005930                0.000446           0.005711            3       True          3\n",
            "2  WeightedEnsemble_L2      -0.004293  -0.006657  root_mean_squared_error        0.064323       0.012446  3.722660                 0.009773                0.000556           0.005727            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t11s\t = DyStack   runtime |\t19s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 19s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2016_20260208_161308\"\n",
            "Train Data Rows:    421\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10643.42 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.82s of the 19.21s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0063\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.96s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.58s of the 12.97s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0064\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1.94s of the 8.33s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0068\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.83s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.23s of the 2.25s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0063\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2.11s of the 2.05s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00574865\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 0.82s of the 0.77s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00577765\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 19.23s of the -1.50s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0063\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 21.14s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5568.0 rows/s (53 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2016_20260208_161308\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 194 rows with 5 shuffle sets...\n",
            "\t21.89s\t= Expected runtime (4.38s per shuffle set)\n",
            "\t2.12s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2016: Sharpe=-2.8726, RMSE=0.00811, Best=WeightedEnsemble_L2\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2017\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 396 Á≠Ü (2015-2016), Ê∏¨Ë©¶ÈõÜ 170 Á≠Ü (2017)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2017_20260208_161344\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.44 GB / 12.67 GB (82.4%)\n",
            "Disk Space Avail:   81.70 GB / 107.72 GB (75.9%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2017_20260208_161344/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2017_20260208_161344/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    352\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10690.78 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.09 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.58s of the 6.85s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 436. Best iteration is:\n",
            "\t[141]\tvalid_set's rmse: 0.00618033\n",
            "\t-0.0078\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.35s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1.00s of the 3.27s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00840076\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.33s of the 2.60s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0084\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.4s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.87s of the -4.59s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0078\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.87s of the -6.19s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0078\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.34s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5444.2 rows/s (44 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2017_20260208_161344/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0       LightGBMXT_BAG_L1      -0.005660  -0.007844  root_mean_squared_error        0.062793       0.008008  3.347299                 0.062793                0.008008           3.347299            1       True          1\n",
            "1     WeightedEnsemble_L3      -0.005660  -0.007844  root_mean_squared_error        0.069361       0.008533  3.359957                 0.006568                0.000524           0.012657            3       True          4\n",
            "2     WeightedEnsemble_L2      -0.005660  -0.007844  root_mean_squared_error        0.071481       0.008599  3.361053                 0.008688                0.000590           0.013753            2       True          3\n",
            "3  RandomForestMSE_BAG_L1      -0.005952  -0.008437  root_mean_squared_error        0.164237       0.140770  5.398286                 0.164237                0.140770           5.398286            1       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t15s\t = DyStack   runtime |\t15s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 15s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2017_20260208_161344\"\n",
            "Train Data Rows:    396\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10667.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 9.75s of the 14.61s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0079\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.48s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.06s of the 10.92s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 36. Best iteration is:\n",
            "\t[13]\tvalid_set's rmse: 0.0074456\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 4.49s of the 9.35s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0082\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.21s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 14.63s of the 2.47s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.857, 'RandomForestMSE_BAG_L1': 0.143}\n",
            "\t-0.0078\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2.35s of the 2.32s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00822337\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1.68s of the 1.65s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00819791\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 0.99s of the 0.96s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.008\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.23s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 14.63s of the -4.75s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'RandomForestMSE_BAG_L2': 0.25}\n",
            "\t-0.0078\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.71s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1164.1 rows/s (50 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2017_20260208_161344\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 170 rows with 5 shuffle sets...\n",
            "\t73.09s\t= Expected runtime (14.62s per shuffle set)\n",
            "\t4.01s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2017: Sharpe=-5.8152, RMSE=0.00438, Best=WeightedEnsemble_L3\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2018\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 364 Á≠Ü (2016-2017), Ê∏¨Ë©¶ÈõÜ 213 Á≠Ü (2018)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2018_20260208_161427\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.45 GB / 12.67 GB (82.5%)\n",
            "Disk Space Avail:   81.68 GB / 107.72 GB (75.8%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2018_20260208_161427/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2018_20260208_161427/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    323\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10703.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.12 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.57s of the 6.84s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0058\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.35s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.98s of the 3.26s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00716369\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.29s of the 2.57s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0062\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.67s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.86s of the -3.81s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0058\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.86s of the -5.38s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0058\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.56s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4832.3 rows/s (41 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2018_20260208_161427/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  RandomForestMSE_BAG_L1      -0.007718  -0.006244  root_mean_squared_error        0.161791       0.251901  4.671545                 0.161791                0.251901           4.671545            1       True          2\n",
            "1       LightGBMXT_BAG_L1      -0.008041  -0.005810  root_mean_squared_error        0.084411       0.008360  3.353442                 0.084411                0.008360           3.353442            1       True          1\n",
            "2     WeightedEnsemble_L3      -0.008041  -0.005810  root_mean_squared_error        0.090292       0.008834  3.368490                 0.005881                0.000474           0.015048            3       True          4\n",
            "3     WeightedEnsemble_L2      -0.008041  -0.005810  root_mean_squared_error        0.090298       0.009340  3.372187                 0.005887                0.000980           0.018744            2       True          3\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t14s\t = DyStack   runtime |\t16s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 16s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2018_20260208_161427\"\n",
            "Train Data Rows:    364\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10683.83 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.09 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 10.25s of the 15.35s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0061\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.44s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.54s of the 11.65s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 73. Best iteration is:\n",
            "\t[34]\tvalid_set's rmse: 0.00587092\n",
            "\tRan out of time, early stopping on iteration 230. Best iteration is:\n",
            "\t[39]\tvalid_set's rmse: 0.00372866\n",
            "\tRan out of time, early stopping on iteration 147. Best iteration is:\n",
            "\t[19]\tvalid_set's rmse: 0.00464902\n",
            "\t-0.0061\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.7s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.59s of the 5.70s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0065\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.55s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 15.38s of the 0.43s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-0.0061\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.32s of the 0.28s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00740381\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 15.38s of the -1.76s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t-0.0061\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4631.1 rows/s (46 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2018_20260208_161427\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 213 rows with 5 shuffle sets...\n",
            "\t13.7s\t= Expected runtime (2.74s per shuffle set)\n",
            "\t1.18s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2018: Sharpe=-4.6960, RMSE=0.00648, Best=WeightedEnsemble_L2\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2019\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 383 Á≠Ü (2017-2018), Ê∏¨Ë©¶ÈõÜ 215 Á≠Ü (2019)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2019_20260208_161502\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.44 GB / 12.67 GB (82.4%)\n",
            "Disk Space Avail:   81.67 GB / 107.72 GB (75.8%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2019_20260208_161502/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2019_20260208_161502/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    340\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10690.19 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.09 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.56s of the 6.83s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 23. Best iteration is:\n",
            "\t[23]\tvalid_set's rmse: 0.0046799\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L1.\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3.60s of the 5.86s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00487018\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2.65s of the 4.91s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.35s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.85s of the -0.11s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.85s of the -1.62s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 8.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2427.6 rows/s (340 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2019_20260208_161502/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  RandomForestMSE_BAG_L1      -0.005051  -0.005362  root_mean_squared_error        0.159521       0.139560  3.349183                 0.159521                0.139560           3.349183            1       True          1\n",
            "1     WeightedEnsemble_L3      -0.005051  -0.005362  root_mean_squared_error        0.167009       0.140087  3.356497                 0.007488                0.000528           0.007314            3       True          3\n",
            "2     WeightedEnsemble_L2      -0.005051  -0.005362  root_mean_squared_error        0.168338       0.140055  3.355326                 0.008816                0.000495           0.006144            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t11s\t = DyStack   runtime |\t19s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 19s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2019_20260208_161502\"\n",
            "Train Data Rows:    383\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10680.86 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.57s of the 18.85s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.56s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.75s of the 13.02s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0053\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.06s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2.51s of the 8.78s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0053\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.67s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 18.87s of the 3.43s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.75, 'RandomForestMSE_BAG_L1': 0.25}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3.35s of the 3.30s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00695215\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 2.50s of the 2.44s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00693534\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1.64s of the 1.58s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0053\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.9s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 18.87s of the -5.06s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.667, 'RandomForestMSE_BAG_L2': 0.333}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 24.24s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1086.1 rows/s (48 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2019_20260208_161502\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 215 rows with 5 shuffle sets...\n",
            "\t80.81s\t= Expected runtime (16.16s per shuffle set)\n",
            "\t5.17s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2019: Sharpe=-3.0155, RMSE=0.00463, Best=WeightedEnsemble_L3\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2020\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 428 Á≠Ü (2018-2019), Ê∏¨Ë©¶ÈõÜ 202 Á≠Ü (2020)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2020_20260208_161545\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.43 GB / 12.67 GB (82.3%)\n",
            "Disk Space Avail:   81.65 GB / 107.72 GB (75.8%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2020_20260208_161545/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2020_20260208_161545/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    380\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10670.35 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.54s of the 6.78s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 43. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00667949\n",
            "\tRan out of time, early stopping on iteration 99. Best iteration is:\n",
            "\t[44]\tvalid_set's rmse: 0.00510961\n",
            "\tRan out of time, early stopping on iteration 140. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00487142\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.62s of the 2.87s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00669933\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.82s of the 0.26s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.17s of the 0.15s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00443414\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.82s of the -2.02s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 9.25s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3974.3 rows/s (48 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2020_20260208_161545/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0    LightGBMXT_BAG_L1      -0.005801  -0.005188  root_mean_squared_error        0.073380       0.011766  3.640590                 0.073380                0.011766           3.640590            1       True          1\n",
            "1  WeightedEnsemble_L3      -0.005801  -0.005188  root_mean_squared_error        0.079970       0.012422  3.646388                 0.006590                0.000656           0.005797            3       True          3\n",
            "2  WeightedEnsemble_L2      -0.005801  -0.005188  root_mean_squared_error        0.080791       0.014234  3.649641                 0.007411                0.002468           0.009051            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t11s\t = DyStack   runtime |\t19s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 19s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2020_20260208_161545\"\n",
            "Train Data Rows:    428\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10699.40 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.34s of the 18.49s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.16s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5.75s of the 11.90s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 475. Best iteration is:\n",
            "\t[471]\tvalid_set's rmse: 0.00635886\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.45s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1.09s of the 7.25s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.01s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 18.52s of the 0.47s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.667, 'RandomForestMSE_BAG_L1': 0.333}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.37s of the 0.31s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00492344\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 18.52s of the -2.05s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.667, 'RandomForestMSE_BAG_L1': 0.333}\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 20.93s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1278.9 rows/s (54 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2020_20260208_161545\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 202 rows with 5 shuffle sets...\n",
            "\t52.95s\t= Expected runtime (10.59s per shuffle set)\n",
            "\t4.07s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2020: Sharpe=-0.4200, RMSE=0.01014, Best=WeightedEnsemble_L2\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2021\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 417 Á≠Ü (2019-2020), Ê∏¨Ë©¶ÈõÜ 206 Á≠Ü (2021)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2021_20260208_161624\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.43 GB / 12.67 GB (82.3%)\n",
            "Disk Space Avail:   81.64 GB / 107.72 GB (75.8%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2021_20260208_161624/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2021_20260208_161624/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    370\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10681.03 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.56s of the 6.82s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 242. Best iteration is:\n",
            "\t[33]\tvalid_set's rmse: 0.00792488\n",
            "\tRan out of time, early stopping on iteration 85. Best iteration is:\n",
            "\t[8]\tvalid_set's rmse: 0.00771135\n",
            "\tRan out of time, early stopping on iteration 141. Best iteration is:\n",
            "\t[2]\tvalid_set's rmse: 0.00511379\n",
            "\tRan out of time, early stopping on iteration 2. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.0092571\n",
            "\tRan out of time, early stopping on iteration 2. Best iteration is:\n",
            "\t[2]\tvalid_set's rmse: 0.00497754\n",
            "\t-0.0076\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.32s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.85s of the 0.94s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0076\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.83s of the 0.80s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00566537\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 0.15s of the 0.12s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00565733\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.85s of the -1.91s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0076\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 9.09s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3632.3 rows/s (47 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2021_20260208_161624/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0    LightGBMXT_BAG_L1      -0.006772  -0.007634  root_mean_squared_error        0.084956       0.012868  4.316656                 0.084956                0.012868           4.316656            1       True          1\n",
            "1  WeightedEnsemble_L3      -0.006772  -0.007634  root_mean_squared_error        0.089733       0.013653  4.323319                 0.004777                0.000785           0.006663            3       True          3\n",
            "2  WeightedEnsemble_L2      -0.006772  -0.007634  root_mean_squared_error        0.094519       0.013429  4.322674                 0.009563                0.000561           0.006018            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t11s\t = DyStack   runtime |\t19s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 19s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2021_20260208_161624\"\n",
            "Train Data Rows:    417\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10702.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.70s of the 19.03s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0075\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 8.97s of the 15.30s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 133. Best iteration is:\n",
            "\t[42]\tvalid_set's rmse: 0.00650732\n",
            "\tRan out of time, early stopping on iteration 79. Best iteration is:\n",
            "\t[6]\tvalid_set's rmse: 0.00579102\n",
            "\tRan out of time, early stopping on iteration 358. Best iteration is:\n",
            "\t[120]\tvalid_set's rmse: 0.00739374\n",
            "\t-0.0075\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2.21s of the 8.54s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0087\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.2s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.07s of the 2.57s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0075\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2.47s of the 2.44s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00632726\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1.80s of the 1.77s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00630701\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1.12s of the 1.08s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.008\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.56s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 19.07s of the -7.37s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0075\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 26.75s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6839.9 rows/s (53 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2021_20260208_161624\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 206 rows with 5 shuffle sets...\n",
            "\t16.97s\t= Expected runtime (3.39s per shuffle set)\n",
            "\t1.41s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2021: Sharpe=-0.7075, RMSE=0.00868, Best=WeightedEnsemble_L2\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2022\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 408 Á≠Ü (2020-2021), Ê∏¨Ë©¶ÈõÜ 222 Á≠Ü (2022)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2022_20260208_161705\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.45 GB / 12.67 GB (82.5%)\n",
            "Disk Space Avail:   81.61 GB / 107.72 GB (75.8%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2022_20260208_161705/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2022_20260208_161705/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    362\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10699.78 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.09 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.56s of the 6.83s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0093\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.48s of the 2.74s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00685668\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.85s of the 0.74s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0093\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.64s of the 0.60s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00908993\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.85s of the -1.94s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0093\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 9.13s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5002.5 rows/s (46 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2022_20260208_161705/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0    LightGBMXT_BAG_L1      -0.009534  -0.009308  root_mean_squared_error        0.052798       0.009061  3.877645                 0.052798                0.009061           3.877645            1       True          1\n",
            "1  WeightedEnsemble_L3      -0.009534  -0.009308  root_mean_squared_error        0.059538       0.009626  3.883133                 0.006740                0.000565           0.005488            3       True          3\n",
            "2  WeightedEnsemble_L2      -0.009534  -0.009308  root_mean_squared_error        0.060261       0.010118  3.886523                 0.007463                0.001057           0.008878            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t11s\t = DyStack   runtime |\t19s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 19s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2022_20260208_161705\"\n",
            "Train Data Rows:    408\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10702.40 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.61s of the 18.91s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0092\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.62s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 8.74s of the 15.03s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 136. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.0111667\n",
            "\tRan out of time, early stopping on iteration 107. Best iteration is:\n",
            "\t[106]\tvalid_set's rmse: 0.00883197\n",
            "\t-0.0092\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.48s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1.99s of the 8.28s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0104\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.26s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 18.93s of the 2.27s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.5 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0092\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2.15s of the 2.11s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00803909\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1.47s of the 1.43s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00803689\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 0.78s of the 0.74s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0098\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.34s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 18.93s of the -7.45s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0092\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 26.72s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3350.4 rows/s (51 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2022_20260208_161705\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 222 rows with 5 shuffle sets...\n",
            "\t16.21s\t= Expected runtime (3.24s per shuffle set)\n",
            "\t2.35s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2022: Sharpe=-0.4351, RMSE=0.00783, Best=WeightedEnsemble_L2\n",
            "\n",
            "========================================\n",
            "üöÄ ÈñãÂßãËôïÁêÜÂπ¥‰ªΩ: 2023\n",
            "========================================\n",
            "üìä Êï∏ÊìöÁµ±Ë®à: Ë®ìÁ∑¥ÈõÜ 428 Á≠Ü (2021-2022), Ê∏¨Ë©¶ÈõÜ 212 Á≠Ü (2023)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2023_20260208_161748\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       10.45 GB / 12.67 GB (82.4%)\n",
            "Disk Space Avail:   81.59 GB / 107.72 GB (75.7%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2023_20260208_161748/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 7s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2023_20260208_161748/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    380\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10696.45 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.54s of the 6.76s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 233. Best iteration is:\n",
            "\t[36]\tvalid_set's rmse: 0.00829448\n",
            "\t-0.0081\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.56s of the 2.79s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00857805\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 6.82s of the 0.79s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0081\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 0.67s of the 0.64s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00818287\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 6.82s of the -1.89s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0081\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 9.09s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4788.3 rows/s (48 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2023_20260208_161748/ds_sub_fit/sub_fit_ho\")\n",
            "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0    LightGBMXT_BAG_L1       -0.00692  -0.008054  root_mean_squared_error        0.063404       0.009925  3.707571                 0.063404                0.009925           3.707571            1       True          1\n",
            "1  WeightedEnsemble_L2       -0.00692  -0.008054  root_mean_squared_error        0.069961       0.010715  3.715484                 0.006557                0.000790           0.007913            2       True          2\n",
            "2  WeightedEnsemble_L3       -0.00692  -0.008054  root_mean_squared_error        0.071716       0.010455  3.713394                 0.008312                0.000530           0.005823            3       True          3\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t11s\t = DyStack   runtime |\t19s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 19s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2023_20260208_161748\"\n",
            "Train Data Rows:    428\n",
            "Train Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10669.78 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.61s of the 18.91s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\t-0.0079\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 8.68s of the 14.97s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 285. Best iteration is:\n",
            "\t[6]\tvalid_set's rmse: 0.0090084\n",
            "\t-0.008\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.91s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1.50s of the 7.79s of remaining time.\n",
            "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=2, gpus=0\n",
            "\t-0.0083\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.16s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 18.93s of the 1.67s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0079\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1.57s of the 1.53s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00836454\n",
            "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 0.84s of the 0.80s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=1, gpus=0)\n",
            "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's rmse: 0.00839333\n",
            "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 18.93s of the -1.32s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.4 GB\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t-0.0079\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 20.58s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6918.3 rows/s (54 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/roll_2023_20260208_161748\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "Computing feature importance via permutation shuffling for 34 features using 212 rows with 5 shuffle sets...\n",
            "\t17.56s\t= Expected runtime (3.51s per shuffle set)\n",
            "\t1.59s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "‚úÖ ÁâπÂæµÈáçË¶ÅÊÄßÂ∑≤Ëº∏Âá∫\n",
            "üéâ ÂÆåÊàê 2023: Sharpe=-1.8960, RMSE=0.00518, Best=WeightedEnsemble_L2\n",
            "\n",
            "‚úÖ ÊâÄÊúâÂπ¥‰ªΩÊªæÂãïË®ìÁ∑¥ÂÆåÊàêÔºÅÁ∏ΩË°®Â∑≤Â≠òËá≥: /content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/models/rolling_summary_final.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_summary\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"predict_year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2015,\n        \"max\": 2023,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          2022,\n          2016,\n          2020\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_period\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"2020-2021\",\n          \"2014-2015\",\n          \"2018-2019\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0019870191942687548,\n        \"min\": 0.004380230836025958,\n        \"max\": 0.010140388864709375,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.007832691183271609,\n          0.008112794323300662,\n          0.010140388864709375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sharpe\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.4389456147979227,\n        \"min\": -5.815241851660511,\n        \"max\": 2.1658341737678177,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          -0.43505019973016235,\n          -2.872606062011071,\n          -0.41995828263468915\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"WeightedEnsemble_L3\",\n          \"WeightedEnsemble_L2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"roll_2022_20260208_161705\",\n          \"roll_2016_20260208_161308\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_summary"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3d17ec06-01ee-485f-ac21-38c97abbd1b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict_year</th>\n",
              "      <th>train_period</th>\n",
              "      <th>rmse</th>\n",
              "      <th>sharpe</th>\n",
              "      <th>best_model</th>\n",
              "      <th>model_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015</td>\n",
              "      <td>2013-2014</td>\n",
              "      <td>0.007954</td>\n",
              "      <td>2.165834</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2015_20260208_161232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016</td>\n",
              "      <td>2014-2015</td>\n",
              "      <td>0.008113</td>\n",
              "      <td>-2.872606</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2016_20260208_161308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>2015-2016</td>\n",
              "      <td>0.004380</td>\n",
              "      <td>-5.815242</td>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>roll_2017_20260208_161344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018</td>\n",
              "      <td>2016-2017</td>\n",
              "      <td>0.006476</td>\n",
              "      <td>-4.696000</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2018_20260208_161427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019</td>\n",
              "      <td>2017-2018</td>\n",
              "      <td>0.004631</td>\n",
              "      <td>-3.015459</td>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>roll_2019_20260208_161502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020</td>\n",
              "      <td>2018-2019</td>\n",
              "      <td>0.010140</td>\n",
              "      <td>-0.419958</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2020_20260208_161545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021</td>\n",
              "      <td>2019-2020</td>\n",
              "      <td>0.008684</td>\n",
              "      <td>-0.707520</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2021_20260208_161624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2022</td>\n",
              "      <td>2020-2021</td>\n",
              "      <td>0.007833</td>\n",
              "      <td>-0.435050</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2022_20260208_161705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023</td>\n",
              "      <td>2021-2022</td>\n",
              "      <td>0.005180</td>\n",
              "      <td>-1.895955</td>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>roll_2023_20260208_161748</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d17ec06-01ee-485f-ac21-38c97abbd1b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d17ec06-01ee-485f-ac21-38c97abbd1b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d17ec06-01ee-485f-ac21-38c97abbd1b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_4e8662ed-3e89-46cd-893e-d632cac2ac89\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_summary')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4e8662ed-3e89-46cd-893e-d632cac2ac89 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_summary');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   predict_year train_period      rmse    sharpe           best_model  \\\n",
              "0          2015    2013-2014  0.007954  2.165834  WeightedEnsemble_L2   \n",
              "1          2016    2014-2015  0.008113 -2.872606  WeightedEnsemble_L2   \n",
              "2          2017    2015-2016  0.004380 -5.815242  WeightedEnsemble_L3   \n",
              "3          2018    2016-2017  0.006476 -4.696000  WeightedEnsemble_L2   \n",
              "4          2019    2017-2018  0.004631 -3.015459  WeightedEnsemble_L3   \n",
              "5          2020    2018-2019  0.010140 -0.419958  WeightedEnsemble_L2   \n",
              "6          2021    2019-2020  0.008684 -0.707520  WeightedEnsemble_L2   \n",
              "7          2022    2020-2021  0.007833 -0.435050  WeightedEnsemble_L2   \n",
              "8          2023    2021-2022  0.005180 -1.895955  WeightedEnsemble_L2   \n",
              "\n",
              "                  model_path  \n",
              "0  roll_2015_20260208_161232  \n",
              "1  roll_2016_20260208_161308  \n",
              "2  roll_2017_20260208_161344  \n",
              "3  roll_2018_20260208_161427  \n",
              "4  roll_2019_20260208_161502  \n",
              "5  roll_2020_20260208_161545  \n",
              "6  roll_2021_20260208_161624  \n",
              "7  roll_2022_20260208_161705  \n",
              "8  roll_2023_20260208_161748  "
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMS_Th3bCHgq"
      },
      "source": [
        "## 6. ÂΩôÁ∏ΩË°®ÔºàÊï∏ÂÄºËº∏Âá∫‰æõÊú¨Âú∞ÂàÜÊûêÔºâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSMPs8g8CHgq",
        "outputId": "8f50d1b6-7909-4feb-a060-e6fb7f60f85d"
      },
      "source": [
        "# Ëº∏Âá∫Ôºödata/models/{0900,0915,0930}/roll_YYYY/ Âê´ predictions.csv„ÄÅmetrics.json„ÄÅleaderboard Á≠â\n",
        "# ÂΩôÁ∏ΩË°®Ë¶ã‰∏äÊñπË®ìÁ∑¥ cell Ëº∏Âá∫ÔºåÊàñ data/models/rolling_summary_all_cutoffs.csv\n",
        "summary_path = Path(DRIVE_PROJECT_ROOT if IN_COLAB else LOCAL_PROJECT_ROOT) / \"data\" / \"models\" / \"rolling_summary_all_cutoffs.csv\"\n",
        "if summary_path.exists():\n",
        "    display(pd.read_csv(summary_path))\n",
        "else:\n",
        "    print(\"Â∞öÊú™Áî¢ÁîüÂΩôÁ∏ΩË°®ÔºåË´ãÂÖàÂü∑Ë°å‰∏äÊñπÊªæÂãïË®ìÁ∑¥ cell„ÄÇ\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No rolls completed. Check predict_years and data.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}