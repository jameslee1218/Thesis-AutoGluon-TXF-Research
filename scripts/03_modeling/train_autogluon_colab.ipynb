{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Y4IBghCHgk"
      },
      "source": [
        "# AutoGluon 滾動訓練（前兩年訓練、預測第三年）\n",
        "\n",
        "- **輸入**：`merged_for_autogluon_0900.csv`（由 `merge_and_train.py` 產出）。\n",
        "- **流程**：依「年」滾動 — 每年用**前兩年**資料訓練，預測**第三年**；輸出模型、完整 leaderboard、每年各模型表現（RMSE/Sharpe）、回測 Sharpe（與 working/1129 相同邏輯）到 `data/models/`，供本地分析。\n",
        "- **Colab**：掛載 Drive 後將 `DATA_ROOT` 設為專案 `data/`；**本機**：設為專案 `data/` 或指定 `MERGED_CSV_PATH`。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3I6AI3pCHgm"
      },
      "source": [
        "## 1. 掛載 Google Drive（Colab 必跑；本機可略）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 20557,
          "status": "ok",
          "timestamp": 1770550531199,
          "user": {
            "displayName": "李仲翔",
            "userId": "12877791582067812531"
          },
          "user_tz": -480
        },
        "id": "RsnOGcy5CHgn",
        "outputId": "20575ba7-bd89-4b7e-8443-05b9a27fdeaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Colab: True\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "print(\"Colab:\", IN_COLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjGjh9cVCHgo"
      },
      "source": [
        "## 2. 路徑與參數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 179,
          "status": "ok",
          "timestamp": 1770550720978,
          "user": {
            "displayName": "李仲翔",
            "userId": "12877791582067812531"
          },
          "user_tz": -480
        },
        "id": "aNemA6jsCHgo",
        "outputId": "14601ee7-c0e6-4a94-901a-3d173f612034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MERGED_CSV_PATH: /content/drive/MyDrive/2026/論文/Thesis-AutoGluon-TXF-Research/data/merged_for_autogluon_0900/merged_for_autogluon_0900.csv\n",
            "MODEL_SAVE_DIR: /content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/output_0900/models/autogluon_merged\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Colab：設為 Drive 上專案 data 目錄；本機：專案 data/\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data\") if IN_COLAB else Path.cwd().resolve().parent.parent / \"data\"\n",
        "MERGED_CSV_PATH = \"/content/drive/MyDrive/2026/論文/Thesis-AutoGluon-TXF-Research/data/merged_for_autogluon_0900/merged_for_autogluon_0900.csv\" if MERGED_CSV_PATH is None:\n",
        "MERGED_CSV_PATH = DATA_ROOT / \"output_0900\" / \"merged_for_autogluon_0900\" / \"merged_for_autogluon_0900.csv\"\n",
        "MERGED_CSV_PATH = Path(MERGED_CSV_PATH)\n",
        "\n",
        "# 滾動設定：前 TRAIN_YEARS 年訓練，預測下一年\n",
        "TRAIN_YEARS = 2\n",
        "LABEL = \"target_return\"\n",
        "TIME_LIMIT = 600  # 秒；每段訓練時間\n",
        "ROLL_OUTPUT = DATA_ROOT / \"models\"  # 每段存 data/models/roll_YYYY/\n",
        "ROLL_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"MERGED_CSV_PATH:\", MERGED_CSV_PATH)\n",
        "print(\"ROLL_OUTPUT:\", ROLL_OUTPUT)\n",
        "print(\"TRAIN_YEARS:\", TRAIN_YEARS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwJIqJf4CHgp"
      },
      "source": [
        "## 3. 安裝 AutoGluon（Colab 通常需執行一次）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 19978,
          "status": "ok",
          "timestamp": 1770550744832,
          "user": {
            "displayName": "李仲翔",
            "userId": "12877791582067812531"
          },
          "user_tz": -480
        },
        "id": "Fd4N28sOCHgp",
        "outputId": "8d92d581-ab89-4f05-d347-b066c32ca9d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/515.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/515.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install autogluon.tabular --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8rB07Y5CHgp"
      },
      "source": [
        "## 4. 載入資料、去 date、dropna、切分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 804,
          "status": "ok",
          "timestamp": 1770550783597,
          "user": {
            "displayName": "李仲翔",
            "userId": "12877791582067812531"
          },
          "user_tz": -480
        },
        "id": "FCuP06qXCHgq",
        "outputId": "a63e0186-3baa-451d-a1f3-bd098951bcb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after drop date + dropna: (2271, 50)\n",
            "Train: 1362, Val: 454, Test: 455\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(MERGED_CSV_PATH)\n",
        "# 保留 date 以便依年切分\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"date\"]).drop(columns=[\"datetime\"], errors=\"ignore\")\n",
        "df[\"year\"] = df[\"date\"].dt.year\n",
        "df = df.dropna()\n",
        "if LABEL not in df.columns:\n",
        "    raise ValueError(f\"No column '{LABEL}' in CSV.\")\n",
        "\n",
        "years = sorted(df[\"year\"].unique())\n",
        "# 預測年：至少前面有 TRAIN_YEARS 年可訓練\n",
        "predict_years = [y for y in years if y > years[0] and (y - years[0]) >= TRAIN_YEARS]\n",
        "# 若依連續年算：預測年 y 需存在 y-2, y-1\n",
        "predict_years = [y for y in years if (y - 1) in years and (y - 2) in years]\n",
        "predict_years = sorted(set(predict_years))\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Years in data:\", years)\n",
        "print(\"Predict years (train 2yr → predict 1yr):\", predict_years)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sharpe 回測（與 working/1129 intraday_macro_features_test sharpe.ipynb 相同邏輯）\n",
        "COST_PER_TRADE = 0.0005\n",
        "TRADE_THRESHOLD = 0.0001\n",
        "\n",
        "def compute_sharpe_backtest(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    positions = np.zeros_like(y_pred, dtype=float)\n",
        "    positions[y_pred > TRADE_THRESHOLD] = 1.0\n",
        "    positions[y_pred < -TRADE_THRESHOLD] = -1.0\n",
        "    strategy_returns = positions * y_true\n",
        "    trades = np.sum(np.abs(np.diff(np.insert(positions, 0, 0))) > 0)\n",
        "    total_cost = trades * COST_PER_TRADE\n",
        "    mean_return_with_cost = (np.sum(strategy_returns) - total_cost) / max(len(strategy_returns), 1)\n",
        "    std_return = np.std(strategy_returns)\n",
        "    if std_return == 0:\n",
        "        return 0.0\n",
        "    return float(mean_return_with_cost / (std_return + 1e-9) * np.sqrt(252))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 滾動訓練：每段「前兩年訓練 → 預測第三年」並輸出到 data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkCcaWNXCHgq"
      },
      "source": [
        "（執行下方一格即完成滾動訓練與輸出）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1898,
          "status": "error",
          "timestamp": 1770550953765,
          "user": {
            "displayName": "李仲翔",
            "userId": "12877791582067812531"
          },
          "user_tz": -480
        },
        "id": "RwBDokhCCHgq",
        "outputId": "4c23b82e-853f-40af-877c-b680231b5cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/output_0900/models/autogluon_merged\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       11.35 GB / 12.67 GB (89.6%)\n",
            "Disk Space Avail:   82.12 GB / 107.72 GB (76.2%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Using hyperparameters preset: hyperparameters='zeroshot'\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
            "/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
            "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
            "\t\tContext path: \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/output_0900/models/autogluon_merged/ds_sub_fit/sub_fit_ho\"\n",
            "Running DyStack sub-fit ...\n",
            "Beginning AutoGluon training ... Time limit = 150s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/Thesis-AutoGluon-TXF-Research/data/output_0900/models/autogluon_merged/ds_sub_fit/sub_fit_ho\"\n",
            "Train Data Rows:    1210\n",
            "Train Data Columns: 49\n",
            "Tuning Data Rows:    454\n",
            "Tuning Data Columns: 49\n",
            "Label Column:       target_return\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11623.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.62 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 15): ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', 'MACD_compressed_0_median', 'STOCHF_compressed_0_min', 'STOCHF_compressed_0_max', 'STOCHF_compressed_0_median', 'STOCHRSI_compressed_0_min', 'STOCHRSI_compressed_0_max', 'STOCHRSI_compressed_0_median', 'STOCH_compressed_0_min', 'STOCH_compressed_0_max', 'STOCH_compressed_0_median']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 15 | ['BBANDS_compressed_0_min', 'BBANDS_compressed_0_max', 'BBANDS_compressed_0_median', 'MACD_compressed_0_min', 'MACD_compressed_0_max', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['open_mean', 'open_std', 'high_std', 'low_mean', 'low_std', ...]\n",
            "\t\t('int', [])   :  7 | ['open_min', 'high_min', 'low_min', 'low_max', 'close_min', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.43 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Warning: Exception encountered during DyStack sub-fit:\n",
            "\tX_val, y_val is not None, but bagged mode was specified. If calling from `TabularPredictor.fit()`, `tuning_data` should be None.\n",
            "Default bagged mode does not use tuning data / validation data. Instead, all data (`train_data` and `tuning_data`) should be combined and specified as `train_data`.\n",
            "To avoid this error and use `tuning_data` as holdout data in bagged mode, specify the following:\n",
            "\tpredictor.fit(..., tuning_data=tuning_data, use_bag_holdout=True)\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t1s\t = DyStack   runtime |\t599s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Learner is already fit.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1119165447.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rmse\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIME_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/common/utils/decorators.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save predictor to disk to enable prediction and training after interrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_post_fit_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/autogluon/tabular/learner/abstract_learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learner is already fit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Learner is already fit."
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "summary_list = []\n",
        "for predict_year in predict_years:\n",
        "    train_df = df[(df[\"year\"] >= predict_year - TRAIN_YEARS) & (df[\"year\"] < predict_year)].copy()\n",
        "    test_df = df[df[\"year\"] == predict_year].copy()\n",
        "    train_ag = train_df.drop(columns=[\"date\", \"datetime\", \"year\"], errors=\"ignore\").dropna()\n",
        "    test_ag = test_df.drop(columns=[\"date\", \"datetime\", \"year\"], errors=\"ignore\").dropna()\n",
        "    if len(train_ag) < 20 or len(test_ag) < 5:\n",
        "        print(f\"Skip {predict_year}: train={len(train_ag)}, test={len(test_ag)}\")\n",
        "        continue\n",
        "\n",
        "    path_roll = ROLL_OUTPUT / f\"roll_{predict_year}\"\n",
        "    if path_roll.exists():\n",
        "        shutil.rmtree(path_roll)\n",
        "    path_roll.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    predictor = TabularPredictor(\n",
        "        label=LABEL,\n",
        "        problem_type=\"regression\",\n",
        "        eval_metric=\"rmse\",\n",
        "        path=str(path_roll),\n",
        "    ).fit(\n",
        "        train_ag,\n",
        "        time_limit=TIME_LIMIT,\n",
        "        presets=\"best_quality\",\n",
        "        dynamic_stacking=False,\n",
        "        num_stack_levels=1,\n",
        "    )\n",
        "\n",
        "    preds = predictor.predict(test_ag)\n",
        "    rmse = float(np.sqrt(np.mean((np.asarray(preds) - test_ag[LABEL].values) ** 2)))\n",
        "    sharpe = compute_sharpe_backtest(test_ag[LABEL].values, np.asarray(preds))\n",
        "\n",
        "    leaderboard = predictor.leaderboard(test_ag, silent=True)\n",
        "    leaderboard.to_csv(path_roll / \"leaderboard.csv\", index=False)\n",
        "\n",
        "    fi = predictor.feature_importance()\n",
        "    if fi is not None and not fi.empty:\n",
        "        fi.to_csv(path_roll / \"feature_importance.csv\", index=True)\n",
        "\n",
        "    model_perf = []\n",
        "    for m in leaderboard[\"model\"].tolist():\n",
        "        try:\n",
        "            p = predictor.predict(test_ag, model=m)\n",
        "            s = compute_sharpe_backtest(test_ag[LABEL].values, np.asarray(p))\n",
        "            score_col = next((c for c in leaderboard.columns if c != \"model\" and pd.api.types.is_numeric_dtype(leaderboard[c])), None)\n",
        "            rmse_val = float(leaderboard.loc[leaderboard[\"model\"] == m, score_col].iloc[0]) if score_col else np.nan\n",
        "            model_perf.append({\"model\": m, \"rmse\": rmse_val, \"sharpe\": s})\n",
        "        except Exception:\n",
        "            pass\n",
        "    if model_perf:\n",
        "        pd.DataFrame(model_perf).to_csv(path_roll / \"models_performance.csv\", index=False)\n",
        "\n",
        "    out_pred = test_df.loc[test_ag.index].copy()\n",
        "    out_pred[\"pred\"] = preds.values\n",
        "    out_pred[[\"date\", LABEL, \"pred\"]].to_csv(path_roll / \"predictions.csv\", index=False)\n",
        "\n",
        "    metrics = {\n",
        "        \"predict_year\": predict_year,\n",
        "        \"train_years\": [predict_year - TRAIN_YEARS, predict_year - 1],\n",
        "        \"n_train\": len(train_ag),\n",
        "        \"n_test\": len(test_ag),\n",
        "        \"rmse\": rmse,\n",
        "        \"sharpe\": sharpe,\n",
        "        \"best_model\": predictor.model_best,\n",
        "    }\n",
        "    with open(path_roll / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "    summary_list.append(metrics)\n",
        "    print(f\"Done {predict_year}: rmse={rmse:.6f}, sharpe={sharpe:.4f}, best={predictor.model_best}\")\n",
        "\n",
        "if summary_list:\n",
        "    pd.DataFrame(summary_list).to_csv(ROLL_OUTPUT / \"roll_summary.csv\", index=False)\n",
        "    print(\"Roll summary saved to:\", ROLL_OUTPUT / \"roll_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMS_Th3bCHgq"
      },
      "source": [
        "## 6. 彙總表（數值輸出供本地分析）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSMPs8g8CHgq"
      },
      "outputs": [],
      "source": [
        "# 輸出均在 data/models/：各年 roll_YYYY/ 含 leaderboard.csv、predictions.csv、metrics.json、models_performance.csv；roll_summary.csv 為各年 rmse/sharpe 彙總\n",
        "if summary_list:\n",
        "    display(pd.DataFrame(summary_list))\n",
        "else:\n",
        "    print(\"No rolls completed. Check predict_years and data.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
