#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€è¡“æŒ‡æ¨™ Autoencoder å£“ç¸®è¨“ç·´è…³æœ¬
å°‡åŒé¡æŠ€è¡“æŒ‡æ¨™å£“ç¸®ç‚ºå–®ä¸€æ•¸å€¼
"""

import sys
import io

# è¨­å®šè¼¸å‡ºç·¨ç¢¼ç‚º UTF-8ï¼ˆè§£æ±º Windows æ§åˆ¶å°ç·¨ç¢¼å•é¡Œï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# å°ˆæ¡ˆ configï¼ˆåƒ…è®€å¯« data/ï¼‰
from pathlib import Path as _Path
_REPO_ROOT = _Path(__file__).resolve().parents[2]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))
import config as _config
DATA_DIR = str(_config.get_dataset_dir("0900"))
OUTPUT_DIR = str(_config.get_output_0900_dir())

import pandas as pd
import numpy as np
import os
import glob
from pathlib import Path
import warnings
from datetime import datetime
import json
import random

# åœ¨å°å…¥ TensorFlow ä¹‹å‰è¨­å®šç’°å¢ƒè®Šæ•¸ï¼ˆå¹«åŠ© TensorFlow æ‰¾åˆ° CUDAï¼‰
# é€™å¯ä»¥å¹«åŠ© TensorFlow åœ¨ Windows ä¸Šæ‰¾åˆ° CUDA åº«
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # æ¸›å°‘ TensorFlow æ—¥èªŒè¼¸å‡º
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # é—œé–‰ oneDNN é¸é …

# æ·±åº¦å­¸ç¿’ç›¸é—œ
import tensorflow as tf
# TensorFlow 2.10+ ä¸­ï¼Œkeras æ˜¯ç¨ç«‹åŒ…ï¼Œéœ€è¦ä½¿ç”¨ keras.src
import keras
from keras.src import layers, models, callbacks
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from skopt import gp_minimize
from skopt.space import Integer, Real, Categorical
from skopt.utils import use_named_args

# åœ–è¡¨å’Œè¼¸å‡º
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼å¾Œç«¯
import openpyxl
from openpyxl.drawing.image import Image
import io

warnings.filterwarnings('ignore')

# ==================== å¯èª¿æ•´åƒæ•¸ ====================
# æŠ€è¡“æŒ‡æ¨™ç¾¤çµ„å®šç¾©
INDICATOR_GROUPS = {
    "STOCH": ["STOCH_K_14", "STOCH_D_14"],
    "STOCHF": ["STOCHF_K_14", "STOCHF_D_14"],
    "STOCHRSI": ["STOCHRSI_K_14", "STOCHRSI_D_14"],
    "MACD": ["MACD_12_26", "MACD_signal_12_26", "MACD_hist_12_26"],
    "BBANDS": ["BBANDS_upper_20", "BBANDS_middle_20", "BBANDS_lower_20"],
    "ADX_DMI": ["ADX_14", "ADXR_14", "PDI_14", "MDI_14", "DX_14"],
    "AROON": ["AROON_Down_14", "AROON_Up_14", "AROONOSC_14"]
}

# è³‡æ–™è·¯å¾‘ï¼ˆç”±ä¸Šæ–¹ config è¨­å®šï¼Œåƒ… data/dataset/0900ã€data/output_0900ï¼‰
# DATA_DIR, OUTPUT_DIR å·²æ–¼æª”é ­è¨­å®š

# è³‡æ–™åˆ‡åˆ†æ¯”ä¾‹ï¼ˆç¶­æŒï¼‰
VAL_SPLIT = 0.2  # é©—è­‰é›† 20%
TEST_SPLIT = 0.1  # æ¸¬è©¦é›† 10%

# Autoencoder æ¶æ§‹åƒæ•¸ï¼ˆç¶­æŒï¼›è‹¥æƒ³åŒæ­¥æ“´ï¼Œå»ºè­°å†é–‹æ–°æœå°‹ç©ºé–“ï¼‰
ENCODER_DIMS = [256, 128]  # ç·¨ç¢¼å™¨å±¤æ¬¡
DECODER_DIMS = [128, 256]  # è§£ç¢¼å™¨å±¤æ¬¡

# ==================== è¶…åƒæ•¸æœå°‹ç©ºé–“ ====================

# æ“´å¤§å­¸ç¿’ç‡å€™é¸ï¼ˆè¿‘ä¼¼å°æ•¸åˆ»åº¦ï¼‰
LEARNING_RATES = [3e-4, 5e-4, 8e-4, 1e-3, 1.5e-3, 2e-3, 3e-3]

# æ›´ç´°çš„ dropout åˆ»åº¦ï¼ˆ0~0.4 å¸¸è¦‹ç”œå€ï¼‰
DROPOUT_RATES = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]

# æ“´å¤§ batch sizeï¼ˆä¾ GPU VRAM è¦–æƒ…æ³è£å‰ªï¼‰
BATCH_SIZES = [128, 192, 256, 384, 512, 768, 1024]

# ==== è²è‘‰æ–¯å„ªåŒ–åƒæ•¸ï¼ˆåŠ é•·æ¢ç´¢ï¼‰ ====
BAYESIAN_N_CALLS = 32  # åŸ 12 â†’ 48ï¼ˆå¯è¦–è³‡æº 32~60 ä¹‹é–“ï¼‰

# è¨“ç·´åƒæ•¸ï¼ˆç•¥å¾®æ”¾å¯¬ä¸Šé™ã€è€å¿ƒå€¼ï¼‰
EARLY_STOPPING_PATIENCE = 16  # åŸ 12 â†’ 16
MAX_EPOCHS = 300  # åŸ 200 â†’ 300
RANDOM_SEED = 42

# æœç´¢éšæ®µçš„æ—©åœè€å¿ƒï¼ˆè¼ƒçŸ­ï¼ŒåŠ å¿«æœç´¢ï¼‰
SEARCH_EARLY_STOPPING_PATIENCE = 8

# å„ªåŒ–é¸é …
SKIP_FINAL_TRAINING = True  # è¨­ç‚º True å¯è·³éæœ€çµ‚è¨“ç·´ï¼Œç›´æ¥ä½¿ç”¨æœç´¢éšæ®µæœ€ä½³æ¨¡å‹ï¼ˆæ›´å¿«ä½†å¯èƒ½æ€§èƒ½ç¨å·®ï¼‰

# å›ºå®šç“¶é ¸å±¤å¤§å°ï¼ˆå…¨éƒ¨å£“æˆ 1 ç¶­ï¼‰
FIXED_BOTTLENECK = 1

# æ”¹ç‚ºé€£çºŒå–å€¼ï¼ˆloguniform åˆ†ä½ˆï¼‰è€Œéå›ºå®šæ¸…å–®
# Real å’Œ Integer å·²åœ¨æ–‡ä»¶é–‹é ­å°å…¥ï¼Œç„¡éœ€é‡è¤‡å°å…¥
LEARNING_RATE_SPACE = Real(3e-4, 2e-3, prior='log-uniform', name='learning_rate')

# dropoutï¼ˆ0~0.4 å¸¸è¦‹ç”œå€ï¼‰
DROPOUT_RATES = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]

# batch sizeï¼ˆä¾ GPU è¨˜æ†¶é«”èª¿æ•´ï¼‰
BATCH_SIZES = [128, 192, 256, 384, 512, 768, 1024]

# ==== è²è‘‰æ–¯å„ªåŒ–åƒæ•¸ ====
BAYESIAN_N_CALLS = 32
EARLY_STOPPING_PATIENCE = 16
MAX_EPOCHS = 300
RANDOM_SEED = 42
SEARCH_EARLY_STOPPING_PATIENCE = 8
SKIP_FINAL_TRAINING = True


# ==================== è¨­å®šéš¨æ©Ÿç¨®å­ ====================
def set_random_seeds(seed):
    """è¨­å®šæ‰€æœ‰éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_random_seeds(RANDOM_SEED)

# @title
# ==================== è¨­å®šéš¨æ©Ÿç¨®å­ ====================
def set_random_seeds(seed):
    """è¨­å®šæ‰€æœ‰éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_random_seeds(RANDOM_SEED)

# ==================== è³‡æ–™è¼‰å…¥ ====================
def load_all_data(data_dir):
    """è¼‰å…¥æ‰€æœ‰CSVæª”æ¡ˆä¸¦åˆä½µï¼ŒæŒ‰å¹´ä»½åˆ†çµ„"""
    print("=" * 60)
    print("[LOAD] è¼‰å…¥è³‡æ–™...")
    
    csv_files = glob.glob(os.path.join(data_dir, "TX*_1K_qlib_indicators_complete.csv"))
    csv_files.sort()  # æŒ‰æª”åæ’åºä»¥ç¢ºä¿æ™‚é–“é †åº
    
    print(f"æ‰¾åˆ° {len(csv_files)} å€‹CSVæª”æ¡ˆ")
    
    all_data = []
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            df['datetime'] = pd.to_datetime(df['datetime'])
            all_data.append(df)
        except Exception as e:
            print(f"[WARN] è®€å–æª”æ¡ˆå¤±æ•—: {os.path.basename(file)} - {e}")
    
    if not all_data:
        raise ValueError("[ERROR] æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•è³‡æ–™ï¼")
    
    combined_df = pd.concat(all_data, ignore_index=True)
    combined_df = combined_df.sort_values('datetime').reset_index(drop=True)
    
    # æå–å¹´ä»½
    combined_df['year'] = combined_df['datetime'].dt.year
    
    print(f"[OK] æˆåŠŸè¼‰å…¥è³‡æ–™ï¼Œç¸½å…± {len(combined_df):,} ç­†è¨˜éŒ„")
    print(f"[INFO] æ™‚é–“ç¯„åœ: {combined_df['datetime'].min()} è‡³ {combined_df['datetime'].max()}")
    
    # é¡¯ç¤ºå¹´ä»½åˆ†å¸ƒ
    year_counts = combined_df['year'].value_counts().sort_index()
    print(f"[INFO] å¹´ä»½åˆ†å¸ƒ:")
    for year, count in year_counts.items():
        print(f"  {year}: {count:,} ç­†")
    
    return combined_df

# ==================== æ»¾å‹•çª—å£åˆ‡åˆ† ====================
def create_rolling_windows(df, train_years=2, compress_years=1):
    """å‰µå»ºæ»¾å‹•çª—å£ï¼šå‰Nå¹´è¨“ç·´ï¼Œå¾ŒMå¹´å£“ç¸®
    
    Args:
        df: åŒ…å« 'year' å’Œ 'datetime' åˆ—çš„ DataFrame
        train_years: è¨“ç·´å¹´æ•¸ï¼ˆé è¨­2å¹´ï¼‰
        compress_years: å£“ç¸®å¹´æ•¸ï¼ˆé è¨­1å¹´ï¼‰
    
    Returns:
        windows: åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚º (train_df, compress_df, window_name)
    """
    years = sorted(df['year'].unique())
    windows = []
    
    print("=" * 60)
    print(f"[WINDOW] å‰µå»ºæ»¾å‹•çª—å£ï¼ˆè¨“ç·´: {train_years}å¹´, å£“ç¸®: {compress_years}å¹´ï¼‰...")
    print(f"å¯ç”¨å¹´ä»½: {years}")
    
    # å¾ç¬¬ train_years å¹´é–‹å§‹ï¼Œæ¯æ¬¡æ»¾å‹• compress_years å¹´
    i = train_years
    window_idx = 1
    
    while i + compress_years <= len(years):
        train_year_start = years[i - train_years]
        train_year_end = years[i - 1]
        compress_year_start = years[i]
        compress_year_end = years[i + compress_years - 1]
        
        # è¨“ç·´è³‡æ–™ï¼ˆå‰Nå¹´ï¼‰
        train_df = df[(df['year'] >= train_year_start) & (df['year'] <= train_year_end)].copy()
        
        # å£“ç¸®è³‡æ–™ï¼ˆå¾ŒMå¹´ï¼‰
        compress_df = df[(df['year'] >= compress_year_start) & (df['year'] <= compress_year_end)].copy()
        
        if len(train_df) > 0 and len(compress_df) > 0:
            window_name = f"W{window_idx}_{train_year_start}-{train_year_end}_compress_{compress_year_start}-{compress_year_end}"
            windows.append((train_df, compress_df, window_name))
            
            print(f"  çª—å£ {window_idx}: è¨“ç·´ {train_year_start}-{train_year_end} ({len(train_df):,}ç­†), "
                  f"å£“ç¸® {compress_year_start}-{compress_year_end} ({len(compress_df):,}ç­†)")
            window_idx += 1
        
        # æ»¾å‹•åˆ°ä¸‹ä¸€å€‹çª—å£ï¼ˆæ¯æ¬¡å‰é€² compress_years å¹´ï¼‰
        i += compress_years
    
    print(f"[OK] å…±å‰µå»º {len(windows)} å€‹æ»¾å‹•çª—å£")
    return windows

# è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼ˆä¸å†åœ¨æ¨¡çµ„å±¤ç´šåŸ·è¡Œï¼‰
# df = load_all_data(DATA_DIR)  # ç§»åˆ° main() ä¸­åŸ·è¡Œ

# @title
# ==================== è³‡æ–™åˆ‡åˆ†ï¼ˆç”¨æ–¼è¨“ç·´é›†å…§éƒ¨åˆ‡åˆ†ï¼‰ ====================
def time_split_data(df, val_split=0.2, test_split=0.1):
    """æŒ‰æ™‚é–“é †åºåˆ‡åˆ†è³‡æ–™ï¼ˆç”¨æ–¼è¨“ç·´é›†å…§éƒ¨çš„ train/val/test åˆ‡åˆ†ï¼‰"""
    n_total = len(df)
    n_test = int(n_total * test_split)
    n_val = int(n_total * val_split)
    n_train = n_total - n_val - n_test
    
    train_df = df.iloc[:n_train].copy()
    val_df = df.iloc[n_train:n_train+n_val].copy()
    test_df = df.iloc[n_train+n_val:].copy()
    
    print("=" * 60)
    print("[INFO] è³‡æ–™åˆ‡åˆ†çµæœ (Time-split):")
    print(f"  è¨“ç·´é›†: {len(train_df):,} ç­† ({len(train_df)/n_total*100:.1f}%)")
    print(f"  é©—è­‰é›†: {len(val_df):,} ç­† ({len(val_df)/n_total*100:.1f}%)")
    print(f"  æ¸¬è©¦é›†: {len(test_df):,} ç­† ({len(test_df)/n_total*100:.1f}%)")
    print(f"  è¨“ç·´é›†æ™‚é–“: {train_df['datetime'].min()} è‡³ {train_df['datetime'].max()}")
    print(f"  é©—è­‰é›†æ™‚é–“: {val_df['datetime'].min()} è‡³ {val_df['datetime'].max()}")
    print(f"  æ¸¬è©¦é›†æ™‚é–“: {test_df['datetime'].min()} è‡³ {test_df['datetime'].max()}")
    
    return train_df, val_df, test_df

# ==================== è³‡æ–™æº–å‚™ ====================
def prepare_indicator_data(df, indicator_cols):
    """æº–å‚™æŒ‡å®šæŒ‡æ¨™çš„è³‡æ–™"""
    # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
    missing_cols = [col for col in indicator_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"[ERROR] ç¼ºå°‘æ¬„ä½: {missing_cols}")
    
    data = df[indicator_cols].values
    # è™•ç†ç„¡é™å¤§å’ŒNaNå€¼
    data = np.nan_to_num(data, nan=0.0, posinf=1e6, neginf=-1e6)
    return data

# ==================== Autoencoder æ¨¡å‹ ====================
def build_autoencoder(input_dim, bottleneck_size, dropout_rate=0.0):
    """å»ºç«‹ Autoencoder æ¨¡å‹
    
    æ¶æ§‹: Input â†’ 256 â†’ 128 â†’ bottleneck â†’ 128 â†’ 256 â†’ Output
    """
    input_layer = layers.Input(shape=(input_dim,), name='input')
    
    # ç·¨ç¢¼å™¨
    x = layers.Dense(ENCODER_DIMS[0], activation='relu', name='encoder_1')(input_layer)
    if dropout_rate > 0:
        x = layers.Dropout(dropout_rate)(x)
    x = layers.Dense(ENCODER_DIMS[1], activation='relu', name='encoder_2')(x)
    if dropout_rate > 0:
        x = layers.Dropout(dropout_rate)(x)
    bottleneck = layers.Dense(bottleneck_size, activation='relu', name='bottleneck')(x)
    
    # è§£ç¢¼å™¨
    x = layers.Dense(DECODER_DIMS[0], activation='relu', name='decoder_1')(bottleneck)
    if dropout_rate > 0:
        x = layers.Dropout(dropout_rate)(x)
    x = layers.Dense(DECODER_DIMS[1], activation='relu', name='decoder_2')(x)
    if dropout_rate > 0:
        x = layers.Dropout(dropout_rate)(x)
    output = layers.Dense(input_dim, activation='linear', name='output')(x)
    
    model = models.Model(inputs=input_layer, outputs=output, name='autoencoder')
    return model

# ==================== è¨“ç·´å‡½æ•¸ ====================
def train_autoencoder(X_train, X_val, bottleneck_size, lr, dropout_rate, batch_size, 
                     max_epochs=200, patience=12, group_name="", show_progress=False):
    """è¨“ç·´å–®ä¸€ Autoencoder"""
    input_dim = X_train.shape[1]
    
    # å»ºç«‹æ¨¡å‹
    model = build_autoencoder(input_dim, bottleneck_size, dropout_rate)
    
    # ç·¨è­¯æ¨¡å‹
    optimizer = keras.optimizers.Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])
    
    # æ—©åœå›èª¿
    early_stopping = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=patience,
        restore_best_weights=True,
        verbose=0
    )
    
    # è¨“ç·´æ­·å²è¨˜éŒ„
    history_callback = callbacks.History()
    
    # è‡ªå®šç¾©å›èª¿ä¾†é¡¯ç¤ºé€²åº¦
    class ProgressCallback(callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            if show_progress and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{max_epochs}, Loss: {logs['loss']:.6f}, Val Loss: {logs['val_loss']:.6f}", 
                      end='\r', flush=True)
    
    progress_callback = ProgressCallback() if show_progress else None
    
    # è¨˜éŒ„è¨“ç·´é–‹å§‹æ™‚é–“
    train_start_time = datetime.now()
    
    # è¨“ç·´æ¨¡å‹
    callbacks_list = [early_stopping, history_callback]
    if progress_callback:
        callbacks_list.append(progress_callback)
    
    history = model.fit(
        X_train, X_train,
        validation_data=(X_val, X_val),
        epochs=max_epochs,
        batch_size=batch_size,
        callbacks=callbacks_list,
        verbose=0
    )
    
    # è¨˜éŒ„è¨“ç·´çµæŸæ™‚é–“
    train_end_time = datetime.now()
    train_duration = (train_end_time - train_start_time).total_seconds()
    
    # è¨ˆç®—æ¯å€‹epochçš„æ™‚é–“ï¼ˆå¹³å‡ï¼‰
    n_epochs = len(history.history['loss'])
    if n_epochs > 0:
        time_per_epoch = train_duration / n_epochs
        # ç”Ÿæˆæ™‚é–“è»¸ï¼ˆç´¯ç©æ™‚é–“ï¼‰
        epoch_times = [time_per_epoch * (i + 1) for i in range(n_epochs)]
        history.history['epoch_times'] = epoch_times
        history.history['total_time'] = train_duration
    else:
        history.history['epoch_times'] = []
        history.history['total_time'] = 0
    
    return model, history.history

# ==================== è¶…åƒæ•¸æœå°‹ï¼ˆè²è‘‰æ–¯å„ªåŒ–ï¼‰ ====================
def hyperparameter_search(X_train, X_val, group_name, indicator_cols):
    """ä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–é€²è¡Œè¶…åƒæ•¸æœå°‹"""
    print(f"\n{'='*60}")
    print(f"[SEARCH] é–‹å§‹è²è‘‰æ–¯å„ªåŒ–è¶…åƒæ•¸æœå°‹: {group_name}")
    print(f"   è¼¸å…¥ç¶­åº¦: {len(indicator_cols)}")
    print(f"   å„ªåŒ–è¿­ä»£æ¬¡æ•¸: {BAYESIAN_N_CALLS}")
    
    # å®šç¾©æœç´¢ç©ºé–“
    # Bottleneck å›ºå®šç‚º 1ï¼ˆå°‡æ‰€æœ‰ç‰¹å¾µå£“ç¸®æˆä¸€ç¶­åº¦ï¼‰
    input_dim = len(indicator_cols)
    FIXED_BOTTLENECK = 1  # å›ºå®šå€¼
    
    print(f"   Bottleneck å›ºå®šç‚º: {FIXED_BOTTLENECK}ï¼ˆå°‡ {input_dim} ç¶­å£“ç¸®ç‚º 1 ç¶­ï¼‰")
    
    # ç¢ºä¿æ‰€æœ‰ç¶­åº¦éƒ½æœ‰è‡³å°‘å…©å€‹å€™é¸å€¼ï¼ˆlearning rate ä½¿ç”¨é€£çºŒç©ºé–“ï¼Œç„¡éœ€æª¢æŸ¥ï¼‰
    assert len(DROPOUT_RATES) >= 2, f"Dropout å€™é¸å¿…é ˆè‡³å°‘ 2 å€‹ï¼Œç•¶å‰: {DROPOUT_RATES}"
    assert len(BATCH_SIZES) >= 2, f"Batch Size å€™é¸å¿…é ˆè‡³å°‘ 2 å€‹ï¼Œç•¶å‰: {BATCH_SIZES}"
    
    # æœç´¢ç©ºé–“ï¼šåªæœç´¢ lr, dropout, batchï¼ˆbottleneck å·²å›ºå®šï¼‰
    dimensions = [
        LEARNING_RATE_SPACE,  # learning rateï¼ˆé€£çºŒç©ºé–“ï¼Œåç¨±='learning_rate'ï¼‰
        Integer(0, len(DROPOUT_RATES) - 1, name='dropout_idx'),  # dropout ç´¢å¼•
        Integer(0, len(BATCH_SIZES) - 1, name='batch_idx'),  # batch size ç´¢å¼•
    ]
    
    # å„²å­˜æ‰€æœ‰è©•ä¼°çµæœ
    results = []
    best_val_mse = float('inf')
    best_config = None
    best_model = None
    best_history = None
    iteration_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨åµŒå¥—å‡½æ•¸ä¸­ä¿®æ”¹
    
    search_start_time = datetime.now()
    
    # å®šç¾©ç›®æ¨™å‡½æ•¸ï¼ˆè²è‘‰æ–¯å„ªåŒ–è¦æœ€å°åŒ–çš„å‡½æ•¸ï¼‰
    @use_named_args(dimensions=dimensions)
    def objective(learning_rate, dropout_idx, batch_idx):
        """ç›®æ¨™å‡½æ•¸ï¼šè¿”å›é©—è­‰é›† MSEï¼ˆè¦æœ€å°åŒ–ï¼‰
        
        åƒæ•¸ï¼š
            learning_rate: å­¸ç¿’ç‡ï¼ˆé€£çºŒå€¼ï¼Œä¾†è‡ª LEARNING_RATE_SPACEï¼‰
            dropout_idx: dropout ç´¢å¼•ï¼ˆé›¢æ•£å€¼ï¼‰
            batch_idx: batch size ç´¢å¼•ï¼ˆé›¢æ•£å€¼ï¼‰
        """
        iteration_count[0] += 1
        idx = iteration_count[0]
        
        # å°‡ç´¢å¼•è½‰æ›ç‚ºå¯¦éš›å€¼ï¼ˆbottleneck å›ºå®šç‚º 1ï¼‰
        bottleneck = FIXED_BOTTLENECK
        lr = float(learning_rate)  # å­¸ç¿’ç‡æ˜¯é€£çºŒå€¼ï¼Œç›´æ¥ä½¿ç”¨
        dropout = DROPOUT_RATES[int(dropout_idx)]
        batch = BATCH_SIZES[int(batch_idx)]
        
        config = {
            'bottleneck': bottleneck,
            'lr': lr,
            'dropout': dropout,
            'batch': batch
        }
        
        # è¨ˆç®—é€²åº¦
        progress = (idx - 1) / BAYESIAN_N_CALLS * 100
        elapsed_time = (datetime.now() - search_start_time).total_seconds()
        
        print(f"\n  [è²è‘‰æ–¯å„ªåŒ– {progress:.1f}%] [{idx}/{BAYESIAN_N_CALLS}] æ¸¬è©¦çµ„åˆ:")
        print(f"    Bottleneck: {bottleneck}, LR: {lr:.0e}, "
              f"Dropout: {dropout}, Batch: {batch}")
        
        if idx > 1:
            avg_time = elapsed_time / (idx - 1)
            remaining = avg_time * (BAYESIAN_N_CALLS - idx + 1)
            print(f"    å·²ç”¨æ™‚é–“: {elapsed_time:.1f}ç§’ | é è¨ˆå‰©é¤˜: {remaining:.1f}ç§’")
        
        try:
            # è¨“ç·´æ¨¡å‹ï¼ˆæœç´¢éšæ®µä½¿ç”¨è¼ƒçŸ­çš„è€å¿ƒå€¼ï¼‰
            print(f"    [è¨“ç·´ä¸­...] ", end='', flush=True)
            model, history = train_autoencoder(
                X_train, X_val,
                bottleneck_size=bottleneck,
                lr=lr,
                dropout_rate=dropout,
                batch_size=batch,
                max_epochs=MAX_EPOCHS,
                patience=SEARCH_EARLY_STOPPING_PATIENCE,  # æœç´¢éšæ®µä½¿ç”¨è¼ƒçŸ­çš„è€å¿ƒå€¼ï¼ˆ8ï¼‰
                group_name=group_name
            )
            
            # ä½¿ç”¨è¨“ç·´æ­·å²ä¸­çš„æœ€ä½³é©—è­‰æå¤±ä½œç‚ºç›®æ¨™å€¼ï¼ˆæ›´æº–ç¢ºä¸”æ›´å¿«ï¼‰
            best_val = float(np.min(history['val_loss']))
            
            # é¡¯ç¤ºè¨“ç·´æ™‚é–“å’Œçµæœ
            if 'total_time' in history:
                epochs = len(history.get('loss', []))
                print(f"[å®Œæˆ] è¨“ç·´æ™‚é–“: {history['total_time']:.2f}ç§’ ({epochs} epochs)")
            
            print(f"    [çµæœ] æœ€ä½³ Val Loss: {best_val:.6f}")
            
            # æ›´æ–°æœ€ä½³æ¨¡å‹ï¼ˆbottleneck å›ºå®šç‚º 1ï¼Œåªéœ€æ¯”è¼ƒé©—è­‰æå¤±ï¼‰
            nonlocal best_val_mse, best_config, best_model, best_history
            is_better = False
            if best_val < best_val_mse * 0.99:  # æ˜é¡¯æ›´å¥½ï¼ˆ>1%ï¼‰
                is_better = True
            elif best_config is not None and best_val <= best_val_mse * 1.01:
                # æ¥è¿‘ï¼ˆÂ±1%ï¼‰ä¸”ç›¸åŒæˆ–æ›´å¥½ï¼ˆbottleneck å›ºå®šï¼Œç„¡éœ€æ¯”è¼ƒï¼‰
                is_better = True
            
            # åªæœ‰ç•¶ is_better æˆç«‹æ™‚ï¼Œæ‰è¨ˆç®—è©³ç´°çš„ MSEï¼ˆç”¨æ–¼å ±è¡¨ï¼Œç¯€çœæ™‚é–“ï¼‰
            train_mse = None
            val_mse = None
            if is_better:
                print(f"[è©•ä¼°ä¸­...] ", end='', flush=True)
                val_pred = model.predict(X_val, verbose=0)
                val_mse = mean_squared_error(X_val, val_pred)
                
                train_pred = model.predict(X_train, verbose=0)
                train_mse = mean_squared_error(X_train, train_pred)
                
                print(f"[å®Œæˆ] Train MSE: {train_mse:.6f}, Val MSE: {val_mse:.6f}")
                
                best_val_mse = best_val  # ä½¿ç”¨ val_loss ä½œç‚ºæ¯”è¼ƒåŸºæº–
                best_config = config
                best_model = model
                best_history = history
                print(f"    [BEST] æ›´æ–°æœ€ä½³æ¨¡å‹ï¼")
            
            # å„²å­˜çµæœï¼ˆåªä¿å­˜åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…ä¿å­˜æ‰€æœ‰æ¨¡å‹ï¼‰
            result = {
                'config': config,
                'train_mse': train_mse,  # å¯èƒ½ç‚º Noneï¼ˆåªæœ‰ is_better æ™‚æ‰è¨ˆç®—ï¼‰
                'val_mse': val_mse,  # å¯èƒ½ç‚º Noneï¼ˆåªæœ‰ is_better æ™‚æ‰è¨ˆç®—ï¼‰
                'best_val_loss': best_val,  # ä½¿ç”¨é€™å€‹ä½œç‚ºä¸»è¦æŒ‡æ¨™ï¼ˆå¾ history ä¸­å–å¾—ï¼‰
                'model': model if is_better else None,  # åªä¿å­˜æœ€ä½³æ¨¡å‹ä»¥ç¯€çœè¨˜æ†¶é«”
                'history': history
            }
            results.append(result)
            
            # è¿”å›æœ€ä½³é©—è­‰æå¤±ï¼ˆè²è‘‰æ–¯å„ªåŒ–è¦æœ€å°åŒ–çš„å€¼ï¼‰
            return best_val
        
        except Exception as e:
            print(f"    [ERROR] è¨“ç·´å¤±æ•—: {e}")
            # è¿”å›ä¸€å€‹å¾ˆå¤§çš„å€¼ï¼Œè¡¨ç¤ºé€™çµ„åƒæ•¸ä¸å¥½
            return 1e10
    
    # åŸ·è¡Œè²è‘‰æ–¯å„ªåŒ–
    print(f"\n[INFO] é–‹å§‹è²è‘‰æ–¯å„ªåŒ–ï¼ˆé«˜æ–¯éç¨‹ï¼‰...")
    result_optimization = gp_minimize(
        func=objective,
        dimensions=dimensions,
        n_calls=BAYESIAN_N_CALLS,
        random_state=RANDOM_SEED,
        n_initial_points=min(4, BAYESIAN_N_CALLS),  # åˆå§‹éš¨æ©Ÿæ¡æ¨£é»æ•¸
        acq_func='EI',  # Expected Improvement æ¡ acquisition function
        verbose=False
    )
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„çµæœ
    if best_config is None:
        raise ValueError(f"[ERROR] {group_name} æ²’æœ‰æˆåŠŸçš„è¨“ç·´çµæœï¼")
    
    print(f"\n{'='*60}")
    print(f"[BEST] æœ€ä½³è¶…åƒæ•¸ ({group_name}):")
    print(f"   Bottleneck: {FIXED_BOTTLENECK} (å›ºå®š)")
    print(f"   Learning Rate: {best_config['lr']:.0e}")
    print(f"   Dropout: {best_config['dropout']}")
    print(f"   Batch Size: {best_config['batch']}")
    print(f"   æœ€ä½³é©—è­‰æå¤±: {best_val_mse:.6f}")
    # å¦‚æœæœ‰è¨ˆç®—éçš„ val_mseï¼Œé¡¯ç¤ºå®ƒ
    best_result = next((r for r in results if r.get('val_mse') is not None and r['config'] == best_config), None)
    if best_result:
        print(f"   é©—è­‰é›† MSE: {best_result['val_mse']:.6f}")
    print(f"\n[INFO] è²è‘‰æ–¯å„ªåŒ–æ‰¾åˆ°çš„æœ€ä½³ç›®æ¨™å€¼: {result_optimization.fun:.6f}")
    print(f"[INFO] æœ€ä½³åƒæ•¸ä½ç½®: {result_optimization.x}")
    
    return best_model, best_config, best_history, results

# ==================== æœ€çµ‚è¨“ç·´ ====================
def final_training(X_train, X_val, X_test, best_config, group_name, best_model=None):
    """ä½¿ç”¨æœ€ä½³åƒæ•¸åœ¨ Train+Val ä¸Šé‡è¨“ï¼Œè©•ä¼° Test
    
    å¦‚æœæä¾› best_modelï¼Œä½¿ç”¨é·ç§»å­¸ç¿’ï¼ˆç¹¼çºŒè¨“ç·´ï¼‰è€Œéå®Œå…¨é‡æ–°è¨“ç·´ï¼Œç¯€çœæ™‚é–“
    """
    print(f"\n{'='*60}")
    print(f"[FINAL] æœ€çµ‚è¨“ç·´: {group_name}")
    print(f"   ä½¿ç”¨ Train+Val è³‡æ–™é‡æ–°è¨“ç·´...")
    
    # åˆä½µ Train å’Œ Val
    X_train_val = np.vstack([X_train, X_val])
    
    # åˆ‡åˆ† Train+Val ç‚ºæ–°çš„ train å’Œ valï¼ˆç”¨æ–¼æ—©åœï¼Œæ¯”ä¾‹ç‚º 80:20ï¼‰
    n_val_final = int(len(X_train_val) * 0.2)
    X_train_final = X_train_val[:-n_val_final]
    X_val_final = X_train_val[-n_val_final:]
    
    print(f"   æœ€çµ‚è¨“ç·´é›†: {len(X_train_final):,} ç­†")
    print(f"   æœ€çµ‚é©—è­‰é›†: {len(X_val_final):,} ç­†ï¼ˆç”¨æ–¼æ—©åœï¼‰")
    print(f"   æ¸¬è©¦é›†: {len(X_test):,} ç­†")
    
    # å¦‚æœæä¾›äº†æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨é·ç§»å­¸ç¿’ï¼ˆç¹¼çºŒè¨“ç·´ï¼‰è€Œéå®Œå…¨é‡æ–°è¨“ç·´
    if best_model is not None:
        print(f"   [å„ªåŒ–] ä½¿ç”¨é·ç§»å­¸ç¿’ï¼šå¾æœç´¢éšæ®µæœ€ä½³æ¨¡å‹ç¹¼çºŒè¨“ç·´ï¼ˆç¯€çœæ™‚é–“ï¼‰")
        input_dim = X_train_final.shape[1]
        
        # å»ºç«‹ç›¸åŒæ¶æ§‹çš„æ–°æ¨¡å‹
        model = build_autoencoder(
            input_dim, 
            best_config['bottleneck'], 
            best_config['dropout']
        )
        
        # ç·¨è­¯æ¨¡å‹
        optimizer = keras.optimizers.Adam(learning_rate=best_config['lr'])
        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])
        
        # è¤‡è£½æœ€ä½³æ¨¡å‹çš„æ¬Šé‡ï¼ˆé·ç§»å­¸ç¿’ï¼‰
        try:
            # å˜—è©¦è¤‡è£½æ¬Šé‡å±¤å°æ‡‰
            best_layers = best_model.layers
            new_layers = model.layers
            
            # è¤‡è£½å¯è¨“ç·´å±¤çš„æ¬Šé‡
            for best_layer, new_layer in zip(best_layers, new_layers):
                if len(best_layer.get_weights()) > 0 and len(new_layer.get_weights()) > 0:
                    # æª¢æŸ¥å±¤çµæ§‹æ˜¯å¦åŒ¹é…
                    if (best_layer.get_weights()[0].shape == new_layer.get_weights()[0].shape and
                        len(best_layer.get_weights()) == len(new_layer.get_weights())):
                        new_layer.set_weights(best_layer.get_weights())
            print(f"   âœ… æ¬Šé‡é·ç§»æˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸ æ¬Šé‡é·ç§»å¤±æ•—ï¼Œå°‡å¾é ­è¨“ç·´: {e}")
        
        # æ—©åœå›èª¿
        early_stopping = callbacks.EarlyStopping(
            monitor='val_loss',
            patience=EARLY_STOPPING_PATIENCE,
            restore_best_weights=True,
            verbose=0
        )
        
        history_callback = callbacks.History()
        
        # è¨˜éŒ„è¨“ç·´é–‹å§‹æ™‚é–“
        train_start_time = datetime.now()
        
        # ç¹¼çºŒè¨“ç·´ï¼ˆé€šå¸¸åªéœ€è¦å¾ˆå°‘çš„ epochsï¼Œå› ç‚ºå·²ç¶“æœ‰å¥½çš„åˆå§‹æ¬Šé‡ï¼‰
        history = model.fit(
            X_train_final, X_train_final,
            validation_data=(X_val_final, X_val_final),
            epochs=MAX_EPOCHS,
            batch_size=best_config['batch'],
            callbacks=[early_stopping, history_callback],
            verbose=0,
            initial_epoch=0
        )
        
        # è¨˜éŒ„è¨“ç·´çµæŸæ™‚é–“
        train_end_time = datetime.now()
        train_duration = (train_end_time - train_start_time).total_seconds()
        
        # è¨ˆç®—æ¯å€‹epochçš„æ™‚é–“
        n_epochs = len(history.history['loss'])
        if n_epochs > 0:
            time_per_epoch = train_duration / n_epochs
            epoch_times = [time_per_epoch * (i + 1) for i in range(n_epochs)]
            history.history['epoch_times'] = epoch_times
            history.history['total_time'] = train_duration
        else:
            history.history['epoch_times'] = []
            history.history['total_time'] = 0
        
        history = history.history
    else:
        # å®Œå…¨é‡æ–°è¨“ç·´ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
        model, history = train_autoencoder(
            X_train_final, X_val_final,
            bottleneck_size=best_config['bottleneck'],
            lr=best_config['lr'],
            dropout_rate=best_config['dropout'],
            batch_size=best_config['batch'],
            max_epochs=MAX_EPOCHS,
            patience=EARLY_STOPPING_PATIENCE,
            group_name=group_name
        )
    
    epochs = len(history.get('loss', []))
    train_time = history.get('total_time', 0)
    print(f"[å®Œæˆ] è¨“ç·´æ™‚é–“: {train_time:.2f}ç§’ ({epochs} epochs)")
    
    print(f"   [è©•ä¼°ä¸­...] ", end='', flush=True)
    
    # è©•ä¼°æ‰€æœ‰é›†åˆï¼ˆä½¿ç”¨å®Œæ•´çš„ Train+Val å’Œ Testï¼‰
    train_val_pred = model.predict(X_train_val, verbose=0)
    test_pred = model.predict(X_test, verbose=0)
    
    train_val_mse = mean_squared_error(X_train_val, train_val_pred)
    test_mse = mean_squared_error(X_test, test_pred)
    
    # åˆ†åˆ¥è¨ˆç®— Train å’Œ Val çš„ MSEï¼ˆåƒ…ç”¨æ–¼å ±å‘Šï¼‰
    train_pred_only = model.predict(X_train, verbose=0)
    val_pred_only = model.predict(X_val, verbose=0)
    train_mse = mean_squared_error(X_train, train_pred_only)
    val_mse = mean_squared_error(X_val, val_pred_only)
    
    print(f"[å®Œæˆ]")
    print(f"   [çµæœ] Train MSE: {train_mse:.6f}")
    print(f"   [çµæœ] Val MSE: {val_mse:.6f}")
    print(f"   [çµæœ] Train+Val MSE: {train_val_mse:.6f}")
    print(f"   [çµæœ] Test MSE: {test_mse:.6f}")
    
    return model, {
        'train_mse': train_mse,
        'val_mse': val_mse,
        'train_val_mse': train_val_mse,
        'test_mse': test_mse,
        'history': history
    }

# ==================== å£“ç¸®ä¸¦ä¿å­˜è³‡æ–™ ====================
def compress_and_save_data(model, scaler, df, indicator_cols, group_name, output_dir, bottleneck_size, window_name=""):
    """ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹å£“ç¸®è³‡æ–™ä¸¦ä¿å­˜ç‚ºæ™‚é–“åºåˆ—æ ¼å¼
    
    Args:
        model: è¨“ç·´å¥½çš„ autoencoder æ¨¡å‹
        scaler: æ¨™æº–åŒ–å™¨
        df: è¦å£“ç¸®çš„è³‡æ–™ DataFrame
        indicator_cols: æŒ‡æ¨™æ¬„ä½åˆ—è¡¨
        group_name: æŒ‡æ¨™ç¾¤çµ„åç¨±
        output_dir: è¼¸å‡ºç›®éŒ„
        bottleneck_size: ç“¶é ¸å±¤å¤§å°
        window_name: çª—å£åç¨±ï¼ˆç”¨æ–¼å€åˆ†ä¸åŒçª—å£çš„çµæœï¼‰
    """
    # å‰µå»ºå£“ç¸®è³‡æ–™è¼¸å‡ºç›®éŒ„
    compressed_dir = os.path.join(output_dir, "compressed_data")
    os.makedirs(compressed_dir, exist_ok=True)
    
    # æº–å‚™æ‰€æœ‰è³‡æ–™
    print(f"   æº–å‚™å£“ç¸®è³‡æ–™...")
    all_data = prepare_indicator_data(df, indicator_cols)
    
    # æ¨™æº–åŒ–
    all_data_scaled = scaler.transform(all_data)
    
    # æå–ç·¨ç¢¼å™¨éƒ¨åˆ†ï¼ˆå¾è¼¸å…¥åˆ° bottleneckï¼‰
    # æ§‹å»ºç·¨ç¢¼å™¨æ¨¡å‹
    encoder_input = model.input
    encoder_output = None
    
    # æ‰¾åˆ° bottleneck å±¤çš„è¼¸å‡º
    for layer in model.layers:
        if layer.name == 'bottleneck':
            encoder_output = layer.output
            break
    
    if encoder_output is None:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹ç·¨ç¢¼å™¨å±¤
        # æ‰¾åˆ° bottleneck ä¹‹å‰çš„å±¤
        bottleneck_layer_idx = None
        for i, layer in enumerate(model.layers):
            if layer.name == 'bottleneck':
                bottleneck_layer_idx = i
                break
        
        if bottleneck_layer_idx is not None:
            encoder_output = model.layers[bottleneck_layer_idx].output
        else:
            raise ValueError("ç„¡æ³•æ‰¾åˆ°ç·¨ç¢¼å™¨è¼¸å‡ºå±¤")
    
    # å‰µå»ºç·¨ç¢¼å™¨æ¨¡å‹
    encoder_model = models.Model(inputs=encoder_input, outputs=encoder_output)
    
    # ä½¿ç”¨ç·¨ç¢¼å™¨å£“ç¸®è³‡æ–™
    print(f"   ä½¿ç”¨ç·¨ç¢¼å™¨å£“ç¸®è³‡æ–™...")
    compressed_data = encoder_model.predict(all_data_scaled, verbose=0)
    
    # å‰µå»ºåŒ…å« datetime å’Œå£“ç¸®ç‰¹å¾µçš„ DataFrame
    compressed_df = pd.DataFrame(
        compressed_data,
        columns=[f"{group_name}_compressed_{i}" for i in range(bottleneck_size)]
    )
    
    # æ·»åŠ  datetime åˆ—ï¼ˆå¦‚æœåŸå§‹è³‡æ–™æœ‰ï¼‰
    if 'datetime' in df.columns:
        compressed_df['datetime'] = df['datetime'].values
        # å°‡ datetime ç§»åˆ°ç¬¬ä¸€åˆ—
        cols = ['datetime'] + [col for col in compressed_df.columns if col != 'datetime']
        compressed_df = compressed_df[cols]
    
    # ä¿å­˜ç‚º CSVï¼ˆå¦‚æœæä¾›çª—å£åç¨±ï¼ŒåŠ å…¥æ–‡ä»¶åï¼‰
    if window_name:
        output_path = os.path.join(compressed_dir, f"{group_name}_{window_name}_compressed.csv")
    else:
        output_path = os.path.join(compressed_dir, f"{group_name}_compressed.csv")
    compressed_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    # é¡¯ç¤ºå£“ç¸®çµ±è¨ˆ
    original_size = all_data.shape[1]
    compressed_size = bottleneck_size
    compression_ratio = original_size / compressed_size
    
    print(f"   âœ… å£“ç¸®å®Œæˆï¼")
    print(f"   åŸå§‹ç¶­åº¦: {original_size}")
    print(f"   å£“ç¸®å¾Œç¶­åº¦: {compressed_size}")
    print(f"   å£“ç¸®æ¯”: {compression_ratio:.2f}:1")
    print(f"   è³‡æ–™ç­†æ•¸: {len(compressed_df):,}")
    print(f"   ä¿å­˜è·¯å¾‘: {output_path}")
    
    return output_path

# ==================== ç¹ªåœ–å‡½æ•¸ ====================
def plot_training_history(history, group_name, output_path):
    """ç¹ªè£½è¨“ç·´æ­·å²ï¼ˆåŒ…å«æ™‚é–“è»¸ï¼‰"""
    # å¦‚æœæœ‰æ™‚é–“ä¿¡æ¯ï¼Œå‰µå»º3å€‹å­åœ–ï¼Œå¦å‰‡2å€‹
    has_time = 'epoch_times' in history and len(history['epoch_times']) > 0
    
    if has_time:
        fig, axes = plt.subplots(1, 3, figsize=(20, 5))
        epoch_times = history['epoch_times']
    else:
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # MSE åœ–ï¼ˆæŒ‰ Epochï¼‰
    axes[0].plot(history['loss'], label='Train MSE', linewidth=2)
    axes[0].plot(history['val_loss'], label='Val MSE', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('MSE', fontsize=12)
    axes[0].set_title(f'{group_name} - Training MSE (by Epoch)', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3)
    
    # å°æ•¸å°ºåº¦ MSE åœ–ï¼ˆæŒ‰ Epochï¼‰
    axes[1].semilogy(history['loss'], label='Train MSE', linewidth=2)
    axes[1].semilogy(history['val_loss'], label='Val MSE', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('MSE (log scale)', fontsize=12)
    axes[1].set_title(f'{group_name} - Training MSE (Log Scale, by Epoch)', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(True, alpha=0.3)
    
    # å¦‚æœæœ‰æ™‚é–“ä¿¡æ¯ï¼Œæ·»åŠ æ™‚é–“è»¸åœ–
    if has_time:
        # MSE åœ–ï¼ˆæŒ‰æ™‚é–“ï¼‰
        axes[2].plot(epoch_times, history['loss'], label='Train MSE', linewidth=2)
        axes[2].plot(epoch_times, history['val_loss'], label='Val MSE', linewidth=2)
        axes[2].set_xlabel('Training Time (seconds)', fontsize=12)
        axes[2].set_ylabel('MSE', fontsize=12)
        axes[2].set_title(f'{group_name} - Training MSE (by Time)\nTotal: {history.get("total_time", 0):.1f}s', 
                         fontsize=14, fontweight='bold')
        axes[2].legend(fontsize=10)
        axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return output_path

# ==================== Excel è¼¸å‡º ====================
def save_results_to_excel(all_results, output_dir):
    """å°‡æ‰€æœ‰çµæœä¿å­˜åˆ° Excel"""
    print(f"\n{'='*60}")
    print("[SAVE] ä¿å­˜çµæœåˆ° Excel...")
    
    os.makedirs(output_dir, exist_ok=True)
    excel_path = os.path.join(output_dir, f"autoencoder_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
    
    wb = openpyxl.Workbook()
    
    # 1. æ‘˜è¦è¡¨
    ws_summary = wb.active
    ws_summary.title = "æ‘˜è¦"
    ws_summary.append(["æŠ€è¡“æŒ‡æ¨™ç¾¤çµ„", "è¼¸å…¥ç¶­åº¦", "æœ€ä½³ Bottleneck", "æœ€ä½³ LR", "æœ€ä½³ Dropout", 
                      "æœ€ä½³ Batch Size", "Train MSE", "Val MSE", "Train+Val MSE", "Test MSE",
                      "è¨“ç·´æ™‚é–“(ç§’)", "ç¸½è¨“ç·´æ™‚é–“(ç§’)"])
    
    for group_name, result in all_results.items():
        config = result['best_config']
        final_scores = result['final_scores']
        history = final_scores.get('history', {})
        search_time = history.get('total_time', 0) if 'total_time' in history else 0
        
        # è¨ˆç®—ç¸½è¨“ç·´æ™‚é–“ï¼ˆæœå°‹éšæ®µ + æœ€çµ‚è¨“ç·´ï¼‰
        final_history = final_scores.get('history', {})
        final_time = final_history.get('total_time', 0) if 'total_time' in final_history else 0
        
        # è¨ˆç®—æœå°‹éšæ®µçš„ç¸½æ™‚é–“
        search_total_time = 0
        for search_result in result.get('search_results', []):
            if isinstance(search_result, dict) and 'history' in search_result:
                search_total_time += search_result['history'].get('total_time', 0)
        
        total_training_time = search_total_time + final_time
        
        ws_summary.append([
            group_name,
            result['input_dim'],
            config['bottleneck'],
            config['lr'],
            config['dropout'],
            config['batch'],
            f"{final_scores['train_mse']:.6f}",
            f"{final_scores['val_mse']:.6f}",
            f"{final_scores.get('train_val_mse', final_scores['train_mse']):.6f}",
            f"{final_scores['test_mse']:.6f}",
            f"{final_time:.2f}",
            f"{total_training_time:.2f}"
        ])
    
    # 2. ç‚ºæ¯å€‹ç¾¤çµ„å‰µå»ºè©³ç´°å·¥ä½œè¡¨
    for group_name, result in all_results.items():
        ws = wb.create_sheet(title=group_name[:31])  # Excelå·¥ä½œè¡¨åç¨±é™åˆ¶31å­—å…ƒ
        
        # è¶…åƒæ•¸æœå°‹çµæœ
        ws.append(["è¶…åƒæ•¸æœå°‹çµæœ"])
        ws.append(["Bottleneck", "Learning Rate", "Dropout", "Batch Size", "Train MSE", "Val MSE"])
        
        for search_result in result['search_results']:
            config = search_result['config']
            train_mse = search_result.get('train_mse')
            val_mse = search_result.get('val_mse')
            ws.append([
                config['bottleneck'],
                config['lr'],
                config['dropout'],
                config['batch'],
                f"{train_mse:.6f}" if train_mse is not None else "N/A",
                f"{val_mse:.6f}" if val_mse is not None else "N/A"
            ])
        
        ws.append([])
        ws.append(["æœ€ä½³é…ç½®"])
        best_config = result['best_config']
        ws.append(["Bottleneck", best_config['bottleneck']])
        ws.append(["Learning Rate", best_config['lr']])
        ws.append(["Dropout", best_config['dropout']])
        ws.append(["Batch Size", best_config['batch']])
        
        ws.append([])
        ws.append(["æœ€çµ‚è©•ä¼°çµæœ"])
        final_scores = result['final_scores']
        ws.append(["Train MSE", f"{final_scores['train_mse']:.6f}"])
        ws.append(["Val MSE", f"{final_scores['val_mse']:.6f}"])
        if 'train_val_mse' in final_scores:
            ws.append(["Train+Val MSE", f"{final_scores['train_val_mse']:.6f}"])
        ws.append(["Test MSE", f"{final_scores['test_mse']:.6f}"])
        
        # æ·»åŠ æ™‚é–“ä¿¡æ¯
        ws.append([])
        ws.append(["è¨“ç·´æ™‚é–“ä¿¡æ¯"])
        final_history = final_scores.get('history', {})
        if 'total_time' in final_history:
            ws.append(["æœ€çµ‚è¨“ç·´æ™‚é–“", f"{final_history['total_time']:.2f} ç§’"])
            ws.append(["å¹³å‡æ¯ Epoch æ™‚é–“", f"{final_history['total_time'] / max(len(final_history.get('loss', [])), 1):.2f} ç§’"])
        else:
            ws.append(["æœ€çµ‚è¨“ç·´æ™‚é–“", "æœªè¨˜éŒ„"])
        
        # è¨ˆç®—æœå°‹éšæ®µçš„ç¸½æ™‚é–“
        search_total_time = 0
        for search_result in result.get('search_results', []):
            if 'history' in search_result and 'total_time' in search_result['history']:
                search_total_time += search_result['history']['total_time']
        if search_total_time > 0:
            ws.append(["è¶…åƒæ•¸æœå°‹ç¸½æ™‚é–“", f"{search_total_time:.2f} ç§’"])
            ws.append(["ç¸½è¨“ç·´æ™‚é–“", f"{search_total_time + final_history.get('total_time', 0):.2f} ç§’"])
        
        # æ’å…¥åœ–ç‰‡
        img_path = result['plot_path']
        if os.path.exists(img_path):
            try:
                img = Image(img_path)
                img.width = 800
                img.height = 300
                ws.add_image(img, f'A{ws.max_row + 3}')
            except Exception as e:
                print(f"  [WARN] ç„¡æ³•æ’å…¥åœ–ç‰‡ {img_path}: {e}")
    
    # 3. è¨“ç·´æ—¥èªŒ
    ws_log = wb.create_sheet(title="è¨“ç·´æ—¥èªŒ")
    ws_log.append(["æ™‚é–“", "ç¾¤çµ„", "éšæ®µ", "è¨Šæ¯"])
    
    for group_name, result in all_results.items():
        if 'log' in result:
            for log_entry in result['log']:
                ws_log.append(log_entry)
    
    wb.save(excel_path)
    print(f"[OK] Excel å·²ä¿å­˜: {excel_path}")
    
    return excel_path

# ==================== ä¸»ç¨‹å¼ ====================
def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("[START] æŠ€è¡“æŒ‡æ¨™ Autoencoder å£“ç¸®è¨“ç·´")
    print("=" * 60)
    print(f"[TIME] é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"[SEED] éš¨æ©Ÿç¨®å­: {RANDOM_SEED}")
    
    # GPU æª¢æŸ¥å’Œé…ç½®
    print("\n[GPU] GPU æª¢æŸ¥å’Œé…ç½®...")
    print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
    print(f"TensorFlow æ˜¯å¦æ§‹å»ºæ™‚åŒ…å« CUDA æ”¯æŒ: {tf.test.is_built_with_cuda()}")
    
    gpus = tf.config.list_physical_devices('GPU')
    print(f"å¯ç”¨ GPU æ¸…å–®: {gpus}")
    
    if len(gpus) > 0:
        print(f"âœ… æª¢æ¸¬åˆ° {len(gpus)} å€‹ GPU è¨­å‚™")
        try:
            # å•Ÿç”¨ GPU è¨˜æ†¶é«”å¢é•·ï¼ˆé¿å…ä¸€æ¬¡æ€§åˆ†é…æ‰€æœ‰è¨˜æ†¶é«”ï¼‰
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("âœ… GPU è¨˜æ†¶é«”å¢é•·å·²å•Ÿç”¨")
            
            # é©—è­‰ GPU æ˜¯å¦å¯ç”¨
            logical_gpus = tf.config.list_logical_devices('GPU')
            if len(logical_gpus) > 0:
                print(f"âœ… GPU å¯ç”¨æ–¼ TensorFlow é‹ç®—: {logical_gpus}")
                
                # æ¸¬è©¦ GPU é‹ç®—
                try:
                    with tf.device('/GPU:0'):
                        test_tensor = tf.constant([1.0, 2.0, 3.0])
                        result = tf.reduce_sum(test_tensor)
                    print(f"âœ… GPU é‹ç®—æ¸¬è©¦æˆåŠŸ: {result.numpy()}")
                    print("TensorFlow æ˜¯å¦ä½¿ç”¨ GPU: True")
                except Exception as e:
                    print(f"âš ï¸ GPU é‹ç®—æ¸¬è©¦å¤±æ•—: {e}")
                    print("TensorFlow æ˜¯å¦ä½¿ç”¨ GPU: False")
            else:
                print("âŒ GPU ä¸å¯ç”¨æ–¼ TensorFlow é‹ç®—")
                print("TensorFlow æ˜¯å¦ä½¿ç”¨ GPU: False")
        except RuntimeError as e:
            print(f"âš ï¸ GPU è¨­å®šè­¦å‘Š: {e}")
            print("TensorFlow æ˜¯å¦ä½¿ç”¨ GPU: False")
    else:
        print("âŒ æ²’æœ‰æª¢æ¸¬åˆ° GPU è¨­å‚™")
        if not tf.test.is_built_with_cuda():
            print("âš ï¸ TensorFlow ç•¶å‰ç‰ˆæœ¬ä¼¼ä¹ä¸åŒ…å« CUDA æ”¯æŒï¼ˆCPU-only æ§‹å»ºï¼‰")
            print("ğŸ’¡ æç¤ºï¼šå¦‚æœå·²å®‰è£ CUDAï¼Œå¯èƒ½éœ€è¦ï¼š")
            print("   1. ç¢ºä¿å·²å®‰è£å®Œæ•´çš„ CUDA Toolkitï¼ˆä¸åƒ…æ˜¯é©…å‹•ï¼‰")
            print("   2. å®‰è£å°æ‡‰ç‰ˆæœ¬çš„ cuDNN")
            print("   3. æˆ–è€ƒæ…®ä½¿ç”¨ conda å®‰è£æ”¯æŒ GPU çš„ TensorFlow")
        print("å°‡ä½¿ç”¨ CPU é€²è¡Œé‹ç®—")
        print("TensorFlow æ˜¯å¦ä½¿ç”¨ GPU: False")
    
    print()
    
    # è¼‰å…¥æ‰€æœ‰è³‡æ–™
    df = load_all_data(DATA_DIR)
    
    # å‰µå»ºæ»¾å‹•çª—å£ï¼ˆå‰2å¹´è¨“ç·´ï¼Œå¾Œ1å¹´å£“ç¸®ï¼‰
    windows = create_rolling_windows(df, train_years=2, compress_years=1)
    
    if len(windows) == 0:
        raise ValueError("[ERROR] ç„¡æ³•å‰µå»ºä»»ä½•æ»¾å‹•çª—å£ï¼è«‹æª¢æŸ¥è³‡æ–™å¹´ä»½æ˜¯å¦è¶³å¤ ã€‚")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # å„²å­˜æ‰€æœ‰çµæœï¼ˆæŒ‰çª—å£å’Œç¾¤çµ„ï¼‰
    all_results = {}
    
    # å°æ¯å€‹æ»¾å‹•çª—å£é€²è¡Œè™•ç†
    total_windows = len(windows)
    overall_start_time = datetime.now()
    
    for window_idx, (train_df_window, compress_df_window, window_name) in enumerate(windows, 1):
        print(f"\n{'='*80}")
        print(f"[WINDOW {window_idx}/{total_windows}] è™•ç†çª—å£: {window_name}")
        print(f"{'='*80}")
        
        # è¨ˆç®—å·²ç”¨æ™‚é–“å’Œé è¨ˆå‰©é¤˜æ™‚é–“
        elapsed = (datetime.now() - overall_start_time).total_seconds()
        if window_idx > 1:
            avg_time_per_window = elapsed / (window_idx - 1)
            remaining_windows = total_windows - window_idx + 1
            remaining_time = avg_time_per_window * remaining_windows
            print(f"å·²ç”¨æ™‚é–“: {elapsed:.1f}ç§’ | é è¨ˆå‰©é¤˜: {remaining_time:.1f}ç§’ ({remaining_time/60:.1f}åˆ†é˜)")
        
        # ç‚ºç•¶å‰çª—å£å‰µå»ºè¼¸å‡ºç›®éŒ„
        window_output_dir = os.path.join(OUTPUT_DIR, window_name)
        os.makedirs(window_output_dir, exist_ok=True)
        models_dir = os.path.join(window_output_dir, "models")
        plots_dir = os.path.join(window_output_dir, "plots")
        os.makedirs(models_dir, exist_ok=True)
        os.makedirs(plots_dir, exist_ok=True)
        
        # å°è¨“ç·´è³‡æ–™é€²è¡Œå…§éƒ¨åˆ‡åˆ†ï¼ˆtrain/val/testï¼‰
        train_df, val_df, test_df = time_split_data(train_df_window, VAL_SPLIT, TEST_SPLIT)
        
        # å°æ¯å€‹æŠ€è¡“æŒ‡æ¨™ç¾¤çµ„é€²è¡Œè™•ç†
        total_groups = len(INDICATOR_GROUPS)
        window_start_time = datetime.now()
        
        for group_idx, (group_name, indicator_cols) in enumerate(INDICATOR_GROUPS.items(), 1):
            print(f"\n{'#'*60}")
            print(f"[WINDOW {window_idx}/{total_windows}] [GROUP {group_idx}/{total_groups}] è™•ç†æŠ€è¡“æŒ‡æ¨™ç¾¤çµ„: {group_name}")
            print(f"   åŒ…å«æŒ‡æ¨™: {', '.join(indicator_cols)}")
            print(f"   çª—å£é€²åº¦: {group_idx}/{total_groups} ({group_idx/total_groups*100:.1f}%)")
            
            # è¨ˆç®—å·²ç”¨æ™‚é–“å’Œé è¨ˆå‰©é¤˜æ™‚é–“
            elapsed_group = (datetime.now() - window_start_time).total_seconds()
            if group_idx > 1:
                avg_time_per_group = elapsed_group / (group_idx - 1)
                remaining_groups = total_groups - group_idx + 1
                remaining_time = avg_time_per_group * remaining_groups
                print(f"   çª—å£å…§å·²ç”¨æ™‚é–“: {elapsed_group:.1f}ç§’ | é è¨ˆå‰©é¤˜: {remaining_time:.1f}ç§’ ({remaining_time/60:.1f}åˆ†é˜)")
            
            print(f"{'#'*60}")
        
            try:
                # æº–å‚™è³‡æ–™
                X_train_raw = prepare_indicator_data(train_df, indicator_cols)
                X_val_raw = prepare_indicator_data(val_df, indicator_cols)
                X_test_raw = prepare_indicator_data(test_df, indicator_cols)
                
                # æ¨™æº–åŒ–ï¼ˆåªåœ¨è¨“ç·´é›†ä¸Š fitï¼‰
                scaler = StandardScaler()
                X_train = scaler.fit_transform(X_train_raw)
                X_val = scaler.transform(X_val_raw)
                X_test = scaler.transform(X_test_raw)
                
                print(f"[OK] è³‡æ–™æº–å‚™å®Œæˆ")
                print(f"   è¨“ç·´é›†å½¢ç‹€: {X_train.shape}")
                print(f"   é©—è­‰é›†å½¢ç‹€: {X_val.shape}")
                print(f"   æ¸¬è©¦é›†å½¢ç‹€: {X_test.shape}")
                
                # è¶…åƒæ•¸æœå°‹
                best_model, best_config, best_history, search_results = hyperparameter_search(
                    X_train, X_val, group_name, indicator_cols
                )
                
                # ä¿å­˜æœå°‹éšæ®µçš„æ¨¡å‹ï¼ˆå¯é¸ï¼‰
                search_model_path = os.path.join(models_dir, f"{group_name}_search_best.h5")
                best_model.save(search_model_path)
            
                # æœ€çµ‚è¨“ç·´ï¼ˆå¯é¸ï¼Œå¦‚æœ SKIP_FINAL_TRAINING=True å‰‡è·³éä»¥ç¯€çœæ™‚é–“ï¼‰
                if SKIP_FINAL_TRAINING:
                    print(f"\n[OPTIMIZE] è·³éæœ€çµ‚è¨“ç·´ï¼Œç›´æ¥ä½¿ç”¨æœç´¢éšæ®µæœ€ä½³æ¨¡å‹ï¼ˆç¯€çœæ™‚é–“ï¼‰")
                    final_model = best_model
                    
                    # ä½¿ç”¨æœç´¢éšæ®µçš„æ­·å²è¨˜éŒ„
                    final_history = best_history
                    
                    # è©•ä¼°æœ€çµ‚æ¨¡å‹ï¼ˆåœ¨å®Œæ•´æ•¸æ“šé›†ä¸Šï¼‰
                    print(f"   [è©•ä¼°ä¸­...] ", end='', flush=True)
                    X_train_val = np.vstack([X_train, X_val])
                    
                    train_val_pred = final_model.predict(X_train_val, verbose=0)
                    test_pred = final_model.predict(X_test, verbose=0)
                    
                    train_val_mse = mean_squared_error(X_train_val, train_val_pred)
                    test_mse = mean_squared_error(X_test, test_pred)
                    
                    train_pred_only = final_model.predict(X_train, verbose=0)
                    val_pred_only = final_model.predict(X_val, verbose=0)
                    train_mse = mean_squared_error(X_train, train_pred_only)
                    val_mse = mean_squared_error(X_val, val_pred_only)
                    
                    print(f"[å®Œæˆ]")
                    print(f"   [çµæœ] Train MSE: {train_mse:.6f}")
                    print(f"   [çµæœ] Val MSE: {val_mse:.6f}")
                    print(f"   [çµæœ] Train+Val MSE: {train_val_mse:.6f}")
                    print(f"   [çµæœ] Test MSE: {test_mse:.6f}")
                    
                    final_scores = {
                        'train_mse': train_mse,
                        'val_mse': val_mse,
                        'train_val_mse': train_val_mse,
                        'test_mse': test_mse,
                        'history': final_history
                    }
                    
                    final_model_path = search_model_path  # ä½¿ç”¨æœç´¢éšæ®µçš„æ¨¡å‹
                else:
                    # æœ€çµ‚è¨“ç·´ï¼ˆTrain+Val é‡è¨“ï¼Œä½¿ç”¨é·ç§»å­¸ç¿’ç¯€çœæ™‚é–“ï¼‰
                    final_model, final_scores = final_training(
                        X_train, X_val, X_test, best_config, group_name, best_model=best_model
                    )
                    
                    # ä¿å­˜æœ€çµ‚æ¨¡å‹
                    final_model_path = os.path.join(models_dir, f"{group_name}_final.h5")
                    final_model.save(final_model_path)
                
                # ç¹ªè£½è¨“ç·´æ­·å²
                plot_path = os.path.join(plots_dir, f"{group_name}_training_history.png")
                plot_training_history(final_scores['history'], group_name, plot_path)
                
                # ä¿å­˜æ¨™æº–åŒ–å™¨
                scaler_path = os.path.join(models_dir, f"{group_name}_scaler.pkl")
                import pickle
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)
                
                # å£“ç¸®ä¸¦è¼¸å‡ºæ™‚é–“åºåˆ—è³‡æ–™ï¼ˆä½¿ç”¨å£“ç¸®çª—å£çš„è³‡æ–™ï¼‰
                print(f"\n[COMPRESS] é–‹å§‹å£“ç¸®æ™‚é–“åºåˆ—è³‡æ–™: {group_name} (çª—å£: {window_name})")
                compressed_data_path = compress_and_save_data(
                    final_model, scaler, compress_df_window, indicator_cols, group_name, 
                    window_output_dir, best_config['bottleneck'], window_name=window_name
                )
                
                # å„²å­˜çµæœï¼ˆæŒ‰çª—å£å’Œç¾¤çµ„ï¼‰
                if window_name not in all_results:
                    all_results[window_name] = {}
                
                all_results[window_name][group_name] = {
                    'input_dim': len(indicator_cols),
                    'best_config': best_config,
                    'search_results': [
                        {
                            'config': r['config'],
                            'train_mse': r['train_mse'],
                            'val_mse': r['val_mse'],
                            'history': r.get('history', {})  # ä¿å­˜æ­·å²è¨˜éŒ„ï¼ˆåŒ…å«æ™‚é–“ä¿¡æ¯ï¼‰
                        }
                        for r in search_results
                    ],
                    'final_scores': final_scores,
                    'plot_path': plot_path,
                    'model_path': final_model_path,
                    'scaler_path': scaler_path,
                    'compressed_data_path': compressed_data_path
                }
                
                elapsed_group_time = (datetime.now() - window_start_time).total_seconds()
                print(f"[OK] {group_name} è™•ç†å®Œæˆï¼ (çª—å£å…§è€—æ™‚: {elapsed_group_time:.1f}ç§’)")
                print(f"   å·²å®Œæˆ {group_idx}/{total_groups} å€‹ç¾¤çµ„ ({group_idx/total_groups*100:.1f}%)")
                
            except Exception as e:
                print(f"[ERROR] {group_name} è™•ç†å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ç‚ºç•¶å‰çª—å£ä¿å­˜çµæœåˆ° Excel
        window_excel_path = save_results_to_excel(all_results[window_name], window_output_dir)
        print(f"[OK] çª—å£ {window_name} è™•ç†å®Œæˆï¼")
        print(f"   çµæœå·²ä¿å­˜åˆ°: {window_output_dir}")
    
    # ä¿å­˜æ•´é«” JSON å ±å‘Š
    json_path = os.path.join(OUTPUT_DIR, f"all_windows_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    # ç§»é™¤ç„¡æ³•åºåˆ—åŒ–çš„å°è±¡
    json_results = {}
    for window_name, window_results in all_results.items():
        json_results[window_name] = {}
        for group_name, result in window_results.items():
            json_results[window_name][group_name] = {
                'input_dim': result['input_dim'],
                'best_config': result['best_config'],
                'final_scores': result['final_scores'],
                'search_results': result['search_results']
            }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*80}")
    print("[DONE] æ‰€æœ‰æ»¾å‹•çª—å£è™•ç†å®Œæˆï¼")
    print(f"[DIR] çµæœç›®éŒ„: {OUTPUT_DIR}")
    print(f"[JSON] æ•´é«” JSON å ±å‘Š: {json_path}")
    
    # é¡¯ç¤ºæ‰€æœ‰çª—å£çš„å£“ç¸®è³‡æ–™è·¯å¾‘
    print(f"\n[COMPRESSED] å£“ç¸®æ™‚é–“åºåˆ—è³‡æ–™:")
    for window_name in all_results.keys():
        window_output_dir = os.path.join(OUTPUT_DIR, window_name)
        compressed_dir = os.path.join(window_output_dir, "compressed_data")
        if os.path.exists(compressed_dir):
            compressed_files = [f for f in os.listdir(compressed_dir) if f.endswith('.csv')]
            if compressed_files:
                print(f"   çª—å£ {window_name}:")
                print(f"     ç›®éŒ„: {compressed_dir}")
                print(f"     æª”æ¡ˆæ•¸: {len(compressed_files)}")
                for f in compressed_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    print(f"     - {f}")
                if len(compressed_files) > 5:
                    print(f"     ... é‚„æœ‰ {len(compressed_files) - 5} å€‹æª”æ¡ˆ")
    
    total_time = (datetime.now() - overall_start_time).total_seconds()
    print(f"\n[TIME] ç¸½è™•ç†æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é˜)")
    print(f"[TIME] çµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

if __name__ == "__main__":
    main()
