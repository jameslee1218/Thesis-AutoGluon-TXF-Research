{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ILMbqCA2DkW",
        "outputId": "0cfff42e-59b6-4834-d21f-4464678f8c78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown #掛載Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74900717",
        "outputId": "039254b1-5633-4917-a20c-e8a868b59dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.5.3)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-26.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (26.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-26.2.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-26.2.1 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovoIGwag1kHv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "技術指標 Autoencoder 壓縮訓練腳本\n",
        "將同類技術指標壓縮為單一數值\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import io\n",
        "\n",
        "# 設定輸出編碼為 UTF-8（解決 Windows 控制台編碼問題）\n",
        "if sys.platform == 'win32':\n",
        "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\n",
        "    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import json\n",
        "import random\n",
        "\n",
        "# 在導入 TensorFlow 之前設定環境變數（幫助 TensorFlow 找到 CUDA）\n",
        "# 這可以幫助 TensorFlow 在 Windows 上找到 CUDA 庫\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # 減少 TensorFlow 日誌輸出\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # 關閉 oneDNN 選項\n",
        "\n",
        "# 深度學習相關\n",
        "import tensorflow as tf\n",
        "# TensorFlow 2.10+ 中，keras 是獨立包，需要使用 keras.src\n",
        "import keras\n",
        "from keras.src import layers, models, callbacks\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# 圖表和輸出\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # 使用非交互式後端\n",
        "import openpyxl\n",
        "from openpyxl.drawing.image import Image\n",
        "import io\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xge3JyZ02WFS"
      },
      "outputs": [],
      "source": [
        "# ==================== 可調整參數 ====================\n",
        "# 技術指標群組定義\n",
        "INDICATOR_GROUPS = {\n",
        "    \"STOCH\": [\"STOCH_K_14\", \"STOCH_D_14\"],\n",
        "    \"STOCHF\": [\"STOCHF_K_14\", \"STOCHF_D_14\"],\n",
        "    \"STOCHRSI\": [\"STOCHRSI_K_14\", \"STOCHRSI_D_14\"],\n",
        "    \"MACD\": [\"MACD_12_26\", \"MACD_signal_12_26\", \"MACD_hist_12_26\"],\n",
        "    \"BBANDS\": [\"BBANDS_upper_20\", \"BBANDS_middle_20\", \"BBANDS_lower_20\"],\n",
        "    \"ADX_DMI\": [\"ADX_14\", \"ADXR_14\", \"PDI_14\", \"MDI_14\", \"DX_14\"],\n",
        "    \"AROON\": [\"AROON_Down_14\", \"AROON_Up_14\", \"AROONOSC_14\"]\n",
        "}\n",
        "\n",
        "# 資料路徑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHhuqwjh2XN1"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/工作區/論文/論文code/1106/technical_indicators_data_extracted\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/工作區/論文/論文code/1106/output2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4ASDp2z20_C",
        "outputId": "a73fce6e-af67-4b4a-bc5d-b7231376cdfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Listing first 5 files in: /content/drive/MyDrive/工作區/論文/論文code/1106/technical_indicators_data_extracted\n",
            "- TX20110103_1K_qlib_indicators_complete.csv\n",
            "- TX20110104_1K_qlib_indicators_complete.csv\n",
            "- TX20110105_1K_qlib_indicators_complete.csv\n",
            "- TX20110106_1K_qlib_indicators_complete.csv\n",
            "- TX20110107_1K_qlib_indicators_complete.csv\n",
            "... and 2719 more files\n",
            "\n",
            "Listing first 5 files in: /content/drive/MyDrive/工作區/論文/論文code/1106/output2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def list_top_n_files(directory, n=5):\n",
        "    \"\"\"Lists the first n files in a given directory.\"\"\"\n",
        "    if not os.path.isdir(directory):\n",
        "        print(f\"Error: Directory not found: {directory}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nListing first {n} files in: {directory}\")\n",
        "    try:\n",
        "        files = os.listdir(directory)\n",
        "        # Sort files to get a consistent order\n",
        "        files.sort()\n",
        "        for i, file in enumerate(files[:n]):\n",
        "            print(f\"- {file}\")\n",
        "        if len(files) > n:\n",
        "            print(f\"... and {len(files) - n} more files\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files in {directory}: {e}\")\n",
        "\n",
        "# List files in DATA_DIR\n",
        "list_top_n_files(DATA_DIR)\n",
        "\n",
        "# List files in OUTPUT_DIR\n",
        "list_top_n_files(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZxLibj32ZJd"
      },
      "outputs": [],
      "source": [
        "VAL_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1\n",
        "\n",
        "ENCODER_DIMS = [256, 128]\n",
        "DECODER_DIMS = [128, 256]\n",
        "\n",
        "# 擴大學習率候選（近似對數刻度）\n",
        "LEARNING_RATES = [3e-4, 5e-4, 8e-4, 1e-3, 1.5e-3, 2e-3, 3e-3]\n",
        "\n",
        "# 更細的 dropout 刻度（0~0.4 常見甜區）\n",
        "DROPOUT_RATES = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "# 擴大 batch size（依 GPU VRAM 視情況裁剪）\n",
        "BATCH_SIZES = 32786\n",
        "\n",
        "# ==== 貝葉斯優化參數（加長探索） ====\n",
        "BAYESIAN_N_CALLS = 32  # 原 12 → 48（可視資源 32~60 之間）\n",
        "\n",
        "# 訓練參數（略微放寬上限、耐心值）\n",
        "EARLY_STOPPING_PATIENCE = 16  # 原 12 → 16\n",
        "MAX_EPOCHS = 300  # 原 200 → 300\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# 搜索階段的早停耐心（較短，加快搜索）\n",
        "SEARCH_EARLY_STOPPING_PATIENCE = 8\n",
        "\n",
        "# 優化選項\n",
        "SKIP_FINAL_TRAINING = True  # 設為 True 可跳過最終訓練，直接使用搜索階段最佳模型（更快但可能性能稍差）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CZTD0GZtiBS"
      },
      "outputs": [],
      "source": [
        "# ==================== 超參數搜尋空間 ====================\n",
        "\n",
        "# 固定瓶頸層大小（全部壓成 1 維）\n",
        "FIXED_BOTTLENECK = 1\n",
        "\n",
        "# 改為連續取值（loguniform 分佈）而非固定清單\n",
        "from skopt.space import Real, Integer\n",
        "\n",
        "LEARNING_RATE_SPACE = Real(3e-4, 2e-3, prior='log-uniform', name='learning_rate')\n",
        "\n",
        "# dropout（0~0.4 常見甜區）\n",
        "DROPOUT_RATES = [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "# batch size（依 GPU 記憶體調整）\n",
        "BATCH_SIZES = 32786\n",
        "\n",
        "# ==== 貝葉斯優化參數 ====\n",
        "BAYESIAN_N_CALLS = 24\n",
        "EARLY_STOPPING_PATIENCE = 16\n",
        "MAX_EPOCHS = 300\n",
        "RANDOM_SEED = 42\n",
        "SEARCH_EARLY_STOPPING_PATIENCE = 8\n",
        "SKIP_FINAL_TRAINING = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAvJqfj-ouPz",
        "outputId": "2d034ee7-814d-47f7-b0c0-c7bf43083936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "[LOAD] 載入資料...\n",
            "找到 2723 個CSV檔案\n",
            "[OK] 成功載入資料，總共 819,623 筆記錄\n",
            "[INFO] 時間範圍: 2011-01-03 08:45:00 至 2023-12-22 13:45:00\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ==================== 設定隨機種子 ====================\n",
        "def set_random_seeds(seed):\n",
        "    \"\"\"設定所有隨機種子以確保可重現性\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_random_seeds(RANDOM_SEED)\n",
        "\n",
        "# ==================== 資料載入 ====================\n",
        "def load_all_data(data_dir):\n",
        "    \"\"\"載入所有CSV檔案並合併\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"[LOAD] 載入資料...\")\n",
        "\n",
        "    csv_files = glob.glob(os.path.join(data_dir, \"TX*_1K_qlib_indicators_complete.csv\"))\n",
        "    csv_files.sort()  # 按檔名排序以確保時間順序\n",
        "\n",
        "    print(f\"找到 {len(csv_files)} 個CSV檔案\")\n",
        "\n",
        "    all_data = []\n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "            all_data.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] 讀取檔案失敗: {os.path.basename(file)} - {e}\")\n",
        "\n",
        "    if not all_data:\n",
        "        raise ValueError(\"[ERROR] 沒有成功載入任何資料！\")\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    combined_df = combined_df.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "    print(f\"[OK] 成功載入資料，總共 {len(combined_df):,} 筆記錄\")\n",
        "    print(f\"[INFO] 時間範圍: {combined_df['datetime'].min()} 至 {combined_df['datetime'].max()}\")\n",
        "\n",
        "    return combined_df\n",
        "df = load_all_data(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D3UFIBJ42QDg",
        "outputId": "4de73452-32e8-4724-f021-99a4d45664ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "[START] 技術指標 Autoencoder 壓縮訓練\n",
            "============================================================\n",
            "[TIME] 開始時間: 2026-02-08 15:56:59\n",
            "[SEED] 隨機種子: 42\n",
            "\n",
            "[GPU] GPU 檢查和配置...\n",
            "TensorFlow 版本: 2.19.0\n",
            "TensorFlow 是否構建時包含 CUDA 支持: True\n",
            "可用 GPU 清單: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "✅ 檢測到 1 個 GPU 設備\n",
            "✅ GPU 記憶體增長已啟用\n",
            "✅ GPU 可用於 TensorFlow 運算: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
            "✅ GPU 運算測試成功: 6.0\n",
            "TensorFlow 是否使用 GPU: True\n",
            "\n",
            "============================================================\n",
            "[INFO] 改用「交易日」切分資料 (總天數: 2723 天)\n",
            "  訓練集天數: 1907\n",
            "  驗證集天數: 544\n",
            "  測試集天數: 272\n",
            "  Cutoff 1 (Train|Val): 2020-02-24\n",
            "  Cutoff 2 (Val|Test) : 2022-09-30\n",
            "------------------------------------------------------------\n",
            "  訓練集: 574,007 筆 (70.0%) | 時間: 2011-01-03 08:45:00 至 2020-02-21 13:45:00\n",
            "  驗證集: 163,744 筆 (20.0%) | 時間: 2020-02-24 08:45:00 至 2022-09-29 13:45:00\n",
            "  測試集: 81,872 筆 (10.0%) | 時間: 2022-09-30 08:45:00 至 2023-12-22 13:45:00\n",
            "\n",
            "############################################################\n",
            "[GROUP 1/7] 處理技術指標群組: STOCH\n",
            "   包含指標: STOCH_K_14, STOCH_D_14\n",
            "   總體進度: 1/7 (14.3%)\n",
            "############################################################\n",
            "[OK] 資料準備完成\n",
            "   訓練集形狀: (574007, 2)\n",
            "   驗證集形狀: (163744, 2)\n",
            "   測試集形狀: (81872, 2)\n",
            "\n",
            "============================================================\n",
            "[SEARCH] 開始貝葉斯優化超參數搜尋: STOCH\n",
            "   輸入維度: 2\n",
            "   優化迭代次數: 24\n",
            "   Bottleneck 固定為: 1（將 2 維壓縮為 1 維）\n",
            "\n",
            "[INFO] 開始貝葉斯優化（高斯過程）...\n",
            "\n",
            "  [貝葉斯優化 0.0%] [1/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 1e-03, Dropout: 0.05, Batch: 32786\n",
            "    [訓練中...] [完成] 訓練時間: 37.76秒 (60 epochs)\n",
            "    [結果] 最佳 Val Loss: 1.137341\n",
            "[評估中...] [完成] Train MSE: 1.000001, Val MSE: 1.137341\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 4.2%] [2/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 1e-03, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 79.2秒 | 預計剩餘: 1820.8秒\n",
            "    [訓練中...] [完成] 訓練時間: 12.73秒 (13 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.032367\n",
            "[評估中...] [完成] Train MSE: 0.029700, Val MSE: 0.032368\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 8.3%] [3/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 127.8秒 | 預計剩餘: 1405.4秒\n",
            "    [訓練中...] [完成] 訓練時間: 42.31秒 (56 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.024949\n",
            "[評估中...] [完成] Train MSE: 0.024360, Val MSE: 0.024949\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 12.5%] [4/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.1, Batch: 32786\n",
            "    已用時間: 206.1秒 | 預計剩餘: 1443.0秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.14秒 (23 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.027283\n",
            "\n",
            "  [貝葉斯優化 16.7%] [5/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 1e-03, Dropout: 0.3, Batch: 32786\n",
            "    已用時間: 220.4秒 | 預計剩餘: 1102.1秒\n",
            "    [訓練中...] [完成] 訓練時間: 12.16秒 (12 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.041456\n",
            "\n",
            "  [貝葉斯優化 20.8%] [6/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.3, Batch: 32786\n",
            "    已用時間: 232.8秒 | 預計剩餘: 884.7秒\n",
            "    [訓練中...] [完成] 訓練時間: 13.73秒 (21 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.057214\n",
            "\n",
            "  [貝葉斯優化 25.0%] [7/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 4e-04, Dropout: 0.4, Batch: 32786\n",
            "    已用時間: 246.8秒 | 預計剩餘: 740.4秒\n",
            "    [訓練中...] [完成] 訓練時間: 33.62秒 (128 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.313382\n",
            "\n",
            "  [貝葉斯優化 29.2%] [8/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 3e-04, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 280.7秒 | 預計剩餘: 681.6秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.63秒 (26 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.034788\n",
            "\n",
            "  [貝葉斯優化 33.3%] [9/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 2e-03, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 295.5秒 | 預計剩餘: 591.1秒\n",
            "    [訓練中...] [完成] 訓練時間: 10.35秒 (9 epochs)\n",
            "    [結果] 最佳 Val Loss: 1.137263\n",
            "\n",
            "  [貝葉斯優化 37.5%] [10/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.05, Batch: 32786\n",
            "    已用時間: 306.1秒 | 預計剩餘: 510.2秒\n",
            "    [訓練中...] [完成] 訓練時間: 15.37秒 (30 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.026330\n",
            "\n",
            "  [貝葉斯優化 41.7%] [11/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 9e-04, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 321.8秒 | 預計剩餘: 450.5秒\n",
            "    [訓練中...] [完成] 訓練時間: 12.98秒 (17 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.038692\n",
            "\n",
            "  [貝葉斯優化 45.8%] [12/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 335.0秒 | 預計剩餘: 396.0秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.05秒 (23 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.043025\n",
            "\n",
            "  [貝葉斯優化 50.0%] [13/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 349.3秒 | 預計剩餘: 349.3秒\n",
            "    [訓練中...] [完成] 訓練時間: 10.54秒 (10 epochs)\n",
            "    [結果] 最佳 Val Loss: 1.137333\n",
            "\n",
            "  [貝葉斯優化 54.2%] [14/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 3e-04, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 360.1秒 | 預計剩餘: 304.7秒\n",
            "    [訓練中...] [完成] 訓練時間: 15.46秒 (28 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.035325\n",
            "\n",
            "  [貝葉斯優化 58.3%] [15/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 8e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 375.8秒 | 預計剩餘: 268.4秒\n",
            "    [訓練中...] [完成] 訓練時間: 16.45秒 (45 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.024952\n",
            "[評估中...] [完成] Train MSE: 0.024359, Val MSE: 0.024952\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 62.5%] [16/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.1, Batch: 32786\n",
            "    已用時間: 428.1秒 | 預計剩餘: 256.8秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.10秒 (23 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.029700\n",
            "\n",
            "  [貝葉斯優化 66.7%] [17/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 442.4秒 | 預計剩餘: 221.2秒\n",
            "    [訓練中...] [完成] 訓練時間: 13.37秒 (19 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.035360\n",
            "\n",
            "  [貝葉斯優化 70.8%] [18/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 4e-04, Dropout: 0.05, Batch: 32786\n",
            "    已用時間: 456.1秒 | 預計剩餘: 187.8秒\n",
            "    [訓練中...] [完成] 訓練時間: 11.94秒 (11 epochs)\n",
            "    [結果] 最佳 Val Loss: 1.137312\n",
            "\n",
            "  [貝葉斯優化 75.0%] [19/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.4, Batch: 32786\n",
            "    已用時間: 468.3秒 | 預計剩餘: 156.1秒\n",
            "    [訓練中...] [完成] 訓練時間: 13.17秒 (18 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.053698\n",
            "\n",
            "  [貝葉斯優化 79.2%] [20/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 481.7秒 | 預計剩餘: 126.8秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.28秒 (24 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.039495\n",
            "\n",
            "  [貝葉斯優化 83.3%] [21/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 8e-04, Dropout: 0.1, Batch: 32786\n",
            "    已用時間: 496.8秒 | 預計剩餘: 99.4秒\n",
            "    [訓練中...] [完成] 訓練時間: 11.62秒 (9 epochs)\n",
            "    [結果] 最佳 Val Loss: 1.137324\n",
            "\n",
            "  [貝葉斯優化 87.5%] [22/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.1, Batch: 32786\n",
            "    已用時間: 508.7秒 | 預計剩餘: 72.7秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.23秒 (24 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.027705\n",
            "\n",
            "  [貝葉斯優化 91.7%] [23/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 8e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 523.2秒 | 預計剩餘: 47.6秒\n",
            "    [訓練中...] [完成] 訓練時間: 15.62秒 (40 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.024956\n",
            "[評估中...] [完成] Train MSE: 0.024371, Val MSE: 0.024955\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 95.8%] [24/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.1, Batch: 32786\n",
            "    已用時間: 575.1秒 | 預計剩餘: 25.0秒\n",
            "    [訓練中...] [完成] 訓練時間: 13.85秒 (22 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.028083\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "[BEST] 最佳超參數 (STOCH):\n",
            "   Bottleneck: 1 (固定)\n",
            "   Learning Rate: 8e-04\n",
            "   Dropout: 0.0\n",
            "   Batch Size: 32786\n",
            "   最佳驗證損失: 0.024956\n",
            "   驗證集 MSE: 0.024955\n",
            "\n",
            "[INFO] 貝葉斯優化找到的最佳目標值: 0.024949\n",
            "[INFO] 最佳參數位置: [0.0006989510422011376, np.int64(0)]\n",
            "\n",
            "[OPTIMIZE] 跳過最終訓練，直接使用搜索階段最佳模型（節省時間）\n",
            "   [評估中...] [完成]\n",
            "   [結果] Train MSE: 0.024371\n",
            "   [結果] Val MSE: 0.024955\n",
            "   [結果] Train+Val MSE: 0.024500\n",
            "   [結果] Test MSE: 0.026518\n",
            "\n",
            "[COMPRESS] 開始壓縮時間序列資料: STOCH\n",
            "   準備壓縮資料...\n",
            "   使用編碼器壓縮資料...\n",
            "   ✅ 壓縮完成！\n",
            "   原始維度: 2\n",
            "   壓縮後維度: 1\n",
            "   壓縮比: 2.00:1\n",
            "   資料筆數: 819,623\n",
            "   保存路徑: /content/drive/MyDrive/工作區/論文/論文code/1106/output2/compressed_data/STOCH_compressed.csv\n",
            "[OK] STOCH 處理完成！ (耗時: 719.8秒)\n",
            "   已完成 1/7 個群組 (14.3%)\n",
            "\n",
            "############################################################\n",
            "[GROUP 2/7] 處理技術指標群組: STOCHF\n",
            "   包含指標: STOCHF_K_14, STOCHF_D_14\n",
            "   總體進度: 2/7 (28.6%)\n",
            "   已用時間: 719.8秒 | 預計剩餘: 4318.7秒 (72.0分鐘)\n",
            "############################################################\n",
            "[OK] 資料準備完成\n",
            "   訓練集形狀: (574007, 2)\n",
            "   驗證集形狀: (163744, 2)\n",
            "   測試集形狀: (81872, 2)\n",
            "\n",
            "============================================================\n",
            "[SEARCH] 開始貝葉斯優化超參數搜尋: STOCHF\n",
            "   輸入維度: 2\n",
            "   優化迭代次數: 24\n",
            "   Bottleneck 固定為: 1（將 2 維壓縮為 1 維）\n",
            "\n",
            "[INFO] 開始貝葉斯優化（高斯過程）...\n",
            "\n",
            "  [貝葉斯優化 0.0%] [1/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 1e-03, Dropout: 0.05, Batch: 32786\n",
            "    [訓練中...] [完成] 訓練時間: 13.88秒 (21 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.051182\n",
            "[評估中...] [完成] Train MSE: 0.052306, Val MSE: 0.051184\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 4.2%] [2/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 1e-03, Dropout: 0.2, Batch: 32786\n",
            "    已用時間: 49.8秒 | 預計剩餘: 1145.1秒\n",
            "    [訓練中...] [完成] 訓練時間: 12.64秒 (15 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.055913\n",
            "\n",
            "  [貝葉斯優化 8.3%] [3/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 62.5秒 | 預計剩餘: 687.2秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.44秒 (33 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.050369\n",
            "[評估中...] [完成] Train MSE: 0.051606, Val MSE: 0.050369\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 12.5%] [4/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.1, Batch: 32786\n",
            "    已用時間: 112.5秒 | 預計剩餘: 787.4秒\n",
            "    [訓練中...] [完成] 訓練時間: 13.20秒 (18 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.053453\n",
            "\n",
            "  [貝葉斯優化 16.7%] [5/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 3e-04, Dropout: 0.4, Batch: 32786\n",
            "    已用時間: 125.9秒 | 預計剩餘: 629.3秒\n",
            "    [訓練中...] [完成] 訓練時間: 15.90秒 (33 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.116054\n",
            "\n",
            "  [貝葉斯優化 20.8%] [6/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 2e-03, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 142.0秒 | 預計剩餘: 539.4秒\n",
            "    [訓練中...] [完成] 訓練時間: 10.56秒 (10 epochs)\n",
            "    [結果] 最佳 Val Loss: 1.117315\n",
            "\n",
            "  [貝葉斯優化 25.0%] [7/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 5e-04, Dropout: 0.3, Batch: 32786\n",
            "    已用時間: 152.7秒 | 預計剩餘: 458.2秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.04秒 (23 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.068602\n",
            "\n",
            "  [貝葉斯優化 29.2%] [8/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 4e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 167.0秒 | 預計剩餘: 405.6秒\n",
            "    [訓練中...] [完成] 訓練時間: 20.88秒 (66 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.050268\n",
            "[評估中...] [完成] Train MSE: 0.051546, Val MSE: 0.050268\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 33.3%] [9/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.4, Batch: 32786\n",
            "    已用時間: 223.7秒 | 預計剩餘: 447.5秒\n",
            "    [訓練中...] [完成] 訓練時間: 13.53秒 (20 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.094724\n",
            "\n",
            "  [貝葉斯優化 37.5%] [10/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 7e-04, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 237.5秒 | 預計剩餘: 395.8秒\n",
            "    [訓練中...] [完成] 訓練時間: 19.42秒 (63 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.050205\n",
            "[評估中...] [完成] Train MSE: 0.051497, Val MSE: 0.050205\n",
            "    [BEST] 更新最佳模型！\n",
            "\n",
            "  [貝葉斯優化 41.7%] [11/24] 測試組合:\n",
            "    Bottleneck: 1, LR: 1e-03, Dropout: 0.0, Batch: 32786\n",
            "    已用時間: 292.7秒 | 預計剩餘: 409.8秒\n",
            "    [訓練中...] [完成] 訓練時間: 14.71秒 (35 epochs)\n",
            "    [結果] 最佳 Val Loss: 0.050247\n",
            "[評估中...] "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-338853913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-338853913.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0;31m# 超參數搜尋\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             best_model, best_config, best_history, search_results = hyperparameter_search(\n\u001b[0m\u001b[1;32m    850\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             )\n",
            "\u001b[0;32m/tmp/ipython-input-338853913.py\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(X_train, X_val, group_name, indicator_cols)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;31m# 執行貝葉斯優化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[INFO] 開始貝葉斯優化（高斯過程）...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     result_optimization = gp_minimize(\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdimensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     return base_minimize(\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-338853913.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(learning_rate, dropout_idx)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mval_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                 \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_to_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ==================== 資料切分 ====================\n",
        "def time_split_data(df, val_split=0.2, test_split=0.1):\n",
        "    \"\"\"\n",
        "    按「交易日」為單位進行時間順序切分 (防止日內資料洩漏)\n",
        "    \"\"\"\n",
        "    # 確保 datetime 是 datetime 格式\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df['datetime']):\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    # 1. 提取所有不重複的交易日期，並排序\n",
        "    unique_dates = sorted(df['datetime'].dt.date.unique())\n",
        "    n_days = len(unique_dates)\n",
        "\n",
        "    # 2. 計算切分點的「日期索引」\n",
        "    n_test_days = int(n_days * test_split)\n",
        "    n_val_days = int(n_days * val_split)\n",
        "    n_train_days = n_days - n_val_days - n_test_days\n",
        "\n",
        "    # 3. 找出切分日期的「邊界值」\n",
        "    # 訓練集結束日期 (不包含此日) / 驗證集開始日期\n",
        "    val_start_date = unique_dates[n_train_days]\n",
        "\n",
        "    # 驗證集結束日期 (不包含此日) / 測試集開始日期\n",
        "    test_start_date = unique_dates[n_train_days + n_val_days]\n",
        "\n",
        "    print(f\"=\" * 60)\n",
        "    print(f\"[INFO] 改用「交易日」切分資料 (總天數: {n_days} 天)\")\n",
        "    print(f\"  訓練集天數: {n_train_days}\")\n",
        "    print(f\"  驗證集天數: {n_val_days}\")\n",
        "    print(f\"  測試集天數: {n_test_days}\")\n",
        "    print(f\"  Cutoff 1 (Train|Val): {val_start_date}\")\n",
        "    print(f\"  Cutoff 2 (Val|Test) : {test_start_date}\")\n",
        "\n",
        "    # 4. 使用日期進行 Mask 切分\n",
        "    # 訓練集: < val_start_date\n",
        "    train_mask = df['datetime'].dt.date < val_start_date\n",
        "    train_df = df[train_mask].copy()\n",
        "\n",
        "    # 驗證集: >= val_start_date 且 < test_start_date\n",
        "    val_mask = (df['datetime'].dt.date >= val_start_date) & (df['datetime'].dt.date < test_start_date)\n",
        "    val_df = df[val_mask].copy()\n",
        "\n",
        "    # 測試集: >= test_start_date\n",
        "    test_mask = df['datetime'].dt.date >= test_start_date\n",
        "    test_df = df[test_mask].copy()\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    n_total = len(df)\n",
        "    print(f\"  訓練集: {len(train_df):,} 筆 ({len(train_df)/n_total*100:.1f}%) | 時間: {train_df['datetime'].min()} 至 {train_df['datetime'].max()}\")\n",
        "    print(f\"  驗證集: {len(val_df):,} 筆 ({len(val_df)/n_total*100:.1f}%) | 時間: {val_df['datetime'].min()} 至 {val_df['datetime'].max()}\")\n",
        "    print(f\"  測試集: {len(test_df):,} 筆 ({len(test_df)/n_total*100:.1f}%) | 時間: {test_df['datetime'].min()} 至 {test_df['datetime'].max()}\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# ==================== 資料準備 ====================\n",
        "def prepare_indicator_data(df, indicator_cols):\n",
        "    \"\"\"準備指定指標的資料\"\"\"\n",
        "    # 檢查欄位是否存在\n",
        "    missing_cols = [col for col in indicator_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"[ERROR] 缺少欄位: {missing_cols}\")\n",
        "\n",
        "    data = df[indicator_cols].values\n",
        "    # 處理無限大和NaN值\n",
        "    data = np.nan_to_num(data, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    return data\n",
        "\n",
        "# ==================== Autoencoder 模型 ====================\n",
        "def build_autoencoder(input_dim, bottleneck_size, dropout_rate=0.0):\n",
        "    \"\"\"建立 Autoencoder 模型\n",
        "\n",
        "    架構: Input → 256 → 128 → bottleneck → 128 → 256 → Output\n",
        "    \"\"\"\n",
        "    input_layer = layers.Input(shape=(input_dim,), name='input')\n",
        "\n",
        "    # 編碼器\n",
        "    x = layers.Dense(ENCODER_DIMS[0], activation='relu', name='encoder_1')(input_layer)\n",
        "    if dropout_rate > 0:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(ENCODER_DIMS[1], activation='relu', name='encoder_2')(x)\n",
        "    if dropout_rate > 0:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    bottleneck = layers.Dense(bottleneck_size, activation='relu', name='bottleneck')(x)\n",
        "\n",
        "    # 解碼器\n",
        "    x = layers.Dense(DECODER_DIMS[0], activation='relu', name='decoder_1')(bottleneck)\n",
        "    if dropout_rate > 0:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(DECODER_DIMS[1], activation='relu', name='decoder_2')(x)\n",
        "    if dropout_rate > 0:\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    output = layers.Dense(input_dim, activation='linear', name='output')(x)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output, name='autoencoder')\n",
        "    return model\n",
        "\n",
        "# ==================== 訓練函數 ====================\n",
        "def train_autoencoder(X_train, X_val, bottleneck_size, lr, dropout_rate, batch_size,\n",
        "                     max_epochs=200, patience=12, group_name=\"\", show_progress=False):\n",
        "    \"\"\"訓練單一 Autoencoder\"\"\"\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    # 建立模型\n",
        "    model = build_autoencoder(input_dim, bottleneck_size, dropout_rate)\n",
        "\n",
        "    # 編譯模型\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
        "\n",
        "    # 早停回調\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=patience,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 訓練歷史記錄\n",
        "    history_callback = callbacks.History()\n",
        "\n",
        "    # 自定義回調來顯示進度\n",
        "    class ProgressCallback(callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            if show_progress and (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {logs['loss']:.6f}, Val Loss: {logs['val_loss']:.6f}\",\n",
        "                      end='\\r', flush=True)\n",
        "\n",
        "    progress_callback = ProgressCallback() if show_progress else None\n",
        "\n",
        "    # 記錄訓練開始時間\n",
        "    train_start_time = datetime.now()\n",
        "\n",
        "    # 訓練模型\n",
        "    callbacks_list = [early_stopping, history_callback]\n",
        "    if progress_callback:\n",
        "        callbacks_list.append(progress_callback)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, X_train,\n",
        "        validation_data=(X_val, X_val),\n",
        "        epochs=max_epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 記錄訓練結束時間\n",
        "    train_end_time = datetime.now()\n",
        "    train_duration = (train_end_time - train_start_time).total_seconds()\n",
        "\n",
        "    # 計算每個epoch的時間（平均）\n",
        "    n_epochs = len(history.history['loss'])\n",
        "    if n_epochs > 0:\n",
        "        time_per_epoch = train_duration / n_epochs\n",
        "        # 生成時間軸（累積時間）\n",
        "        epoch_times = [time_per_epoch * (i + 1) for i in range(n_epochs)]\n",
        "        history.history['epoch_times'] = epoch_times\n",
        "        history.history['total_time'] = train_duration\n",
        "    else:\n",
        "        history.history['epoch_times'] = []\n",
        "        history.history['total_time'] = 0\n",
        "\n",
        "    return model, history.history\n",
        "\n",
        "# ==================== 超參數搜尋（貝葉斯優化） ====================\n",
        "def hyperparameter_search(X_train, X_val, group_name, indicator_cols):\n",
        "    \"\"\"使用貝葉斯優化進行超參數搜尋\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[SEARCH] 開始貝葉斯優化超參數搜尋: {group_name}\")\n",
        "    print(f\"   輸入維度: {len(indicator_cols)}\")\n",
        "    print(f\"   優化迭代次數: {BAYESIAN_N_CALLS}\")\n",
        "\n",
        "    # 定義搜索空間\n",
        "    # Bottleneck 固定為 1（將所有特徵壓縮成一維度）\n",
        "    input_dim = len(indicator_cols)\n",
        "    FIXED_BOTTLENECK = 1  # 固定值\n",
        "\n",
        "    print(f\"   Bottleneck 固定為: {FIXED_BOTTLENECK}（將 {input_dim} 維壓縮為 1 維）\")\n",
        "\n",
        "    # 確保所有維度都有至少兩個候選值（learning rate 使用連續空間，無需檢查）\n",
        "    assert len(DROPOUT_RATES) >= 2, f\"Dropout 候選必須至少 2 個，當前: {DROPOUT_RATES}\"\n",
        "\n",
        "    # 搜索空間：只搜索 lr, dropout, batch（bottleneck 已固定）\n",
        "    dimensions = [\n",
        "        LEARNING_RATE_SPACE,  # learning rate（連續空間，名稱='learning_rate'）\n",
        "        Integer(0, len(DROPOUT_RATES) - 1, name='dropout_idx'),  # dropout 索引\n",
        "    ]\n",
        "\n",
        "    # 儲存所有評估結果\n",
        "    results = []\n",
        "    best_val_mse = float('inf')\n",
        "    best_config = None\n",
        "    best_model = None\n",
        "    best_history = None\n",
        "    iteration_count = [0]  # 使用列表以便在嵌套函數中修改\n",
        "\n",
        "    search_start_time = datetime.now()\n",
        "\n",
        "    # 定義目標函數（貝葉斯優化要最小化的函數）\n",
        "    @use_named_args(dimensions=dimensions)\n",
        "    def objective(learning_rate, dropout_idx):\n",
        "        \"\"\"目標函數：返回驗證集 MSE（要最小化）\n",
        "\n",
        "        參數：\n",
        "            learning_rate: 學習率（連續值，來自 LEARNING_RATE_SPACE）\n",
        "            dropout_idx: dropout 索引（離散值）\n",
        "        \"\"\"\n",
        "        iteration_count[0] += 1\n",
        "        idx = iteration_count[0]\n",
        "\n",
        "        # 將索引轉換為實際值（bottleneck 固定為 1）\n",
        "        bottleneck = FIXED_BOTTLENECK\n",
        "        lr = float(learning_rate)  # 學習率是連續值，直接使用\n",
        "        dropout = DROPOUT_RATES[int(dropout_idx)]\n",
        "        batch = BATCH_SIZES # Directly use the fixed BATCH_SIZES\n",
        "\n",
        "        config = {\n",
        "            'bottleneck': bottleneck,\n",
        "            'lr': lr,\n",
        "            'dropout': dropout,\n",
        "            'batch': batch\n",
        "        }\n",
        "\n",
        "        # 計算進度\n",
        "        progress = (idx - 1) / BAYESIAN_N_CALLS * 100\n",
        "        elapsed_time = (datetime.now() - search_start_time).total_seconds()\n",
        "\n",
        "        print(f\"\\n  [貝葉斯優化 {progress:.1f}%] [{idx}/{BAYESIAN_N_CALLS}] 測試組合:\")\n",
        "        print(f\"    Bottleneck: {bottleneck}, LR: {lr:.0e}, \"\n",
        "              f\"Dropout: {dropout}, Batch: {batch}\")\n",
        "\n",
        "        if idx > 1:\n",
        "            avg_time = elapsed_time / (idx - 1)\n",
        "            remaining = avg_time * (BAYESIAN_N_CALLS - idx + 1)\n",
        "            print(f\"    已用時間: {elapsed_time:.1f}秒 | 預計剩餘: {remaining:.1f}秒\")\n",
        "\n",
        "        try:\n",
        "            # 訓練模型（搜索階段使用較短的耐心值）\n",
        "            print(f\"    [訓練中...] \", end='', flush=True)\n",
        "            model, history = train_autoencoder(\n",
        "                X_train, X_val,\n",
        "                bottleneck_size=bottleneck,\n",
        "                lr=lr,\n",
        "                dropout_rate=dropout,\n",
        "                batch_size=batch,\n",
        "                max_epochs=MAX_EPOCHS,\n",
        "                patience=SEARCH_EARLY_STOPPING_PATIENCE,  # 搜索階段使用較短的耐心值（8）\n",
        "                group_name=group_name\n",
        "            )\n",
        "\n",
        "            # 使用訓練歷史中的最佳驗證損失作為目標值（更準確且更快）\n",
        "            best_val = float(np.min(history['val_loss']))\n",
        "\n",
        "            # 顯示訓練時間和結果\n",
        "            if 'total_time' in history:\n",
        "                epochs = len(history.get('loss', []))\n",
        "                print(f\"[完成] 訓練時間: {history['total_time']:.2f}秒 ({epochs} epochs)\")\n",
        "\n",
        "            print(f\"    [結果] 最佳 Val Loss: {best_val:.6f}\")\n",
        "\n",
        "            # 更新最佳模型（bottleneck 固定為 1，只需比較驗證損失）\n",
        "            nonlocal best_val_mse, best_config, best_model, best_history\n",
        "            is_better = False\n",
        "            if best_val < best_val_mse * 0.99:  # 明顯更好（>1%）\n",
        "                is_better = True\n",
        "            elif best_config is not None and best_val <= best_val_mse * 1.01:\n",
        "                # 接近（±1%）且相同或更好（bottleneck 固定，無需比較）\n",
        "                is_better = True\n",
        "\n",
        "            # 只有當 is_better 成立時，才計算詳細的 MSE（用於報表，節省時間）\n",
        "            train_mse = None\n",
        "            val_mse = None\n",
        "            if is_better:\n",
        "                print(f\"[評估中...] \", end='', flush=True)\n",
        "                val_pred = model.predict(X_val, verbose=0)\n",
        "                val_mse = mean_squared_error(X_val, val_pred)\n",
        "\n",
        "                train_pred = model.predict(X_train, verbose=0)\n",
        "                train_mse = mean_squared_error(X_train, train_pred)\n",
        "\n",
        "                print(f\"[完成] Train MSE: {train_mse:.6f}, Val MSE: {val_mse:.6f}\")\n",
        "\n",
        "                best_val_mse = best_val  # 使用 val_loss 作為比較基準\n",
        "                best_config = config\n",
        "                best_model = model\n",
        "                best_history = history\n",
        "                print(f\"    [BEST] 更新最佳模型！\")\n",
        "\n",
        "            # 儲存結果（只保存基本信息，避免保存所有模型）\n",
        "            result = {\n",
        "                'config': config,\n",
        "                'train_mse': train_mse,  # 可能為 None（只有 is_better 時才計算）\n",
        "                'val_mse': val_mse,  # 可能為 None（只有 is_better 時才計算）\n",
        "                'best_val_loss': best_val,  # 使用這個作為主要指標（從 history 中取得）\n",
        "                'model': model if is_better else None,  # 只保存最佳模型以節省記憶體\n",
        "                'history': history\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # 返回最佳驗證損失（貝葉斯優化要最小化的值）\n",
        "            return best_val\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    [ERROR] 訓練失敗: {e}\")\n",
        "            # 返回一個很大的值，表示這組參數不好\n",
        "            return 1e10\n",
        "\n",
        "    # 執行貝葉斯優化\n",
        "    print(f\"\\n[INFO] 開始貝葉斯優化（高斯過程）...\")\n",
        "    result_optimization = gp_minimize(\n",
        "        func=objective,\n",
        "        dimensions=dimensions,\n",
        "        n_calls=BAYESIAN_N_CALLS,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_initial_points=min(4, BAYESIAN_N_CALLS),  # 初始隨機採樣點數\n",
        "        acq_func='EI',  # Expected Improvement 採 acquisition function\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # 檢查是否有成功的結果\n",
        "    if best_config is None:\n",
        "        raise ValueError(f\"[ERROR] {group_name} 沒有成功的訓練結果！\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[BEST] 最佳超參數 ({group_name}):\")\n",
        "    print(f\"   Bottleneck: {FIXED_BOTTLENECK} (固定)\")\n",
        "    print(f\"   Learning Rate: {best_config['lr']:.0e}\")\n",
        "    print(f\"   Dropout: {best_config['dropout']}\")\n",
        "    print(f\"   Batch Size: {best_config['batch']}\")\n",
        "    print(f\"   最佳驗證損失: {best_val_mse:.6f}\")\n",
        "    # 如果有計算過的 val_mse，顯示它\n",
        "    best_result = next((r for r in results if r.get('val_mse') is not None and r['config'] == best_config), None)\n",
        "    if best_result:\n",
        "        print(f\"   驗證集 MSE: {best_result['val_mse']:.6f}\")\n",
        "    print(f\"\\n[INFO] 貝葉斯優化找到的最佳目標值: {result_optimization.fun:.6f}\")\n",
        "    print(f\"[INFO] 最佳參數位置: {result_optimization.x}\")\n",
        "\n",
        "    return best_model, best_config, best_history, results\n",
        "\n",
        "# ==================== 最終訓練 ====================\n",
        "def final_training(X_train, X_val, X_test, best_config, group_name, best_model=None):\n",
        "    \"\"\"使用最佳參數在 Train+Val 上重訓，評估 Test\n",
        "\n",
        "    如果提供 best_model，使用遷移學習（繼續訓練）而非完全重新訓練，節省時間\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[FINAL] 最終訓練: {group_name}\")\n",
        "    print(f\"   使用 Train+Val 資料重新訓練...\")\n",
        "\n",
        "    # 合併 Train 和 Val\n",
        "    X_train_val = np.vstack([X_train, X_val])\n",
        "\n",
        "    # 切分 Train+Val 為新的 train 和 val（用於早停，比例為 80:20）\n",
        "    n_val_final = int(len(X_train_val) * 0.2)\n",
        "    X_train_final = X_train_val[:-n_val_final]\n",
        "    X_val_final = X_train_val[-n_val_final:]\n",
        "\n",
        "    print(f\"   最終訓練集: {len(X_train_final):,} 筆\")\n",
        "    print(f\"   最終驗證集: {len(X_val_final):,} 筆（用於早停）\")\n",
        "    print(f\"   測試集: {len(X_test):,} 筆\")\n",
        "\n",
        "    # 如果提供了最佳模型，使用遷移學習（繼續訓練）而非完全重新訓練\n",
        "    if best_model is not None:\n",
        "        print(f\"   [優化] 使用遷移學習：從搜索階段最佳模型繼續訓練（節省時間）\")\n",
        "        input_dim = X_train_final.shape[1]\n",
        "\n",
        "        # 建立相同架構的新模型\n",
        "        model = build_autoencoder(\n",
        "            input_dim,\n",
        "            best_config['bottleneck'],\n",
        "            best_config['dropout']\n",
        "        )\n",
        "\n",
        "        # 編譯模型\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=best_config['lr'])\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
        "\n",
        "        # 複製最佳模型的權重（遷移學習）\n",
        "        try:\n",
        "            # 嘗試複製權重層對應\n",
        "            best_layers = best_model.layers\n",
        "            new_layers = model.layers\n",
        "\n",
        "            # 複製可訓練層的權重\n",
        "            for best_layer, new_layer in zip(best_layers, new_layers):\n",
        "                if len(best_layer.get_weights()) > 0 and len(new_layer.get_weights()) > 0:\n",
        "                    # 檢查層結構是否匹配\n",
        "                    if (best_layer.get_weights()[0].shape == new_layer.get_weights()[0].shape and\n",
        "                        len(best_layer.get_weights()) == len(new_layer.get_weights())):\n",
        "                        new_layer.set_weights(best_layer.get_weights())\n",
        "            print(f\"   ✅ 權重遷移成功\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ 權重遷移失敗，將從頭訓練: {e}\")\n",
        "\n",
        "        # 早停回調\n",
        "        early_stopping = callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=EARLY_STOPPING_PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        history_callback = callbacks.History()\n",
        "\n",
        "        # 記錄訓練開始時間\n",
        "        train_start_time = datetime.now()\n",
        "\n",
        "        # 繼續訓練（通常只需要很少的 epochs，因為已經有好的初始權重）\n",
        "        history = model.fit(\n",
        "            X_train_final, X_train_final,\n",
        "            validation_data=(X_val_final, X_val_final),\n",
        "            epochs=MAX_EPOCHS,\n",
        "            batch_size=best_config['batch'],\n",
        "            callbacks=[early_stopping, history_callback],\n",
        "            verbose=0,\n",
        "            initial_epoch=0\n",
        "        )\n",
        "\n",
        "        # 記錄訓練結束時間\n",
        "        train_end_time = datetime.now()\n",
        "        train_duration = (train_end_time - train_start_time).total_seconds()\n",
        "\n",
        "        # 計算每個epoch的時間\n",
        "        n_epochs = len(history.history['loss'])\n",
        "        if n_epochs > 0:\n",
        "            time_per_epoch = train_duration / n_epochs\n",
        "            epoch_times = [time_per_epoch * (i + 1) for i in range(n_epochs)]\n",
        "            history.history['epoch_times'] = epoch_times\n",
        "            history.history['total_time'] = train_duration\n",
        "        else:\n",
        "            history.history['epoch_times'] = []\n",
        "            history.history['total_time'] = 0\n",
        "\n",
        "        history = history.history\n",
        "    else:\n",
        "        # 完全重新訓練（原始方法）\n",
        "        model, history = train_autoencoder(\n",
        "            X_train_final, X_val_final,\n",
        "            bottleneck_size=best_config['bottleneck'],\n",
        "            lr=best_config['lr'],\n",
        "            dropout_rate=best_config['dropout'],\n",
        "            batch_size=best_config['batch'],\n",
        "            max_epochs=MAX_EPOCHS,\n",
        "            patience=EARLY_STOPPING_PATIENCE,\n",
        "            group_name=group_name\n",
        "        )\n",
        "\n",
        "    epochs = len(history.get('loss', []))\n",
        "    train_time = history.get('total_time', 0)\n",
        "    print(f\"[完成] 訓練時間: {train_time:.2f}秒 ({epochs} epochs)\")\n",
        "\n",
        "    print(f\"   [評估中...] \", end='', flush=True)\n",
        "\n",
        "    # 評估所有集合（使用完整的 Train+Val 和 Test）\n",
        "    train_val_pred = model.predict(X_train_val, verbose=0)\n",
        "    test_pred = model.predict(X_test, verbose=0)\n",
        "\n",
        "    train_val_mse = mean_squared_error(X_train_val, train_val_pred)\n",
        "    test_mse = mean_squared_error(X_test, test_pred)\n",
        "\n",
        "    # 分別計算 Train 和 Val 的 MSE（僅用於報告）\n",
        "    train_pred_only = model.predict(X_train, verbose=0)\n",
        "    val_pred_only = model.predict(X_val, verbose=0)\n",
        "    train_mse = mean_squared_error(X_train, train_pred_only)\n",
        "    val_mse = mean_squared_error(X_val, val_pred_only)\n",
        "\n",
        "    print(f\"[完成]\")\n",
        "    print(f\"   [結果] Train MSE: {train_mse:.6f}\")\n",
        "    print(f\"   [結果] Val MSE: {val_mse:.6f}\")\n",
        "    print(f\"   [結果] Train+Val MSE: {train_val_mse:.6f}\")\n",
        "    print(f\"   [結果] Test MSE: {test_mse:.6f}\")\n",
        "\n",
        "    return model, {\n",
        "        'train_mse': train_mse,\n",
        "        'val_mse': val_mse,\n",
        "        'train_val_mse': train_val_mse,\n",
        "        'test_mse': test_mse,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# ==================== 壓縮並保存資料 ====================\n",
        "def compress_and_save_data(model, scaler, df, indicator_cols, group_name, output_dir, bottleneck_size):\n",
        "    \"\"\"使用訓練好的模型壓縮資料並保存為時間序列格式\"\"\"\n",
        "    # 創建壓縮資料輸出目錄\n",
        "    compressed_dir = os.path.join(output_dir, \"compressed_data\")\n",
        "    os.makedirs(compressed_dir, exist_ok=True)\n",
        "\n",
        "    # 準備所有資料\n",
        "    print(f\"   準備壓縮資料...\")\n",
        "    all_data = prepare_indicator_data(df, indicator_cols)\n",
        "\n",
        "    # 標準化\n",
        "    all_data_scaled = scaler.transform(all_data)\n",
        "\n",
        "    # 提取編碼器部分（從輸入到 bottleneck）\n",
        "    # 構建編碼器模型\n",
        "    encoder_input = model.input\n",
        "    encoder_output = None\n",
        "\n",
        "    # 找到 bottleneck 層的輸出\n",
        "    for layer in model.layers:\n",
        "        if layer.name == 'bottleneck':\n",
        "            encoder_output = layer.output\n",
        "            break\n",
        "\n",
        "    if encoder_output is None:\n",
        "        # 如果找不到，使用最後一個編碼器層\n",
        "        # 找到 bottleneck 之前的層\n",
        "        bottleneck_layer_idx = None\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            if layer.name == 'bottleneck':\n",
        "                bottleneck_layer_idx = i\n",
        "                break\n",
        "\n",
        "        if bottleneck_layer_idx is not None:\n",
        "            encoder_output = model.layers[bottleneck_layer_idx].output\n",
        "        else:\n",
        "            raise ValueError(\"無法找到編碼器輸出層\")\n",
        "\n",
        "    # 創建編碼器模型\n",
        "    encoder_model = models.Model(inputs=encoder_input, outputs=encoder_output)\n",
        "\n",
        "    # 使用編碼器壓縮資料\n",
        "    print(f\"   使用編碼器壓縮資料...\")\n",
        "    compressed_data = encoder_model.predict(all_data_scaled, verbose=0)\n",
        "\n",
        "    # 創建包含 datetime 和壓縮特徵的 DataFrame\n",
        "    compressed_df = pd.DataFrame(\n",
        "        compressed_data,\n",
        "        columns=[f\"{group_name}_compressed_{i}\" for i in range(bottleneck_size)]\n",
        "    )\n",
        "\n",
        "    # 添加 datetime 列（如果原始資料有）\n",
        "    if 'datetime' in df.columns:\n",
        "        compressed_df['datetime'] = df['datetime'].values\n",
        "        # 將 datetime 移到第一列\n",
        "        cols = ['datetime'] + [col for col in compressed_df.columns if col != 'datetime']\n",
        "        compressed_df = compressed_df[cols]\n",
        "\n",
        "    # 保存為 CSV\n",
        "    output_path = os.path.join(compressed_dir, f\"{group_name}_compressed.csv\")\n",
        "    compressed_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    # 顯示壓縮統計\n",
        "    original_size = all_data.shape[1]\n",
        "    compressed_size = bottleneck_size\n",
        "    compression_ratio = original_size / compressed_size\n",
        "\n",
        "    print(f\"   ✅ 壓縮完成！\")\n",
        "    print(f\"   原始維度: {original_size}\")\n",
        "    print(f\"   壓縮後維度: {compressed_size}\")\n",
        "    print(f\"   壓縮比: {compression_ratio:.2f}:1\")\n",
        "    print(f\"   資料筆數: {len(compressed_df):,}\")\n",
        "    print(f\"   保存路徑: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# ==================== 繪圖函數 ====================\n",
        "def plot_training_history(history, group_name, output_path):\n",
        "    \"\"\"繪製訓練歷史（包含時間軸）\"\"\"\n",
        "    # 如果有時間信息，創建3個子圖，否則2個\n",
        "    has_time = 'epoch_times' in history and len(history['epoch_times']) > 0\n",
        "\n",
        "    if has_time:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "        epoch_times = history['epoch_times']\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # MSE 圖（按 Epoch）\n",
        "    axes[0].plot(history['loss'], label='Train MSE', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], label='Val MSE', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('MSE', fontsize=12)\n",
        "    axes[0].set_title(f'{group_name} - Training MSE (by Epoch)', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 對數尺度 MSE 圖（按 Epoch）\n",
        "    axes[1].semilogy(history['loss'], label='Train MSE', linewidth=2)\n",
        "    axes[1].semilogy(history['val_loss'], label='Val MSE', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('MSE (log scale)', fontsize=12)\n",
        "    axes[1].set_title(f'{group_name} - Training MSE (Log Scale, by Epoch)', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 如果有時間信息，添加時間軸圖\n",
        "    if has_time:\n",
        "        # MSE 圖（按時間）\n",
        "        axes[2].plot(epoch_times, history['loss'], label='Train MSE', linewidth=2)\n",
        "        axes[2].plot(epoch_times, history['val_loss'], label='Val MSE', linewidth=2)\n",
        "        axes[2].set_xlabel('Training Time (seconds)', fontsize=12)\n",
        "        axes[2].set_ylabel('MSE', fontsize=12)\n",
        "        axes[2].set_title(f'{group_name} - Training MSE (by Time)\\nTotal: {history.get(\"total_time\", 0):.1f}s',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "        axes[2].legend(fontsize=10)\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# ==================== Excel 輸出 ====================\n",
        "def save_results_to_excel(all_results, output_dir):\n",
        "    \"\"\"將所有結果保存到 Excel\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"[SAVE] 保存結果到 Excel...\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    excel_path = os.path.join(output_dir, f\"autoencoder_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\")\n",
        "\n",
        "    wb = openpyxl.Workbook()\n",
        "\n",
        "    # 1. 摘要表\n",
        "    ws_summary = wb.active\n",
        "    ws_summary.title = \"摘要\"\n",
        "    ws_summary.append([\"技術指標群組\", \"輸入維度\", \"最佳 Bottleneck\", \"最佳 LR\", \"最佳 Dropout\",\n",
        "                      \"最佳 Batch Size\", \"Train MSE\", \"Val MSE\", \"Train+Val MSE\", \"Test MSE\",\n",
        "                      \"訓練時間(秒)\", \"總訓練時間(秒)\"])\n",
        "\n",
        "    for group_name, result in all_results.items():\n",
        "        config = result['best_config']\n",
        "        final_scores = result['final_scores']\n",
        "        history = final_scores.get('history', {})\n",
        "        search_time = history.get('total_time', 0) if 'total_time' in history else 0\n",
        "\n",
        "        # 計算總訓練時間（搜尋階段 + 最終訓練）\n",
        "        final_history = final_scores.get('history', {})\n",
        "        final_time = final_history.get('total_time', 0) if 'total_time' in final_history else 0\n",
        "\n",
        "        # 計算搜尋階段的總時間\n",
        "        search_total_time = 0\n",
        "        for search_result in result.get('search_results', []):\n",
        "            if isinstance(search_result, dict) and 'history' in search_result:\n",
        "                search_total_time += search_result['history'].get('total_time', 0)\n",
        "\n",
        "        total_training_time = search_total_time + final_time\n",
        "\n",
        "        ws_summary.append([\n",
        "            group_name,\n",
        "            result['input_dim'],\n",
        "            config['bottleneck'],\n",
        "            config['lr'],\n",
        "            config['dropout'],\n",
        "            config['batch'],\n",
        "            f\"{final_scores['train_mse']:.6f}\",\n",
        "            f\"{final_scores['val_mse']:.6f}\",\n",
        "            f\"{final_scores.get('train_val_mse', final_scores['train_mse']):.6f}\",\n",
        "            f\"{final_scores['test_mse']:.6f}\",\n",
        "            f\"{final_time:.2f}\",\n",
        "            f\"{total_training_time:.2f}\"\n",
        "        ])\n",
        "\n",
        "    # 2. 為每個群組創建詳細工作表\n",
        "    for group_name, result in all_results.items():\n",
        "        ws = wb.create_sheet(title=group_name[:31])  # Excel工作表名稱限制31字元\n",
        "\n",
        "        # 超參數搜尋結果\n",
        "        ws.append([\"超參數搜尋結果\"])\n",
        "        ws.append([\"Bottleneck\", \"Learning Rate\", \"Dropout\", \"Batch Size\", \"Train MSE\", \"Val MSE\"])\n",
        "\n",
        "        for search_result in result['search_results']:\n",
        "            config = search_result['config']\n",
        "            ws.append([\n",
        "                config['bottleneck'],\n",
        "                config['lr'],\n",
        "                config['dropout'],\n",
        "                config['batch'],\n",
        "                f\"{search_result['train_mse']:.6f}\",\n",
        "                f\"{search_result['val_mse']:.6f}\"\n",
        "            ])\n",
        "\n",
        "        ws.append([])\n",
        "        ws.append([\"最佳配置\"])\n",
        "        best_config = result['best_config']\n",
        "        ws.append([\"Bottleneck\", best_config['bottleneck']])\n",
        "        ws.append([\"Learning Rate\", best_config['lr']])\n",
        "        ws.append([\"Dropout\", best_config['dropout']])\n",
        "        ws.append([\"Batch Size\", best_config['batch']])\n",
        "\n",
        "        ws.append([])\n",
        "        ws.append([\"最終評估結果\"])\n",
        "        final_scores = result['final_scores']\n",
        "        ws.append([\"Train MSE\", f\"{final_scores['train_mse']:.6f}\"])\n",
        "        ws.append([\"Val MSE\", f\"{final_scores['val_mse']:.6f}\"])\n",
        "        if 'train_val_mse' in final_scores:\n",
        "            ws.append([\"Train+Val MSE\", f\"{final_scores['train_val_mse']:.6f}\"])\n",
        "        ws.append([\"Test MSE\", f\"{final_scores['test_mse']:.6f}\"])\n",
        "\n",
        "        # 添加時間信息\n",
        "        ws.append([])\n",
        "        ws.append([\"訓練時間信息\"])\n",
        "        final_history = final_scores.get('history', {})\n",
        "        if 'total_time' in final_history:\n",
        "            ws.append([\"最終訓練時間\", f\"{final_history['total_time']:.2f} 秒\"])\n",
        "            ws.append([\"平均每 Epoch 時間\", f\"{final_history['total_time'] / max(len(final_history.get('loss', [])), 1):.2f} 秒\"])\n",
        "        else:\n",
        "            ws.append([\"最終訓練時間\", \"未記錄\"])\n",
        "\n",
        "        # 計算搜尋階段的總時間\n",
        "        search_total_time = 0\n",
        "        for search_result in result.get('search_results', []):\n",
        "            if 'history' in search_result and 'total_time' in search_result['history']:\n",
        "                search_total_time += search_result['history']['total_time']\n",
        "        if search_total_time > 0:\n",
        "            ws.append([\"超參數搜尋總時間\", f\"{search_total_time:.2f} 秒\"])\n",
        "            ws.append([\"總訓練時間\", f\"{search_total_time + final_history.get('total_time', 0):.2f} 秒\"])\n",
        "\n",
        "        # 插入圖片\n",
        "        img_path = result['plot_path']\n",
        "        if os.path.exists(img_path):\n",
        "            try:\n",
        "                img = Image(img_path)\n",
        "                img.width = 800\n",
        "                img.height = 300\n",
        "                ws.add_image(img, f'A{ws.max_row + 3}')\n",
        "            except Exception as e:\n",
        "                print(f\"  [WARN] 無法插入圖片 {img_path}: {e}\")\n",
        "\n",
        "    # 3. 訓練日誌\n",
        "    ws_log = wb.create_sheet(title=\"訓練日誌\")\n",
        "    ws_log.append([\"時間\", \"群組\", \"階段\", \"訊息\"])\n",
        "\n",
        "    for group_name, result in all_results.items():\n",
        "        if 'log' in result:\n",
        "            for log_entry in result['log']:\n",
        "                ws_log.append(log_entry)\n",
        "\n",
        "    wb.save(excel_path)\n",
        "    print(f\"[OK] Excel 已保存: {excel_path}\")\n",
        "\n",
        "    return excel_path\n",
        "\n",
        "# ==================== 主程式 ====================\n",
        "def main():\n",
        "    \"\"\"主程式\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"[START] 技術指標 Autoencoder 壓縮訓練\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"[TIME] 開始時間: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"[SEED] 隨機種子: {RANDOM_SEED}\")\n",
        "\n",
        "    # GPU 檢查和配置\n",
        "    print(\"\\n[GPU] GPU 檢查和配置...\")\n",
        "    print(f\"TensorFlow 版本: {tf.__version__}\")\n",
        "    print(f\"TensorFlow 是否構建時包含 CUDA 支持: {tf.test.is_built_with_cuda()}\")\n",
        "\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    print(f\"可用 GPU 清單: {gpus}\")\n",
        "\n",
        "    if len(gpus) > 0:\n",
        "        print(f\"✅ 檢測到 {len(gpus)} 個 GPU 設備\")\n",
        "        try:\n",
        "            # 啟用 GPU 記憶體增長（避免一次性分配所有記憶體）\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"✅ GPU 記憶體增長已啟用\")\n",
        "\n",
        "            # 驗證 GPU 是否可用\n",
        "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "            if len(logical_gpus) > 0:\n",
        "                print(f\"✅ GPU 可用於 TensorFlow 運算: {logical_gpus}\")\n",
        "\n",
        "                # 測試 GPU 運算\n",
        "                try:\n",
        "                    with tf.device('/GPU:0'):\n",
        "                        test_tensor = tf.constant([1.0, 2.0, 3.0])\n",
        "                        result = tf.reduce_sum(test_tensor)\n",
        "                    print(f\"✅ GPU 運算測試成功: {result.numpy()}\")\n",
        "                    print(\"TensorFlow 是否使用 GPU: True\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ GPU 運算測試失敗: {e}\")\n",
        "                    print(\"TensorFlow 是否使用 GPU: False\")\n",
        "            else:\n",
        "                print(\"❌ GPU 不可用於 TensorFlow 運算\")\n",
        "                print(\"TensorFlow 是否使用 GPU: False\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"⚠️ GPU 設定警告: {e}\")\n",
        "            print(\"TensorFlow 是否使用 GPU: False\")\n",
        "    else:\n",
        "        print(\"❌ 沒有檢測到 GPU 設備\")\n",
        "        if not tf.test.is_built_with_cuda():\n",
        "            print(\"⚠️ TensorFlow 當前版本似乎不包含 CUDA 支持（CPU-only 構建）\")\n",
        "            print(\"💡 提示：如果已安裝 CUDA，可能需要：\")\n",
        "            print(\"   1. 確保已安裝完整的 CUDA Toolkit（不僅是驅動）\")\n",
        "            print(\"   2. 安裝對應版本的 cuDNN\")\n",
        "            print(\"   3. 或考慮使用 conda 安裝支持 GPU 的 TensorFlow\")\n",
        "        print(\"將使用 CPU 進行運算\")\n",
        "        print(\"TensorFlow 是否使用 GPU: False\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 創建輸出目錄\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    def load_progress(path):\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                with open(path, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception:\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def save_progress(path, data):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "    def split_train_val_by_days(df_in, val_split):\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df_in['datetime']):\n",
        "            df_in['datetime'] = pd.to_datetime(df_in['datetime'])\n",
        "        df_in = df_in.sort_values('datetime').copy()\n",
        "        df_in['date_only'] = df_in['datetime'].dt.date\n",
        "        unique_dates = sorted(df_in['date_only'].unique())\n",
        "        if len(unique_dates) < 2:\n",
        "            return df_in.drop(columns=['date_only']), df_in.drop(columns=['date_only'])\n",
        "        cutoff = int(len(unique_dates) * (1 - val_split))\n",
        "        cutoff = max(1, min(cutoff, len(unique_dates) - 1))\n",
        "        train_dates = set(unique_dates[:cutoff])\n",
        "        val_dates = set(unique_dates[cutoff:])\n",
        "        train_df = df_in[df_in['date_only'].isin(train_dates)].drop(columns=['date_only'])\n",
        "        val_df = df_in[df_in['date_only'].isin(val_dates)].drop(columns=['date_only'])\n",
        "        return train_df, val_df\n",
        "\n",
        "    # 年度切分（前 2 年訓練，當年測試）\n",
        "    df_local = df.copy()\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_local['datetime']):\n",
        "        df_local['datetime'] = pd.to_datetime(df_local['datetime'])\n",
        "    df_local['year'] = df_local['datetime'].dt.year\n",
        "    years = sorted(df_local['year'].unique())\n",
        "    predict_years = [y for y in years if (y - 1) in years and (y - 2) in years]\n",
        "    print(f\"[INFO] 年度滾動: {predict_years}\")\n",
        "\n",
        "    overall_start_time = datetime.now()\n",
        "\n",
        "    for predict_year in predict_years:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[YEAR] 開始年度: {predict_year}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        year_dir = os.path.join(OUTPUT_DIR, f\"year_{predict_year}\")\n",
        "        os.makedirs(year_dir, exist_ok=True)\n",
        "\n",
        "        train_years = [predict_year - 2, predict_year - 1]\n",
        "        train_val_df = df_local[df_local['year'].isin(train_years)].copy()\n",
        "        test_df = df_local[df_local['year'] == predict_year].copy()\n",
        "\n",
        "        if len(train_val_df) == 0 or len(test_df) == 0:\n",
        "            print(f\"[SKIP] 年度 {predict_year} 資料不足\")\n",
        "            continue\n",
        "\n",
        "        train_df, val_df = split_train_val_by_days(train_val_df, VAL_SPLIT)\n",
        "\n",
        "        # 儲存該年度結果\n",
        "        all_results_year = {}\n",
        "\n",
        "        total_groups = len(INDICATOR_GROUPS)\n",
        "        for group_idx, (group_name, indicator_cols) in enumerate(INDICATOR_GROUPS.items(), 1):\n",
        "            print(f\"\\n{'#'*60}\")\n",
        "            print(f\"[YEAR {predict_year}] [GROUP {group_idx}/{total_groups}] {group_name}\")\n",
        "            print(f\"   包含指標: {', '.join(indicator_cols)}\")\n",
        "            print(f\"   總體進度: {group_idx}/{total_groups} ({group_idx/total_groups*100:.1f}%)\")\n",
        "\n",
        "            elapsed = (datetime.now() - overall_start_time).total_seconds()\n",
        "            if group_idx > 1:\n",
        "                avg_time_per_group = elapsed / (group_idx - 1)\n",
        "                remaining_groups = total_groups - group_idx + 1\n",
        "                remaining_time = avg_time_per_group * remaining_groups\n",
        "                print(f\"   已用時間: {elapsed:.1f}秒 | 預計剩餘: {remaining_time:.1f}秒 ({remaining_time/60:.1f}分鐘)\")\n",
        "\n",
        "            print(f\"{'#'*60}\")\n",
        "\n",
        "            group_dir = os.path.join(year_dir, group_name)\n",
        "            models_dir = os.path.join(group_dir, \"models\")\n",
        "            plots_dir = os.path.join(group_dir, \"plots\")\n",
        "            os.makedirs(models_dir, exist_ok=True)\n",
        "            os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "            progress_path = os.path.join(group_dir, \"progress.json\")\n",
        "            result_path = os.path.join(group_dir, \"result.json\")\n",
        "            progress = load_progress(progress_path)\n",
        "            progress.setdefault(\"year\", int(predict_year))\n",
        "            progress.setdefault(\"group\", group_name)\n",
        "\n",
        "            if progress.get(\"compressed_done\") and os.path.exists(progress.get(\"compressed_data_path\", \"\")):\n",
        "                print(f\"[RESUME] 已完成壓縮，跳過: {group_name} ({predict_year})\")\n",
        "                if os.path.exists(result_path):\n",
        "                    try:\n",
        "                        with open(result_path, 'r', encoding='utf-8') as f:\n",
        "                            all_results_year[group_name] = json.load(f)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # 準備資料\n",
        "                X_train_raw = prepare_indicator_data(train_df, indicator_cols)\n",
        "                X_val_raw = prepare_indicator_data(val_df, indicator_cols)\n",
        "                X_test_raw = prepare_indicator_data(test_df, indicator_cols)\n",
        "\n",
        "                # 標準化（若已有 scaler 則直接載入）\n",
        "                scaler = StandardScaler()\n",
        "                if progress.get(\"scaler_path\") and os.path.exists(progress.get(\"scaler_path\")):\n",
        "                    import pickle\n",
        "                    with open(progress[\"scaler_path\"], 'rb') as f:\n",
        "                        scaler = pickle.load(f)\n",
        "                    X_train = scaler.transform(X_train_raw)\n",
        "                    X_val = scaler.transform(X_val_raw)\n",
        "                    X_test = scaler.transform(X_test_raw)\n",
        "                else:\n",
        "                    X_train = scaler.fit_transform(X_train_raw)\n",
        "                    X_val = scaler.transform(X_val_raw)\n",
        "                    X_test = scaler.transform(X_test_raw)\n",
        "\n",
        "                print(f\"[OK] 資料準備完成\")\n",
        "                print(f\"   訓練集形狀: {X_train.shape}\")\n",
        "                print(f\"   驗證集形狀: {X_val.shape}\")\n",
        "                print(f\"   測試集形狀: {X_test.shape}\")\n",
        "\n",
        "                best_config = progress.get(\"best_config\")\n",
        "                search_results = []\n",
        "                best_model = None\n",
        "                best_history = {}\n",
        "\n",
        "                if progress.get(\"final_done\") and os.path.exists(progress.get(\"final_model_path\", \"\")):\n",
        "                    print(\"[RESUME] 已完成訓練，直接進入壓縮\")\n",
        "                    final_model = keras.models.load_model(progress[\"final_model_path\"])\n",
        "                    final_model_path = progress[\"final_model_path\"]\n",
        "                    scaler_path = progress.get(\"scaler_path\")\n",
        "                    if not best_config:\n",
        "                        raise ValueError(\"進度檔缺少 best_config，無法判定 bottleneck 大小\")\n",
        "\n",
        "                    # 重新計算評估（確保一致）\n",
        "                    X_train_val = np.vstack([X_train, X_val])\n",
        "                    train_val_pred = final_model.predict(X_train_val, verbose=0)\n",
        "                    test_pred = final_model.predict(X_test, verbose=0)\n",
        "                    train_val_mse = mean_squared_error(X_train_val, train_val_pred)\n",
        "                    test_mse = mean_squared_error(X_test, test_pred)\n",
        "                    train_pred_only = final_model.predict(X_train, verbose=0)\n",
        "                    val_pred_only = final_model.predict(X_val, verbose=0)\n",
        "                    train_mse = mean_squared_error(X_train, train_pred_only)\n",
        "                    val_mse = mean_squared_error(X_val, val_pred_only)\n",
        "\n",
        "                    final_scores = {\n",
        "                        'train_mse': train_mse,\n",
        "                        'val_mse': val_mse,\n",
        "                        'train_val_mse': train_val_mse,\n",
        "                        'test_mse': test_mse,\n",
        "                        'history': progress.get(\"final_history\", {})\n",
        "                    }\n",
        "                else:\n",
        "                    if progress.get(\"search_done\") and os.path.exists(progress.get(\"search_model_path\", \"\")) and best_config:\n",
        "                        print(\"[RESUME] 已完成搜尋，續接最終訓練/評估\")\n",
        "                        best_model = keras.models.load_model(progress[\"search_model_path\"])\n",
        "                    else:\n",
        "                        # 超參數搜尋\n",
        "                        best_model, best_config, best_history, search_results = hyperparameter_search(\n",
        "                            X_train, X_val, group_name, indicator_cols\n",
        "                        )\n",
        "\n",
        "                        search_model_path = os.path.join(models_dir, f\"{group_name}_search_best.h5\")\n",
        "                        best_model.save(search_model_path)\n",
        "\n",
        "                        progress.update({\n",
        "                            \"search_done\": True,\n",
        "                            \"search_model_path\": search_model_path,\n",
        "                            \"best_config\": best_config\n",
        "                        })\n",
        "                        save_progress(progress_path, progress)\n",
        "\n",
        "                    # 最終訓練（可選）\n",
        "                    if SKIP_FINAL_TRAINING:\n",
        "                        print(f\"\\n[OPTIMIZE] 跳過最終訓練，直接使用搜索階段最佳模型（節省時間）\")\n",
        "                        final_model = best_model\n",
        "                        final_history = best_history\n",
        "\n",
        "                        print(f\"   [評估中...] \", end='', flush=True)\n",
        "                        X_train_val = np.vstack([X_train, X_val])\n",
        "                        train_val_pred = final_model.predict(X_train_val, verbose=0)\n",
        "                        test_pred = final_model.predict(X_test, verbose=0)\n",
        "                        train_val_mse = mean_squared_error(X_train_val, train_val_pred)\n",
        "                        test_mse = mean_squared_error(X_test, test_pred)\n",
        "                        train_pred_only = final_model.predict(X_train, verbose=0)\n",
        "                        val_pred_only = final_model.predict(X_val, verbose=0)\n",
        "                        train_mse = mean_squared_error(X_train, train_pred_only)\n",
        "                        val_mse = mean_squared_error(X_val, val_pred_only)\n",
        "\n",
        "                        print(f\"[完成]\")\n",
        "                        print(f\"   [結果] Train MSE: {train_mse:.6f}\")\n",
        "                        print(f\"   [結果] Val MSE: {val_mse:.6f}\")\n",
        "                        print(f\"   [結果] Train+Val MSE: {train_val_mse:.6f}\")\n",
        "                        print(f\"   [結果] Test MSE: {test_mse:.6f}\")\n",
        "\n",
        "                        final_scores = {\n",
        "                            'train_mse': train_mse,\n",
        "                            'val_mse': val_mse,\n",
        "                            'train_val_mse': train_val_mse,\n",
        "                            'test_mse': test_mse,\n",
        "                            'history': final_history\n",
        "                        }\n",
        "\n",
        "                        final_model_path = progress.get(\"search_model_path\", os.path.join(models_dir, f\"{group_name}_search_best.h5\"))\n",
        "                    else:\n",
        "                        final_model, final_scores = final_training(\n",
        "                            X_train, X_val, X_test, best_config, group_name, best_model=best_model\n",
        "                        )\n",
        "                        final_model_path = os.path.join(models_dir, f\"{group_name}_final.h5\")\n",
        "                        final_model.save(final_model_path)\n",
        "\n",
        "                    scaler_path = progress.get(\"scaler_path\") or os.path.join(models_dir, f\"{group_name}_scaler.pkl\")\n",
        "                    import pickle\n",
        "                    with open(scaler_path, 'wb') as f:\n",
        "                        pickle.dump(scaler, f)\n",
        "\n",
        "                    progress.update({\n",
        "                        \"final_done\": True,\n",
        "                        \"final_model_path\": final_model_path,\n",
        "                        \"scaler_path\": scaler_path,\n",
        "                        \"best_config\": best_config,\n",
        "                        \"final_history\": final_scores.get('history', {})\n",
        "                    })\n",
        "                    save_progress(progress_path, progress)\n",
        "\n",
        "                # 繪製訓練歷史\n",
        "                plot_path = os.path.join(plots_dir, f\"{group_name}_training_history.png\")\n",
        "                plot_training_history(final_scores['history'], group_name, plot_path)\n",
        "\n",
        "                # 壓縮並輸出時間序列資料（只壓縮該年度）\n",
        "                print(f\"\\n[COMPRESS] 開始壓縮時間序列資料: {group_name} ({predict_year})\")\n",
        "                compressed_data_path = compress_and_save_data(\n",
        "                    final_model, scaler, test_df, indicator_cols, group_name,\n",
        "                    group_dir, best_config['bottleneck']\n",
        "                )\n",
        "\n",
        "                progress.update({\n",
        "                    \"compressed_done\": True,\n",
        "                    \"compressed_data_path\": compressed_data_path\n",
        "                })\n",
        "                save_progress(progress_path, progress)\n",
        "\n",
        "                # 儲存結果\n",
        "                all_results_year[group_name] = {\n",
        "                    'year': int(predict_year),\n",
        "                    'input_dim': len(indicator_cols),\n",
        "                    'best_config': best_config,\n",
        "                    'search_results': [\n",
        "                        {\n",
        "                            'config': r['config'],\n",
        "                            'train_mse': r['train_mse'],\n",
        "                            'val_mse': r['val_mse'],\n",
        "                            'history': r.get('history', {})\n",
        "                        }\n",
        "                        for r in search_results\n",
        "                    ],\n",
        "                    'final_scores': final_scores,\n",
        "                    'plot_path': plot_path,\n",
        "                    'model_path': final_model_path,\n",
        "                    'scaler_path': scaler_path,\n",
        "                    'compressed_data_path': compressed_data_path\n",
        "                }\n",
        "\n",
        "                with open(result_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(all_results_year[group_name], f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "                elapsed_group = (datetime.now() - overall_start_time).total_seconds() - (elapsed if group_idx > 1 else 0)\n",
        "                print(f\"[OK] {group_name} ({predict_year}) 處理完成！ (耗時: {elapsed_group:.1f}秒)\")\n",
        "                print(f\"   已完成 {group_idx}/{total_groups} 個群組 ({group_idx/total_groups*100:.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] {group_name} ({predict_year}) 處理失敗: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        # 保存年度結果到 Excel/JSON\n",
        "        if all_results_year:\n",
        "            excel_path = save_results_to_excel(all_results_year, year_dir)\n",
        "            json_path = os.path.join(year_dir, f\"results_{predict_year}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "            json_results = {}\n",
        "            for k, v in all_results_year.items():\n",
        "                json_results[k] = {\n",
        "                    'year': v.get('year'),\n",
        "                    'input_dim': v['input_dim'],\n",
        "                    'best_config': v['best_config'],\n",
        "                    'final_scores': v['final_scores'],\n",
        "                    'search_results': v['search_results']\n",
        "                }\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(json_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "            print(f\"\\n[YEAR DONE] {predict_year} 完成\")\n",
        "            print(f\"[DIR] 結果目錄: {year_dir}\")\n",
        "            print(f\"[EXCEL] Excel 報告: {excel_path}\")\n",
        "            print(f\"[JSON] JSON 報告: {json_path}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"[DONE] 所有年度處理完成！\")\n",
        "    print(f\"[DIR] 結果目錄: {OUTPUT_DIR}\")\n",
        "    print(f\"[TIME] 結束時間: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
